<!DOCTYPE html><html lang="en" class="scroll-smooth"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" as="image" href="/bio.jpg"/><link rel="stylesheet" href="/_next/static/css/66c6e803a86cbc2f.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-679b75d1c4c2027c.js"/><script src="/_next/static/chunks/4bd1b696-70b6d399998de86a.js" async=""></script><script src="/_next/static/chunks/684-5aaa8290a129f299.js" async=""></script><script src="/_next/static/chunks/main-app-912ad050d553b0aa.js" async=""></script><script src="/_next/static/chunks/457-8b8d97145837e580.js" async=""></script><script src="/_next/static/chunks/874-6cc630662f3664af.js" async=""></script><script src="/_next/static/chunks/862-15038af301665bb3.js" async=""></script><script src="/_next/static/chunks/app/layout-d7cbb4646de1c3b3.js" async=""></script><script src="/_next/static/chunks/485-487001f78a0503db.js" async=""></script><script src="/_next/static/chunks/748-c442066f2b36fa6c.js" async=""></script><script src="/_next/static/chunks/app/page-1e259e5be13ce12d.js" async=""></script><link rel="icon" href="/favicon.svg" type="image/svg+xml"/><link rel="dns-prefetch" href="https://google-fonts.jialeliu.com"/><link rel="preconnect" href="https://google-fonts.jialeliu.com" crossorigin=""/><link rel="preload" as="style" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap"/><link rel="stylesheet" id="gfonts-css" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap" media="print"/><script>
              (function(){
                var l = document.getElementById('gfonts-css');
                if (!l) return;
                if (l.media !== 'all') {
                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });
                }
              })();
            </script><noscript><link rel="stylesheet" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap"/></noscript><script>
              try {
                const theme = localStorage.getItem('theme-storage');
                const parsed = theme ? JSON.parse(theme) : null;
                const setting = parsed?.state?.theme || 'system';
                const prefersDark = typeof window !== 'undefined' && window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;
                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));
                var root = document.documentElement;
                root.classList.add(effective);
                root.setAttribute('data-theme', effective);
              } catch (e) {
                var root = document.documentElement;
                root.classList.add('light');
                root.setAttribute('data-theme', 'light');
              }
            </script><title>Zhenting Qi</title><meta name="description" content="Computer Science Ph.D. student at Harvard University, researching intelligent and reliable AI systems"/><meta name="author" content="Zhenting Qi"/><meta name="keywords" content="Zhenting Qi,PhD,Research,Harvard University"/><meta name="creator" content="Zhenting Qi"/><meta name="publisher" content="Zhenting Qi"/><meta property="og:title" content="Zhenting Qi"/><meta property="og:description" content="Computer Science Ph.D. student at Harvard University, researching intelligent and reliable AI systems"/><meta property="og:site_name" content="Zhenting Qi&#x27;s Academic Website"/><meta property="og:locale" content="en_US"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Zhenting Qi"/><meta name="twitter:description" content="Computer Science Ph.D. student at Harvard University, researching intelligent and reliable AI systems"/><link rel="icon" href="/favicon.svg"/><script>document.querySelectorAll('body link[rel="icon"], body link[rel="apple-touch-icon"]').forEach(el => document.head.appendChild(el))</script><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="font-sans antialiased"><div style="visibility:hidden"><nav class="fixed top-0 left-0 right-0 z-50" data-headlessui-state=""><div class="transition-all duration-300 ease-out bg-transparent" style="transform:translateY(-100px)"><div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8"><div class="flex justify-between items-center h-16 lg:h-20"><div class="flex-shrink-0" tabindex="0"><a class="flex items-center space-x-3 text-xl lg:text-2xl font-serif font-semibold text-primary hover:text-accent transition-colors duration-200" href="/"><div class="bg-transparent dark:bg-white p-1 rounded-md dark:shadow-[0_0_15px_rgba(255,255,255,0.6)] dark:hover:shadow-[0_0_25px_rgba(255,255,255,0.9)] transition-shadow duration-300"><img alt="Harvard University" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="object-contain" style="color:transparent" src="/harvard.jpg"/></div><span class="text-neutral-300 dark:text-neutral-600">|</span><span>Zhenting Qi</span></a></div><div class="hidden lg:block"><div class="ml-10 flex items-center space-x-8"><div class="flex items-baseline space-x-8"><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-primary" href="/"><span class="relative z-10">About</span><div class="absolute inset-0 bg-accent/10 rounded-lg"></div></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/publications/"><span class="relative z-10">Publications</span></a></div><div class="flex items-center justify-center w-10 h-10 rounded-lg border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] bg-background dark:bg-neutral-800"><div class="w-4 h-4 rounded-full bg-neutral-300 animate-pulse"></div></div></div></div><div class="lg:hidden flex items-center space-x-2"><div class="flex items-center justify-center w-10 h-10 rounded-lg border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] bg-background dark:bg-neutral-800"><div class="w-4 h-4 rounded-full bg-neutral-300 animate-pulse"></div></div><button class="inline-flex items-center justify-center p-2 rounded-md text-neutral-600 hover:text-primary hover:bg-neutral-100 dark:hover:bg-neutral-800 focus:outline-none focus:ring-2 focus:ring-inset focus:ring-accent transition-colors duration-200" id="headlessui-disclosure-button-¬´R5pdb¬ª" type="button" aria-expanded="false" data-headlessui-state=""><span class="sr-only">Open main menu</span><div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="block h-6 w-6"><path stroke-linecap="round" stroke-linejoin="round" d="M3.75 6.75h16.5M3.75 12h16.5m-16.5 5.25h16.5"></path></svg></div></button></div></div></div></div></nav><main class="min-h-screen pt-16 lg:pt-20"><div class="max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-8 bg-background min-h-screen"><div class="grid grid-cols-1 lg:grid-cols-3 gap-12"><div class="lg:col-span-1"><div class="sticky top-8" style="opacity:0;transform:translateY(20px)"><div class="w-64 h-64 mx-auto mb-6 rounded-2xl overflow-hidden shadow-lg hover:shadow-xl transition-all duration-200 hover:scale-105"><img alt="Zhenting Qi" width="256" height="256" decoding="async" data-nimg="1" class="w-full h-full object-cover object-[32%_center]" style="color:transparent" src="/bio.jpg"/></div><div class="text-center mb-6"><h1 class="text-3xl font-serif font-bold text-primary mb-2">Zhenting Qi</h1><p class="text-lg text-accent font-medium mb-1">Ph.D. Student in Computer Science</p><p class="text-neutral-600 mb-2">Harvard University</p></div><div class="flex flex-wrap justify-center gap-3 sm:gap-4 mb-6 relative px-2"><div class="relative"><button class="p-2 sm:p-2 transition-colors duration-200 text-neutral-600 dark:text-neutral-400 hover:text-accent" aria-label="Email"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-5 w-5"><path stroke-linecap="round" stroke-linejoin="round" d="M21.75 6.75v10.5a2.25 2.25 0 0 1-2.25 2.25h-15a2.25 2.25 0 0 1-2.25-2.25V6.75m19.5 0A2.25 2.25 0 0 0 19.5 4.5h-15a2.25 2.25 0 0 0-2.25 2.25m19.5 0v.243a2.25 2.25 0 0 1-1.07 1.916l-7.5 4.615a2.25 2.25 0 0 1-2.36 0L3.32 8.91a2.25 2.25 0 0 1-1.07-1.916V6.75"></path></svg></button></div><div class="relative"><button class="p-2 sm:p-2 transition-colors duration-200 text-neutral-600 dark:text-neutral-400 hover:text-accent" aria-label="Location"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-5 w-5"><path stroke-linecap="round" stroke-linejoin="round" d="M15 10.5a3 3 0 1 1-6 0 3 3 0 0 1 6 0Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 10.5c0 7.142-7.5 11.25-7.5 11.25S4.5 17.642 4.5 10.5a7.5 7.5 0 1 1 15 0Z"></path></svg></button></div><a href="https://scholar.google.com/citations?hl=en&amp;user=WZ00HCUAAAAJ" target="_blank" rel="noopener noreferrer" class="p-2 sm:p-2 text-neutral-600 dark:text-neutral-400 hover:text-accent transition-colors duration-200" aria-label="Google Scholar"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-5 w-5"><path stroke-linecap="round" stroke-linejoin="round" d="M4.26 10.147a60.438 60.438 0 0 0-.491 6.347A48.62 48.62 0 0 1 12 20.904a48.62 48.62 0 0 1 8.232-4.41 60.46 60.46 0 0 0-.491-6.347m-15.482 0a50.636 50.636 0 0 0-2.658-.813A59.906 59.906 0 0 1 12 3.493a59.903 59.903 0 0 1 10.399 5.84c-.896.248-1.783.52-2.658.814m-15.482 0A50.717 50.717 0 0 1 12 13.489a50.702 50.702 0 0 1 7.74-3.342M6.75 15a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5Zm0 0v-3.675A55.378 55.378 0 0 1 12 8.443m-7.007 11.55A5.981 5.981 0 0 0 6.75 15.75v-1.5"></path></svg></a><a href="https://github.com/zhentingqi" target="_blank" rel="noopener noreferrer" class="p-2 sm:p-2 text-neutral-600 dark:text-neutral-400 hover:text-accent transition-colors duration-200" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-github h-5 w-5" aria-hidden="true"><path d="M15 22v-4a4.8 4.8 0 0 0-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35 0-3.5 0 0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35 0 3.5A5.403 5.403 0 0 0 4 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65-.17.6-.22 1.23-.15 1.85v4"></path><path d="M9 18c-4.51 2-5-2-7-2"></path></svg></a><a href="https://www.linkedin.com/in/zhentingqi" target="_blank" rel="noopener noreferrer" class="p-2 sm:p-2 text-neutral-600 dark:text-neutral-400 hover:text-accent transition-colors duration-200" aria-label="LinkedIn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-linkedin h-5 w-5" aria-hidden="true"><path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"></path><rect width="4" height="12" x="2" y="9"></rect><circle cx="4" cy="4" r="2"></circle></svg></a><a href="https://www.instagram.com/heptacol/" target="_blank" rel="noopener noreferrer" class="p-2 sm:p-2 text-neutral-600 dark:text-neutral-400 hover:text-accent transition-colors duration-200" aria-label="Instagram"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-instagram h-5 w-5" aria-hidden="true"><rect width="20" height="20" x="2" y="2" rx="5" ry="5"></rect><path d="M16 11.37A4 4 0 1 1 12.63 8 4 4 0 0 1 16 11.37z"></path><line x1="17.5" x2="17.51" y1="6.5" y2="6.5"></line></svg></a></div><div class="w-full overflow-hidden flex justify-center mt-6"><div id="map-container" class="w-full"><style>
                          #map-container {
                            width: 100%;
                            max-width: 100%;
                            display: flex;
                            justify-content: center;
                            align-items: center;
                          }
                          #mapmyvisitors,
                          #mapmyvisitors iframe,
                          #mapmyvisitors canvas,
                          #mapmyvisitors > div,
                          #mapmyvisitors > *,
                          #map-container > * {
                            max-width: 100% !important;
                            width: 100% !important;
                            box-sizing: border-box !important;
                            margin: 0 auto !important;
                          }
                          @media (max-width: 640px) {
                            #map-container {
                              transform: scale(0.65);
                              transform-origin: center center;
                              width: 100%;
                              height: auto;
                            }
                            #mapmyvisitors,
                            #mapmyvisitors iframe,
                            #mapmyvisitors canvas,
                            #mapmyvisitors > div,
                            #mapmyvisitors > * {
                              max-width: none !important;
                              width: auto !important;
                              display: block !important;
                              margin: 0 auto !important;
                            }
                          }
                          @media (max-width: 480px) {
                            #map-container {
                              transform: scale(0.5);
                              transform-origin: center center;
                            }
                          }
                          @media (max-width: 360px) {
                            #map-container {
                              transform: scale(0.4);
                              transform-origin: center center;
                            }
                          }
                        </style></div></div></div></div><div class="lg:col-span-2 space-y-8"><section id="about" class="scroll-mt-24 space-y-8"><section style="opacity:0;transform:translateY(20px)"><h2 class="text-2xl font-serif font-bold text-primary mb-4">About</h2><div class="text-neutral-700 dark:text-neutral-600 leading-relaxed"><p class="mb-4 last:mb-0">Welcome! I am a 1st year Computer Science Ph.D. student at <a href="https://www.harvard.edu/" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">Harvard University</a>, where I am honored to be co-advised by Prof. <a href="https://yilundu.github.io/" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">Yilun Du</a> and Prof. <a href="https://himalakkaraju.github.io/" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">Hima Lakkaraju</a>.</p>
<p class="mb-4 last:mb-0">My research centers around developing intelligent and reliable AI systems that benefit human society. Motivated by this, I am generally interested in the following topics (w/o particular order):</p>
<ul class="list-disc mb-2 space-y-1 ml-6 pl-0 list-outside [&amp;_ul]:ml-6 [&amp;_ul]:mt-1" node="[object Object]">
<li class="mb-1 pl-0 [&amp;&gt;ul]:ml-4 [&amp;&gt;ul]:mt-1">
<p class="mb-4 last:mb-0"><strong class="font-semibold text-primary">Reasoning</strong></p>
<ul class="list-disc mb-2 space-y-1 ml-6 pl-0 list-outside [&amp;_ul]:ml-6 [&amp;_ul]:mt-1" node="[object Object]">
<li class="mb-1 pl-0 [&amp;&gt;ul]:ml-4 [&amp;&gt;ul]:mt-1">Understanding and enhancing reasoning capabilities in foundation models</li>
<li class="mb-1 pl-0 [&amp;&gt;ul]:ml-4 [&amp;&gt;ul]:mt-1">Developing AI systems that generalize effectively to OOD scenarios</li>
<li class="mb-1 pl-0 [&amp;&gt;ul]:ml-4 [&amp;&gt;ul]:mt-1">Training (multi-)agents for compositional reasoning tasks</li>
</ul>
</li>
<li class="mb-1 pl-0 [&amp;&gt;ul]:ml-4 [&amp;&gt;ul]:mt-1">
<p class="mb-4 last:mb-0"><strong class="font-semibold text-primary">Reliability</strong></p>
<ul class="list-disc mb-2 space-y-1 ml-6 pl-0 list-outside [&amp;_ul]:ml-6 [&amp;_ul]:mt-1" node="[object Object]">
<li class="mb-1 pl-0 [&amp;&gt;ul]:ml-4 [&amp;&gt;ul]:mt-1">Improving understanding of foundation models and AI systems</li>
<li class="mb-1 pl-0 [&amp;&gt;ul]:ml-4 [&amp;&gt;ul]:mt-1">Enhancing controllability and robustness</li>
<li class="mb-1 pl-0 [&amp;&gt;ul]:ml-4 [&amp;&gt;ul]:mt-1">Designing scalable computational methods for reliability while advancing capabilities</li>
</ul>
</li>
</ul>
<p class="mb-4 last:mb-0">I&#x27;ve had the privilege of working closely with many distinguished researchers, including (the late) Prof. <a href="http://www.cs.yale.edu/homes/radev/" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">Dragomir R. Radev</a> at Yale, Prof. <a href="https://ece.illinois.edu/about/directory/faculty/kindrtnk" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">Volodymyr Kindratenko</a> at UIUC, Dr. <a href="https://www.microsoft.com/en-us/research/people/lzhani/" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">Li Lyna Zhang</a> at <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">Microsoft Research Asia</a>, Prof. <a href="https://mbzuai.ac.ae/study/faculty/professor-eric-xing/" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">Eric Xing</a> at CMU, and Prof. <a href="https://sls.csail.mit.edu/people/glass.shtml" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">James Glass</a> at MIT.</p>
<p class="mb-4 last:mb-0">For more information about my research, please see <a href="https://scholar.google.com/citations?hl=en&amp;user=WZ00HCUAAAAJ" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">Google Scholar</a>, <a href="https://www.semanticscholar.org/author/Zhenting-Qi/2186056193" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">Semantic Scholar</a>, or <a href="https://dblp.org/pid/329/2118.html" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">DBLP</a>. Please feel free to reach out at: <code>zhentingqi [at] g [dot] harvard [dot] edu</code>.</p></div></section><section style="opacity:0;transform:translateY(20px)"><h2 class="text-2xl font-serif font-bold text-primary mb-4">News</h2><div class="space-y-3"><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 dark:text-neutral-400 mt-1 w-16 flex-shrink-0">2025-11</span><div class="text-sm text-neutral-700 dark:text-neutral-600 flex-1"><p class="mb-0">Will be joining <strong class="font-semibold text-primary">Meta FAIR</strong> (Menlo Park office) as a Research Intern, working on multi-agent training.</p></div></div><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 dark:text-neutral-400 mt-1 w-16 flex-shrink-0">2025-09</span><div class="text-sm text-neutral-700 dark:text-neutral-600 flex-1"><p class="mb-0">Our paper <a href="https://arxiv.org/abs/2506.16029" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm underline"><em class="italic">EvoLM: In Search of Lost Language Model Training Dynamics</em></a> has been accepted to NeurIPS 2025 (oral).</p></div></div><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 dark:text-neutral-400 mt-1 w-16 flex-shrink-0">2025-05</span><div class="text-sm text-neutral-700 dark:text-neutral-600 flex-1"><p class="mb-0">Our paper <a href="https://arxiv.org/abs/2502.02508" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm underline"><em class="italic">Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search</em></a> has been accepted to ICML 2025.</p></div></div><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 dark:text-neutral-400 mt-1 w-16 flex-shrink-0">2025-05</span><div class="text-sm text-neutral-700 dark:text-neutral-600 flex-1"><p class="mb-0">Will be joining <strong class="font-semibold text-primary">Google DeepMind</strong> (Mountain View office) as a Student Researcher, working on language model post-training.</p></div></div><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 dark:text-neutral-400 mt-1 w-16 flex-shrink-0">2025-04</span><div class="text-sm text-neutral-700 dark:text-neutral-600 flex-1"><p class="mb-0">I will continue my research journey at <strong class="font-semibold text-primary">Harvard</strong> as a PhD student!</p></div></div><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 dark:text-neutral-400 mt-1 w-16 flex-shrink-0">2025-01</span><div class="text-sm text-neutral-700 dark:text-neutral-600 flex-1"><p class="mb-0">Our papers <a href="https://arxiv.org/abs/2408.06195" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm underline"><em class="italic">Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solvers</em></a>, <a href="https://arxiv.org/abs/2410.01769" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm underline"><em class="italic">Quantifying Generalization Complexity for Large Language Models</em></a>, <a href="https://arxiv.org/abs/2402.17840" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm underline"><em class="italic">Follow My Instruction and Spill the Beans: Scalable Data Extraction from Retrieval-Augmented Generation Systems</em></a> have been accepted to ICLR 2025.</p></div></div></div></section><section style="opacity:0;transform:translateY(20px)"><div class="flex items-center justify-between mb-4"><h2 class="text-2xl font-serif font-bold text-primary">Selected Publications</h2><a class="text-accent hover:text-accent-dark text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm" href="/publications/">View All ‚Üí</a></div><div class="space-y-4"><div class="bg-neutral-50 dark:bg-neutral-800 p-4 rounded-lg shadow-sm border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] hover:shadow-lg transition-all duration-200 hover:scale-[1.02]" style="opacity:0;transform:translateY(20px)"><div class="flex items-start justify-between gap-2 mb-2"><h3 class="font-semibold text-primary leading-tight flex-1">EvoLM: In Search of Lost Language Model Training Dynamics</h3><div class="flex flex-wrap gap-1 flex-shrink-0"><span class="inline-block px-2 py-1 rounded border-2 border-accent bg-accent/10 text-accent text-xs font-semibold whitespace-nowrap">üèÜ <!-- -->Oral Presentation</span></div></div><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-1"><span><span class="font-semibold text-accent underline">Zhenting Qi</span>, </span><span><span class="">Fan Nie</span>, </span><span><span class="">Alexandre Alahi</span>, </span><span><span class="">James Zou</span>, </span><span><span class="">Himabindu Lakkaraju</span>, </span><span><span class="">Yilun Du</span>, </span><span><span class="">Eric Xing</span>, </span><span><span class="">Sham Kakade</span>, </span><span><span class="">Hanlin Zhang</span></span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-2">Advances in Neural Information Processing Systems (NeurIPS)<!-- --> <!-- -->2025</p><p class="text-sm text-neutral-500 dark:text-neutral-500 line-clamp-2 mb-3">We developed a comprehensive model suite for analyzing language model training dynamics across pre-training, continued pre-training, supervised fine-tuning, and reinforcement learning stages.</p><div class="flex flex-wrap gap-2 mt-2"><a href="https://arxiv.org/abs/2506.16029" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">arXiv</a><a href="https://arxiv.org/pdf/2506.16029.pdf" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">PDF</a></div></div><div class="bg-neutral-50 dark:bg-neutral-800 p-4 rounded-lg shadow-sm border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] hover:shadow-lg transition-all duration-200 hover:scale-[1.02]" style="opacity:0;transform:translateY(20px)"><div class="flex items-start justify-between gap-2 mb-2"><h3 class="font-semibold text-primary leading-tight flex-1">Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search</h3></div><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-1"><span><span class="">Maohao Shen</span><sup class="ml-0 text-neutral-600 dark:text-neutral-500">‚Ä†</sup>, </span><span><span class="">Guangtao Zeng</span><sup class="ml-0 text-neutral-600 dark:text-neutral-500">‚Ä†</sup>, </span><span><span class="font-semibold text-accent underline">Zhenting Qi</span><sup class="ml-0 text-accent">‚Ä†</sup>, </span><span><span class="">Zhang-Wei Hong</span>, </span><span><span class="">Zhenfang Chen</span>, </span><span><span class="">Wei Lu</span>, </span><span><span class="">Gregory Wornell</span>, </span><span><span class="">Subhro Das</span>, </span><span><span class="">David Cox</span>, </span><span><span class="">Chuang Gan</span></span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-2">International Conference on Machine Learning (ICML)<!-- --> <!-- -->2025</p><p class="text-sm text-neutral-500 dark:text-neutral-500 line-clamp-2 mb-3">We introduced the COAT reasoning framework to enhance LLM reasoning via autoregressive search with self-reflection and self-exploration.</p><div class="flex flex-wrap gap-2 mt-2"><a href="https://arxiv.org/abs/2502.02508" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">arXiv</a><a href="https://arxiv.org/pdf/2502.02508.pdf" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">PDF</a></div></div><div class="bg-neutral-50 dark:bg-neutral-800 p-4 rounded-lg shadow-sm border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] hover:shadow-lg transition-all duration-200 hover:scale-[1.02]" style="opacity:0;transform:translateY(20px)"><div class="flex items-start justify-between gap-2 mb-2"><h3 class="font-semibold text-primary leading-tight flex-1">rStar: Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solvers</h3></div><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-1"><span><span class="font-semibold text-accent underline">Zhenting Qi</span><sup class="ml-0 text-accent">‚Ä†</sup>, </span><span><span class="">Mingyuan Ma</span><sup class="ml-0 text-neutral-600 dark:text-neutral-500">‚Ä†</sup>, </span><span><span class="">Jiahang Xu</span><sup class="ml-0 text-neutral-600 dark:text-neutral-500">‚Ä†</sup>, </span><span><span class="">Li Lyna Zhang</span>, </span><span><span class="">Fan Yang</span>, </span><span><span class="">Mao Yang</span></span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-2">International Conference on Learning Representations (ICLR)<!-- --> <!-- -->2025</p><p class="text-sm text-neutral-500 dark:text-neutral-500 line-clamp-2 mb-3">We introduced rStar, a self-play mutual reasoning approach that enhances reasoning capabilities of small language models without fine-tuning or superior models.</p><div class="flex flex-wrap gap-2 mt-2"><a href="https://arxiv.org/abs/2408.06195" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">arXiv</a><a href="https://arxiv.org/pdf/2408.06195.pdf" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">PDF</a></div></div><div class="bg-neutral-50 dark:bg-neutral-800 p-4 rounded-lg shadow-sm border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] hover:shadow-lg transition-all duration-200 hover:scale-[1.02]" style="opacity:0;transform:translateY(20px)"><div class="flex items-start justify-between gap-2 mb-2"><h3 class="font-semibold text-primary leading-tight flex-1">Follow My Instruction and Spill the Beans: Scalable Data Extraction from Retrieval-Augmented Generation Systems</h3></div><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-1"><span><span class="font-semibold text-accent underline">Zhenting Qi</span>, </span><span><span class="">Hanlin Zhang</span>, </span><span><span class="">Eric Xing</span>, </span><span><span class="">Sham Kakade</span>, </span><span><span class="">Himabindu Lakkaraju</span></span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-2">International Conference on Learning Representations (ICLR)<!-- --> <!-- -->2025</p><p class="text-sm text-neutral-500 dark:text-neutral-500 line-clamp-2 mb-3">We developed a scalable method for extracting data from RAG systems using LLMs&#x27; instruction-following capabilities.</p><div class="flex flex-wrap gap-2 mt-2"><a href="https://arxiv.org/abs/2402.17840" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">arXiv</a><a href="https://arxiv.org/pdf/2402.17840.pdf" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">PDF</a></div></div><div class="bg-neutral-50 dark:bg-neutral-800 p-4 rounded-lg shadow-sm border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] hover:shadow-lg transition-all duration-200 hover:scale-[1.02]" style="opacity:0;transform:translateY(20px)"><div class="flex items-start justify-between gap-2 mb-2"><h3 class="font-semibold text-primary leading-tight flex-1">Quantifying Generalization Complexity for Large Language Models</h3></div><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-1"><span><span class="font-semibold text-accent underline">Zhenting Qi</span>, </span><span><span class="">Hongyin Luo</span>, </span><span><span class="">Xuliang Huang</span>, </span><span><span class="">Zhuokai Zhao</span>, </span><span><span class="">Yibo Jiang</span>, </span><span><span class="">Xiangjun Fan</span>, </span><span><span class="">Himabindu Lakkaraju</span>, </span><span><span class="">James Glass</span></span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-2">International Conference on Learning Representations (ICLR)<!-- --> <!-- -->2025</p><p class="text-sm text-neutral-500 dark:text-neutral-500 line-clamp-2 mb-3">We introduced Scylla, a dynamic evaluation framework that quantitatively measures LLMs&#x27; generalization abilities by disentangling generalization from memorization.</p><div class="flex flex-wrap gap-2 mt-2"><a href="https://arxiv.org/abs/2410.01769" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">arXiv</a><a href="https://arxiv.org/pdf/2410.01769.pdf" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">PDF</a></div></div></div></section></section></div></div></div><!--$--><!--/$--><!--$--><!--/$--></main><footer class="border-t border-neutral-200/50 bg-neutral-50/50 dark:bg-neutral-900/50 dark:border-neutral-700/50"><div class="max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-6"><div class="flex flex-col sm:flex-row justify-between items-center gap-2"><p class="text-xs text-neutral-500">Last updated: <!-- -->December 1, 2025</p><p class="text-xs text-neutral-500 flex items-center"><a href="https://github.com/xyjoey/PRISM" target="_blank" rel="noopener noreferrer">Built with PRISM</a><span class="ml-2">üöÄ</span></p></div></div></footer></div><script src="/_next/static/chunks/webpack-679b75d1c4c2027c.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[3719,[\"457\",\"static/chunks/457-8b8d97145837e580.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"862\",\"static/chunks/862-15038af301665bb3.js\",\"177\",\"static/chunks/app/layout-d7cbb4646de1c3b3.js\"],\"ThemeProvider\"]\n3:I[768,[\"457\",\"static/chunks/457-8b8d97145837e580.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"862\",\"static/chunks/862-15038af301665bb3.js\",\"177\",\"static/chunks/app/layout-d7cbb4646de1c3b3.js\"],\"default\"]\n4:I[7555,[],\"\"]\n5:I[1295,[],\"\"]\n6:I[2548,[\"457\",\"static/chunks/457-8b8d97145837e580.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"862\",\"static/chunks/862-15038af301665bb3.js\",\"177\",\"static/chunks/app/layout-d7cbb4646de1c3b3.js\"],\"default\"]\n7:I[2637,[\"457\",\"static/chunks/457-8b8d97145837e580.js\",\"485\",\"static/chunks/485-487001f78a0503db.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"748\",\"static/chunks/748-c442066f2b36fa6c.js\",\"974\",\"static/chunks/app/page-1e259e5be13ce12d.js\"],\"default\"]\n8:I[9507,[\"457\",\"static/chunks/457-8b8d97145837e580.js\",\"485\",\"static/chunks/485-487001f78a0503db.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"748\",\"static/chunks/748-c442066f2b36fa6c.js\",\"974\",\"static/chunks/app/page-1e259e5be13ce12d.js\"],\"default\"]\na:I[1990,[\"457\",\"static/chunks/457-8b8d97145837e580.js\",\"485\",\"static/chunks/485-487001f78a0503db.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"748\",\"static/chunks/748-c442066f2b36fa6c.js\",\"974\",\"static/chunks/app/page-1e259e5be13ce12d.js\"],\"default\"]\nb:I[5218,[\"457\",\"static/chunks/457-8b8d97145837e580.js\",\"485\",\"static/chunks/485-487001f78a0503db.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"748\",\"static/chunks/748-c442066f2b36fa6c.js\",\"974\",\"static/chunks/app/page-1e259e5be13ce12d.js\"],\"default\"]\n14:I[9665,[],\"MetadataBoundary\"]\n16:I[9665,[],\"OutletBoundary\"]\n19:I[4911,[],\"AsyncMetadataOutlet\"]\n1b:I[9665,[],\"ViewportBoundary\"]\n1d:I[6614,[],\"\"]\n:HL[\"/_next/static/css/66c6e803a86cbc2f.css\",\"style\"]\n9:T740,Welcome! I am a 1st year Computer Science Ph.D. student at [Harvard University](https://www.harvard.edu/), where"])</script><script>self.__next_f.push([1," I am honored to be co-advised by Prof. [Yilun Du](https://yilundu.github.io/) and Prof. [Hima Lakkaraju](https://himalakkaraju.github.io/). \n\nMy research centers around developing intelligent and reliable AI systems that benefit human society. Motivated by this, I am generally interested in the following topics (w/o particular order):\n\n- **Reasoning**\n    - Understanding and enhancing reasoning capabilities in foundation models\n    - Developing AI systems that generalize effectively to OOD scenarios\n    - Training (multi-)agents for compositional reasoning tasks\n\n- **Reliability**\n    - Improving understanding of foundation models and AI systems\n    - Enhancing controllability and robustness\n    - Designing scalable computational methods for reliability while advancing capabilities\n\nI've had the privilege of working closely with many distinguished researchers, including (the late) Prof. [Dragomir R. Radev](http://www.cs.yale.edu/homes/radev/) at Yale, Prof. [Volodymyr Kindratenko](https://ece.illinois.edu/about/directory/faculty/kindrtnk) at UIUC, Dr. [Li Lyna Zhang](https://www.microsoft.com/en-us/research/people/lzhani/) at [Microsoft Research Asia](https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/), Prof. [Eric Xing](https://mbzuai.ac.ae/study/faculty/professor-eric-xing/) at CMU, and Prof. [James Glass](https://sls.csail.mit.edu/people/glass.shtml) at MIT.\n\nFor more information about my research, please see [Google Scholar](https://scholar.google.com/citations?hl=en\u0026user=WZ00HCUAAAAJ), [Semantic Scholar](https://www.semanticscholar.org/author/Zhenting-Qi/2186056193), or [DBLP](https://dblp.org/pid/329/2118.html). Please feel free to reach out at: `zhentingqi [at] g [dot] harvard [dot] edu`.c:T4a8,Modern language model (LM) training has been divided into multiple stages, making it difficult for downstream developers to evaluate the impact of design choices made at each stage. We present EvoLM, a model suite that enables systematic and transparent analysis of LMs' training dynamics across p"])</script><script>self.__next_f.push([1,"re-training, continued pre-training, supervised fine-tuning, and reinforcement learning. We train over 100 LMs with 1B and 4B parameters from scratch, and evaluate both upstream (language modeling) and downstream (problem-solving) capabilities, including considerations of both in-domain and out-of-domain generalization. Key insights highlight the diminishing returns from excessive pre-training and post-training, the importance and practices of mitigating forgetting during domain-specific continued pre-training, the crucial role of continued pre-training in bridging pre-training and post-training phases, and various intricate trade-offs when configuring supervised fine-tuning and reinforcement learning. To facilitate open research and reproducibility, we release all pre-trained and post-trained models, training datasets for all stages, and our entire training and evaluation pipeline.d:T664,@inproceedings{evolm2025,\n  title = {EvoLM: In Search of Lost Language Model Training Dynamics},\n  author = {Zhenting Qi and Fan Nie and Alexandre Alahi and James Zou and Himabindu Lakkaraju and Yilun Du and Eric Xing and Sham Kakade and Hanlin Zhang},\n  year = {2025},\n  month = {dec},\n  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},\n  note = {Oral},\n  abstract = {Modern language model (LM) training has been divided into multiple stages, making it difficult for downstream developers to evaluate the impact of design choices made at each stage. We present EvoLM, a model suite that enables systematic and transparent analysis of LMs' training dynamics across pre-training, continued pre-training, supervised fine-tuning, and reinforcement learning. We train over 100 LMs with 1B and 4B parameters from scratch, and evaluate both upstream (language modeling) and downstream (problem-solving) capabilities, including considerations of both in-domain and out-of-domain generalization. Key insights highlight the diminishing returns from excessive pre-training and post-training, the importance and practices of mitiga"])</script><script>self.__next_f.push([1,"ting forgetting during domain-specific continued pre-training, the crucial role of continued pre-training in bridging pre-training and post-training phases, and various intricate trade-offs when configuring supervised fine-tuning and reinforcement learning. To facilitate open research and reproducibility, we release all pre-trained and post-trained models, training datasets for all stages, and our entire training and evaluation pipeline.},\n  arxiv = {https://arxiv.org/abs/2506.16029}\n}e:T57b,Large language models (LLMs) have demonstrated remarkable reasoning capabilities across diverse domains. Recent studies have shown that increasing test-time computation enhances LLMs' reasoning capabilities. This typically involves extensive sampling at inference time guided by an external LLM verifier, resulting in a two-player system. Despite external guidance, the effectiveness of this system demonstrates the potential of a single LLM to tackle complex tasks. Thus, we pose a new research problem: Can we internalize the searching capabilities to fundamentally enhance the reasoning abilities of a single LLM? This work explores an orthogonal direction focusing on post-training LLMs for autoregressive searching (i.e., an extended reasoning process with self-reflection and self-exploration of new strategies). To achieve this, we propose the Chain-of-Action-Thought (COAT) reasoning and a two-stage training paradigm: 1) a small-scale format tuning stage to internalize the COAT reasoning format and 2) a large-scale self-improvement stage leveraging reinforcement learning. Our approach results in Satori, a 7B LLM trained on open-source models and data. Extensive empirical evaluations demonstrate that Satori achieves state-of-the-art performance on mathematical reasoning benchmarks while exhibits strong generalization to out-of-domain tasks. Code, data, and models are fully open-sourced.f:T762,@inproceedings{satori2025,\n  title = {Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive S"])</script><script>self.__next_f.push([1,"earch},\n  author = {Maohao Shen and Guangtao Zeng and Zhenting Qi and Zhang-Wei Hong and Zhenfang Chen and Wei Lu and Gregory Wornell and Subhro Das and David Cox and Chuang Gan},\n  year = {2025},\n  month = {jul},\n  booktitle = {International Conference on Machine Learning (ICML)},\n  abstract = {Large language models (LLMs) have demonstrated remarkable reasoning capabilities across diverse domains. Recent studies have shown that increasing test-time computation enhances LLMs' reasoning capabilities. This typically involves extensive sampling at inference time guided by an external LLM verifier, resulting in a two-player system. Despite external guidance, the effectiveness of this system demonstrates the potential of a single LLM to tackle complex tasks. Thus, we pose a new research problem: Can we internalize the searching capabilities to fundamentally enhance the reasoning abilities of a single LLM? This work explores an orthogonal direction focusing on post-training LLMs for autoregressive searching (i.e., an extended reasoning process with self-reflection and self-exploration of new strategies). To achieve this, we propose the Chain-of-Action-Thought (COAT) reasoning and a two-stage training paradigm: 1) a small-scale format tuning stage to internalize the COAT reasoning format and 2) a large-scale self-improvement stage leveraging reinforcement learning. Our approach results in Satori, a 7B LLM trained on open-source models and data. Extensive empirical evaluations demonstrate that Satori achieves state-of-the-art performance on mathematical reasoning benchmarks while exhibits strong generalization to out-of-domain tasks. Code, data, and models are fully open-sourced.},\n  arxiv = {https://arxiv.org/abs/2502.02508}\n}10:T589,@inproceedings{mutual-reasoning2025,\n  title = {rStar: Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solvers},\n  author = {Zhenting Qi and Mingyuan Ma and Jiahang Xu and Li Lyna Zhang and Fan Yang and Mao Yang},\n  year = {2025},\n  month = {may},\n  booktitle = {International Conferenc"])</script><script>self.__next_f.push([1,"e on Learning Representations (ICLR)},\n  abstract = {This paper introduces rStar, a self-play mutual reasoning approach that significantly improves reasoning capabilities of small language models (SLMs) without fine-tuning or superior models. rStar decouples reasoning into a self-play mutual generation-discrimination process. First, a target SLM augments the Monte Carlo Tree Search (MCTS) with a rich set of human-like reasoning actions to construct higher quality reasoning trajectories. Next, another SLM, with capabilities similar to the target SLM, acts as a discriminator to verify each trajectory generated by the target SLM. The mutually agreed reasoning trajectories are considered mutual consistent, thus are more likely to be correct. Extensive experiments across five SLMs demonstrate rStar can effectively solve diverse reasoning problems, including GSM8K, GSM-Hard, MATH, SVAMP, and StrategyQA. Remarkably, rStar boosts GSM8K accuracy from 12.51\\% to 63.91\\% for LLaMA2-7B, from 36.46\\% to 81.88\\% for Mistral-7B, from 74.53\\% to 91.13\\% for LLaMA3-8B-Instruct.},\n  arxiv = {https://arxiv.org/abs/2408.06195}\n}11:T4ab,@inproceedings{data-extraction-rag2025,\n  title = {Follow My Instruction and Spill the Beans: Scalable Data Extraction from Retrieval-Augmented Generation Systems},\n  author = {Zhenting Qi and Hanlin Zhang and Eric Xing and Sham Kakade and Himabindu Lakkaraju},\n  year = {2025},\n  month = {may},\n  booktitle = {International Conference on Learning Representations (ICLR)},\n  abstract = {This paper presents a novel approach to scalable data extraction from retrieval-augmented generation (RAG) systems. By leveraging instruction-following capabilities of large language models, we design a framework that systematically generates prompts to guide LLMs in revealing embedded information, enabling scalable data extraction without direct access to the underlying retrieval mechanisms. Our method demonstrates effectiveness across various RAG architectures, highlighting potential vulnerabilities and emphasizing the "])</script><script>self.__next_f.push([1,"need for robust data protection strategies in LLM applications. Through extensive experiments, we show that our approach can efficiently extract valuable information from RAG systems, raising important security and privacy concerns.},\n  arxiv = {https://arxiv.org/abs/2402.17840}\n}12:T5e5,While large language models (LLMs) have shown exceptional capabilities in understanding complex queries and performing sophisticated tasks, their generalization abilities are often deeply entangled with memorization, necessitating more precise evaluation. To address this challenge, we introduce Scylla, a dynamic evaluation framework that quantitatively measures the generalization abilities of LLMs. Scylla disentangles generalization from memorization via assessing model performance on both in-distribution (ID) and out-of-distribution (OOD) data through 20 tasks across 5 levels of complexity. Through extensive experiments, we uncover a non-monotonic relationship between task complexity and the performance gap between ID and OOD data, which we term the generalization valley. Specifically, this phenomenon reveals a critical threshold - referred to as critical complexity - where reliance on non-generalizable behavior peaks, indicating the upper bound of LLMs' generalization capabilities. As model size increases, the critical complexity shifts toward higher levels of task complexity, suggesting that larger models can handle more complex reasoning tasks before over-relying on memorization. Leveraging Scylla and the concept of critical complexity, we benchmark 28 LLMs including both open-sourced models such as LLaMA and Qwen families, and close-sourced models like Claude and GPT, providing a more robust evaluation and establishing a clearer understanding of LLMs' generalization capabilities.13:T7a4,@inproceedings{quantifying-generalization2025,\n  title = {Quantifying Generalization Complexity for Large Language Models},\n  author = {Zhenting Qi and Hongyin Luo and Xuliang Huang and Zhuokai Zhao and Yibo Jiang and Xiangjun Fan and Himabin"])</script><script>self.__next_f.push([1,"du Lakkaraju and James Glass},\n  year = {2025},\n  month = {may},\n  booktitle = {International Conference on Learning Representations (ICLR)},\n  abstract = {While large language models (LLMs) have shown exceptional capabilities in understanding complex queries and performing sophisticated tasks, their generalization abilities are often deeply entangled with memorization, necessitating more precise evaluation. To address this challenge, we introduce Scylla, a dynamic evaluation framework that quantitatively measures the generalization abilities of LLMs. Scylla disentangles generalization from memorization via assessing model performance on both in-distribution (ID) and out-of-distribution (OOD) data through 20 tasks across 5 levels of complexity. Through extensive experiments, we uncover a non-monotonic relationship between task complexity and the performance gap between ID and OOD data, which we term the generalization valley. Specifically, this phenomenon reveals a critical threshold - referred to as critical complexity - where reliance on non-generalizable behavior peaks, indicating the upper bound of LLMs' generalization capabilities. As model size increases, the critical complexity shifts toward higher levels of task complexity, suggesting that larger models can handle more complex reasoning tasks before over-relying on memorization. Leveraging Scylla and the concept of critical complexity, we benchmark 28 LLMs including both open-sourced models such as LLaMA and Qwen families, and close-sourced models like Claude and GPT, providing a more robust evaluation and establishing a clearer understanding of LLMs' generalization capabilities.},\n  arxiv = {https://arxiv.org/abs/2410.01769}\n}"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"q949WACoPgBxPEnVJjdV8\",\"p\":\"\",\"c\":[\"\",\"\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"__PAGE__\",{}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/66c6e803a86cbc2f.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"scroll-smooth\",\"suppressHydrationWarning\":true,\"children\":[[\"$\",\"head\",null,{\"children\":[[\"$\",\"link\",null,{\"rel\":\"icon\",\"href\":\"/favicon.svg\",\"type\":\"image/svg+xml\"}],[\"$\",\"link\",null,{\"rel\":\"dns-prefetch\",\"href\":\"https://google-fonts.jialeliu.com\"}],[\"$\",\"link\",null,{\"rel\":\"preconnect\",\"href\":\"https://google-fonts.jialeliu.com\",\"crossOrigin\":\"\"}],[\"$\",\"link\",null,{\"rel\":\"preload\",\"as\":\"style\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\"}],[\"$\",\"link\",null,{\"rel\":\"stylesheet\",\"id\":\"gfonts-css\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\",\"media\":\"print\"}],[\"$\",\"script\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"\\n              (function(){\\n                var l = document.getElementById('gfonts-css');\\n                if (!l) return;\\n                if (l.media !== 'all') {\\n                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });\\n                }\\n              })();\\n            \"}}],[\"$\",\"noscript\",null,{\"children\":[\"$\",\"link\",null,{\"rel\":\"stylesheet\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\"}]}],[\"$\",\"script\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"\\n              try {\\n                const theme = localStorage.getItem('theme-storage');\\n                const parsed = theme ? JSON.parse(theme) : null;\\n                const setting = parsed?.state?.theme || 'system';\\n                const prefersDark = typeof window !== 'undefined' \u0026\u0026 window.matchMedia \u0026\u0026 window.matchMedia('(prefers-color-scheme: dark)').matches;\\n                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));\\n                var root = document.documentElement;\\n                root.classList.add(effective);\\n                root.setAttribute('data-theme', effective);\\n              } catch (e) {\\n                var root = document.documentElement;\\n                root.classList.add('light');\\n                root.setAttribute('data-theme', 'light');\\n              }\\n            \"}}]]}],[\"$\",\"body\",null,{\"className\":\"font-sans antialiased\",\"children\":[\"$\",\"$L2\",null,{\"children\":[[\"$\",\"$L3\",null,{\"items\":[{\"title\":\"About\",\"type\":\"page\",\"target\":\"about\",\"href\":\"/\"},{\"title\":\"Publications\",\"type\":\"page\",\"target\":\"publications\",\"href\":\"/publications\"}],\"siteTitle\":\"Zhenting Qi\",\"enableOnePageMode\":false}],[\"$\",\"main\",null,{\"className\":\"min-h-screen pt-16 lg:pt-20\",\"children\":[\"$\",\"$L4\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L5\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}],[\"$\",\"$L6\",null,{\"lastUpdated\":\"December 1, 2025\"}]]}]}]]}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[[\"$\",\"div\",null,{\"className\":\"max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-8 bg-background min-h-screen\",\"children\":[\"$\",\"div\",null,{\"className\":\"grid grid-cols-1 lg:grid-cols-3 gap-12\",\"children\":[[\"$\",\"div\",null,{\"className\":\"lg:col-span-1\",\"children\":[\"$\",\"$L7\",null,{\"author\":{\"name\":\"Zhenting Qi\",\"title\":\"Ph.D. Student in Computer Science\",\"institution\":\"Harvard University\",\"avatar\":\"/bio.jpg\"},\"social\":{\"email\":\"zhentingqi@g.harvard.edu\",\"location\":\"Cambridge, MA, USA\",\"location_url\":\"https://maps.google.com\",\"location_details\":[\"Harvard University\",\"Cambridge, MA, USA\"],\"google_scholar\":\"https://scholar.google.com/citations?hl=en\u0026user=WZ00HCUAAAAJ\",\"orcid\":\"\",\"github\":\"https://github.com/zhentingqi\",\"linkedin\":\"https://www.linkedin.com/in/zhentingqi\",\"instagram\":\"https://www.instagram.com/heptacol/\"}}]}],[\"$\",\"div\",null,{\"className\":\"lg:col-span-2 space-y-8\",\"children\":[[\"$\",\"section\",\"about\",{\"id\":\"about\",\"className\":\"scroll-mt-24 space-y-8\",\"children\":[[[\"$\",\"$L8\",\"about\",{\"content\":\"$9\",\"title\":\"About\"}],[\"$\",\"$La\",\"news\",{\"items\":[{\"date\":\"2025-11\",\"content\":\"Will be joining **Meta FAIR** (Menlo Park office) as a Research Intern, working on multi-agent training.\"},{\"date\":\"2025-09\",\"content\":\"Our paper [_EvoLM: In Search of Lost Language Model Training Dynamics_](https://arxiv.org/abs/2506.16029) has been accepted to NeurIPS 2025 (oral).\"},{\"date\":\"2025-05\",\"content\":\"Our paper [_Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search_](https://arxiv.org/abs/2502.02508) has been accepted to ICML 2025.\"},{\"date\":\"2025-05\",\"content\":\"Will be joining **Google DeepMind** (Mountain View office) as a Student Researcher, working on language model post-training.\"},{\"date\":\"2025-04\",\"content\":\"I will continue my research journey at **Harvard** as a PhD student!\"},{\"date\":\"2025-01\",\"content\":\"Our papers [_Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solvers_](https://arxiv.org/abs/2408.06195), [_Quantifying Generalization Complexity for Large Language Models_](https://arxiv.org/abs/2410.01769), [_Follow My Instruction and Spill the Beans: Scalable Data Extraction from Retrieval-Augmented Generation Systems_](https://arxiv.org/abs/2402.17840) have been accepted to ICLR 2025.\"}],\"title\":\"News\"}],[\"$\",\"$Lb\",\"featured_publications\",{\"publications\":[{\"id\":\"evolm2025\",\"title\":\"EvoLM: In Search of Lost Language Model Training Dynamics\",\"authors\":[{\"name\":\"Zhenting Qi\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Fan Nie\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Alexandre Alahi\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"James Zou\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Himabindu Lakkaraju\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yilun Du\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Eric Xing\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Sham Kakade\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Hanlin Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2025,\"month\":\"12\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:2:props:publications:0:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"Advances in Neural Information Processing Systems (NeurIPS)\",\"abstract\":\"$c\",\"description\":\"We developed a comprehensive model suite for analyzing language model training dynamics across pre-training, continued pre-training, supervised fine-tuning, and reinforcement learning stages.\",\"selected\":true,\"awards\":[\"Oral Presentation\"],\"arxivId\":\"2506.16029\",\"url\":\"https://arxiv.org/abs/2506.16029\",\"pdfUrl\":\"https://arxiv.org/pdf/2506.16029.pdf\",\"bibtex\":\"$d\"},{\"id\":\"satori2025\",\"title\":\"Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search\",\"authors\":[{\"name\":\"Maohao Shen\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Guangtao Zeng\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Zhenting Qi\",\"isHighlighted\":true,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Zhang-Wei Hong\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhenfang Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Wei Lu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Gregory Wornell\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Subhro Das\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"David Cox\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chuang Gan\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2025,\"month\":\"7\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:2:props:publications:1:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"International Conference on Machine Learning (ICML)\",\"abstract\":\"$e\",\"description\":\"We introduced the COAT reasoning framework to enhance LLM reasoning via autoregressive search with self-reflection and self-exploration.\",\"selected\":true,\"arxivId\":\"2502.02508\",\"url\":\"https://arxiv.org/abs/2502.02508\",\"pdfUrl\":\"https://arxiv.org/pdf/2502.02508.pdf\",\"bibtex\":\"$f\"},{\"id\":\"mutual-reasoning2025\",\"title\":\"rStar: Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solvers\",\"authors\":[{\"name\":\"Zhenting Qi\",\"isHighlighted\":true,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Mingyuan Ma\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Jiahang Xu\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Li Lyna Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Fan Yang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Mao Yang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2025,\"month\":\"5\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:2:props:publications:2:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"International Conference on Learning Representations (ICLR)\",\"abstract\":\"This paper introduces rStar, a self-play mutual reasoning approach that significantly improves reasoning capabilities of small language models (SLMs) without fine-tuning or superior models. rStar decouples reasoning into a self-play mutual generation-discrimination process. First, a target SLM augments the Monte Carlo Tree Search (MCTS) with a rich set of human-like reasoning actions to construct higher quality reasoning trajectories. Next, another SLM, with capabilities similar to the target SLM, acts as a discriminator to verify each trajectory generated by the target SLM. The mutually agreed reasoning trajectories are considered mutual consistent, thus are more likely to be correct. Extensive experiments across five SLMs demonstrate rStar can effectively solve diverse reasoning problems, including GSM8K, GSM-Hard, MATH, SVAMP, and StrategyQA. Remarkably, rStar boosts GSM8K accuracy from 12.51% to 63.91% for LLaMA2-7B, from 36.46% to 81.88% for Mistral-7B, from 74.53% to 91.13% for LLaMA3-8B-Instruct.\",\"description\":\"We introduced rStar, a self-play mutual reasoning approach that enhances reasoning capabilities of small language models without fine-tuning or superior models.\",\"selected\":true,\"arxivId\":\"2408.06195\",\"url\":\"https://arxiv.org/abs/2408.06195\",\"pdfUrl\":\"https://arxiv.org/pdf/2408.06195.pdf\",\"bibtex\":\"$10\"},{\"id\":\"data-extraction-rag2025\",\"title\":\"Follow My Instruction and Spill the Beans: Scalable Data Extraction from Retrieval-Augmented Generation Systems\",\"authors\":[{\"name\":\"Zhenting Qi\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Hanlin Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Eric Xing\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Sham Kakade\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Himabindu Lakkaraju\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2025,\"month\":\"5\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:2:props:publications:3:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"International Conference on Learning Representations (ICLR)\",\"abstract\":\"This paper presents a novel approach to scalable data extraction from retrieval-augmented generation (RAG) systems. By leveraging instruction-following capabilities of large language models, we design a framework that systematically generates prompts to guide LLMs in revealing embedded information, enabling scalable data extraction without direct access to the underlying retrieval mechanisms. Our method demonstrates effectiveness across various RAG architectures, highlighting potential vulnerabilities and emphasizing the need for robust data protection strategies in LLM applications. Through extensive experiments, we show that our approach can efficiently extract valuable information from RAG systems, raising important security and privacy concerns.\",\"description\":\"We developed a scalable method for extracting data from RAG systems using LLMs' instruction-following capabilities.\",\"selected\":true,\"arxivId\":\"2402.17840\",\"url\":\"https://arxiv.org/abs/2402.17840\",\"pdfUrl\":\"https://arxiv.org/pdf/2402.17840.pdf\",\"bibtex\":\"$11\"},{\"id\":\"quantifying-generalization2025\",\"title\":\"Quantifying Generalization Complexity for Large Language Models\",\"authors\":[{\"name\":\"Zhenting Qi\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Hongyin Luo\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xuliang Huang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhuokai Zhao\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yibo Jiang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xiangjun Fan\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Himabindu Lakkaraju\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"James Glass\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2025,\"month\":\"5\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:2:props:publications:4:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"International Conference on Learning Representations (ICLR)\",\"abstract\":\"$12\",\"description\":\"We introduced Scylla, a dynamic evaluation framework that quantitatively measures LLMs' generalization abilities by disentangling generalization from memorization.\",\"selected\":true,\"arxivId\":\"2410.01769\",\"url\":\"https://arxiv.org/abs/2410.01769\",\"pdfUrl\":\"https://arxiv.org/pdf/2410.01769.pdf\",\"bibtex\":\"$13\"}],\"title\":\"Selected Publications\",\"enableOnePageMode\":false}]],false,false,false]}]]}]]}]}],[\"$\",\"$L14\",null,{\"children\":\"$L15\"}],null,[\"$\",\"$L16\",null,{\"children\":[\"$L17\",\"$L18\",[\"$\",\"$L19\",null,{\"promise\":\"$@1a\"}]]}]]}],{},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$1\",\"rwMGY3ORfJeA7EJx-dFx1\",{\"children\":[[\"$\",\"$L1b\",null,{\"children\":\"$L1c\"}],null]}],null]}],false]],\"m\":\"$undefined\",\"G\":[\"$1d\",\"$undefined\"],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"1e:\"$Sreact.suspense\"\n1f:I[4911,[],\"AsyncMetadata\"]\n15:[\"$\",\"$1e\",null,{\"fallback\":null,\"children\":[\"$\",\"$L1f\",null,{\"promise\":\"$@20\"}]}]\n"])</script><script>self.__next_f.push([1,"18:null\n"])</script><script>self.__next_f.push([1,"1c:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n17:null\n"])</script><script>self.__next_f.push([1,"20:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"Zhenting Qi\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Computer Science Ph.D. student at Harvard University, researching intelligent and reliable AI systems\"}],[\"$\",\"meta\",\"2\",{\"name\":\"author\",\"content\":\"Zhenting Qi\"}],[\"$\",\"meta\",\"3\",{\"name\":\"keywords\",\"content\":\"Zhenting Qi,PhD,Research,Harvard University\"}],[\"$\",\"meta\",\"4\",{\"name\":\"creator\",\"content\":\"Zhenting Qi\"}],[\"$\",\"meta\",\"5\",{\"name\":\"publisher\",\"content\":\"Zhenting Qi\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:title\",\"content\":\"Zhenting Qi\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:description\",\"content\":\"Computer Science Ph.D. student at Harvard University, researching intelligent and reliable AI systems\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:site_name\",\"content\":\"Zhenting Qi's Academic Website\"}],[\"$\",\"meta\",\"9\",{\"property\":\"og:locale\",\"content\":\"en_US\"}],[\"$\",\"meta\",\"10\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"11\",{\"name\":\"twitter:card\",\"content\":\"summary\"}],[\"$\",\"meta\",\"12\",{\"name\":\"twitter:title\",\"content\":\"Zhenting Qi\"}],[\"$\",\"meta\",\"13\",{\"name\":\"twitter:description\",\"content\":\"Computer Science Ph.D. student at Harvard University, researching intelligent and reliable AI systems\"}],[\"$\",\"link\",\"14\",{\"rel\":\"icon\",\"href\":\"/favicon.svg\"}]],\"error\":null,\"digest\":\"$undefined\"}\n1a:{\"metadata\":\"$20:metadata\",\"error\":null,\"digest\":\"$undefined\"}\n"])</script></body></html>