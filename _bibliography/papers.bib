---
---

@string{aps = {American Physical Society,}}

@article{shen2025satori,
  title={Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search},
  author={Shen*, Maohao and Zeng*, Guangtao and Qi*, Zhenting and Hong, Zhang-Wei and Chen, Zhenfang and Lu, Wei and Wornell, Gregory and Das, Subhro and Cox, David and Gan, Chuang},
  journal={In the Forty-Second International Conference on Machine Learning (ICML)},
  url={https://arxiv.org/abs/2502.02508},
  pdf={https://arxiv.org/pdf/2502.02508},
  google_scholar_id={MXK_kJrjxJIC},
  abstract={Large language models (LLMs) have demonstrated remarkable reasoning capabilities across diverse domains. Recent studies have shown that increasing test-time computation enhances LLMs' reasoning capabilities. This typically involves extensive sampling at inference time guided by an external LLM verifier, resulting in a two-player system. Despite external guidance, the effectiveness of this system demonstrates the potential of a single LLM to tackle complex tasks. Thus, we pose a new research problem: Can we internalize the searching capabilities to fundamentally enhance the reasoning abilities of a single LLM? This work explores an orthogonal direction focusing on post-training LLMs for autoregressive searching (i.e., an extended reasoning process with self-reflection and self-exploration of new strategies). To achieve this, we propose the Chain-of-Action-Thought (COAT) reasoning and a two-stage training paradigm: 1) a small-scale format tuning stage to internalize the COAT reasoning format and 2) a large-scale self-improvement stage leveraging reinforcement learning. Our approach results in Satori, a 7B LLM trained on open-source models and data. Extensive empirical evaluations demonstrate that Satori achieves state-of-the-art performance on mathematical reasoning benchmarks while exhibits strong generalization to out-of-domain tasks. Code, data, and models will be fully open-sourced.},
  selected={true},
  preview={satori.png},
  year={2025}
}

@inproceedings{qi2025mutual,
  selected = {true},
  title={rStar: Mutual Reasoning Makes Smaller {LLM}s Stronger Problem-Solver},
  author={Zhenting Qi* and Mingyuan Ma* and Jiahang Xu* and Li Lyna Zhang and Fan Yang and Mao Yang},
  booktitle={The Thirteenth International Conference on Learning Representations (ICLR)},
  year={2025},
  url={https://openreview.net/forum?id=6aHUmotXaw},
  pdf={https://arxiv.org/pdf/2408.06195},
  preview={rstar.png},
  google_scholar_id={roLk4NBRz8UC},
  code={https://github.com/zhentingqi/rStar},
  arxiv={2408.06195},
  abstract={This paper introduces rStar, a self-play mutual reasoning approach that significantly improves reasoning capabilities of small language models (SLMs) without fine-tuning or superior models. rStar decouples reasoning into a self-play mutual generation-discrimination process. First, a target SLM augments the Monte Carlo Tree Search (MCTS) with a rich set of human-like reasoning actions to construct higher quality reasoning trajectories. Next, another SLM, with capabilities similar to the target SLM, acts as a discriminator to verify each trajectory generated by the target SLM. The mutually agreed reasoning trajectories are considered mutual consistent, thus are more likely to be correct. Extensive experiments across five SLMs demonstrate rStar can effectively solve diverse reasoning problems, including GSM8K, GSM-Hard, MATH, SVAMP, and StrategyQA. Remarkably, rStar boosts GSM8K accuracy from 12.51% to 63.91% for LLaMA2-7B, from 36.46% to 81.88% for Mistral-7B, from 74.53% to 91.13% for LLaMA3-8B-Instruct.}
}

@inproceedings{qi2025quantifying,
  selected = {true},
  title={Quantifying Generalization Complexity for Large Language Models},
  author={Zhenting Qi and Hongyin Luo and Xuliang Huang and Zhuokai Zhao and Yibo Jiang and Xiangjun Fan and Himabindu Lakkaraju and James R. Glass},
  booktitle={The Thirteenth International Conference on Learning Representations (ICLR)},
  year={2025},
  url={https://openreview.net/forum?id=jpSLXoRKnH},
  pdf={https://arxiv.org/pdf/2410.01769},
  preview={scylla.png},
  abstract={While large language models (LLMs) have shown exceptional capabilities in understanding complex queries and performing sophisticated tasks, their generalization abilities are often deeply entangled with memorization, necessitating more precise evaluation. To address this challenge, we introduce Scylla, a dynamic evaluation framework that quantitatively measures the generalization abilities of LLMs. Scylla disentangles generalization from memorization via assessing model performance on both in-distribution (ID) and out-of-distribution (OOD) data through 20 tasks across 5 levels of complexity. Through extensive experiments, we uncover a non-monotonic relationship between task complexity and the performance gap between ID and OOD data, which we term the generalization valley. Specifically, this phenomenon reveals a critical threshold - referred to as critical complexity - where reliance on non-generalizable behavior peaks, indicating the upper bound of LLMs' generalization capabilities. As model size increases, the critical complexity shifts toward higher levels of task complexity, suggesting that larger models can handle more complex reasoning tasks before over-relying on memorization. Leveraging Scylla and the concept of critical complexity, we benchmark 28LLMs including both open-sourced models such as LLaMA and Qwen families, and close-sourced models like Claude and GPT, providing a more robust evaluation and establishing a clearer understanding of LLMs' generalization capabilities.}
}

@inproceedings{qi2025follow,
  selected = {true},
  title={Follow My Instruction and Spill the Beans: Scalable Data Extraction from Retrieval-Augmented Generation Systems},
  author={Zhenting Qi and Hanlin Zhang and Eric P. Xing and Sham M. Kakade and Himabindu Lakkaraju},
  booktitle={The Thirteenth International Conference on Learning Representations (ICLR)},
  year={2025},
  url={https://openreview.net/forum?id=Y4aWwRh25b},
  pdf={https://arxiv.org/pdf/2402.17840},
  preview={rag.png},
  google_scholar_id={LkGwnXOMwfcC},
  abstract={Retrieval-Augmented Generation (RAG) improves pre-trained models by incorporating external knowledge at test time to enable customized adaptation. We study the risk of datastore leakage in Retrieval-In-Context RAG Language Models (LMs). We show that an adversary can exploit LMs' instruction-following capabilities to easily extract text data verbatim from the datastore of RAG systems built with instruction-tuned LMs via prompt injection. The vulnerability exists for a wide range of modern LMs that span Llama2, Mistral/Mixtral, Vicuna, SOLAR, WizardLM, Qwen1.5, and Platypus2, and the exploitability exacerbates as the model size scales up. We also study multiple effects of RAG setup on the extractability of data, indicating that following unexpected instructions to regurgitate data can be an outcome of failure in effectively utilizing contexts for modern LMs, and further show that such vulnerability can be greatly mitigated by position bias elimination strategies. Extending our study to production RAG models GPTs, we design an attack that can cause datastore leakage with a 100% success rate on 25 randomly selected customized GPTs with at most 2 queries, and we extract text data verbatim at a rate of 41% from a book of 77,000 words and 3% from a corpus of 1,569,000 words by prompting the GPTs with only 100 queries generated by themselves.}
}

@article{zhao2022reastap,
  title={ReasTAP: Injecting table reasoning skills during pre-training via synthetic reasoning examples},
  author={Zhao, Yilun and Nan, Linyong and Qi, Zhenting and Zhang, Rui and Radev, Dragomir},
  journal={arXiv preprint arXiv:2210.12374},
  url={https://arxiv.org/abs/2210.12374},
  year={2022}
}

@inproceedings{tan-etal-2023-self,
    title = "Self-Criticism: Aligning Large Language Models with their Understanding of Helpfulness, Honesty, and Harmlessness",
    author = "Tan, Xiaoyu  and
      Shi, Shaojie  and
      Qiu, Xihe  and
      Qu, Chao  and
      Qi, Zhenting  and
      Xu, Yinghui  and
      Qi, Yuan",
    editor = "Wang, Mingxuan  and
      Zitouni, Imed",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Industry Track",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-industry.62/",
    doi = "10.18653/v1/2023.emnlp-industry.62",
    pages = "650--662",
    abstract = "Recently, there has been a notable surge in the significance of large language models (LLMs) that engage in conversational-style interactions, such as ChatGPT and Claude, as they contribute significantly to the progress of artificial general intelligence (AGI). Typically, these models undergo a two-phase fine-tuning process: instruction fine-tuning (IF) and reinforcement learning from human feedback (RLHF). These methods aim to align the LLMs to be helpful, honest, and harmless (HHH). However, RLHF, which incorporates independent reward models trained on high-quality human feedback datasets, incurs high costs in terms of hardware resources and human efforts. Therefore, we explore the possibility of aligning LLMs with their own understanding of HHH through IF and in-context learning (ICL). In this study, we propose a novel framework called Self-Criticism, which allows LLMs to align themselves with HHH based on the definition they learned from a large-scale text corpus. We begin by employing IF on a given instruction set and learning HHH discrimination through few-shot ICL. Subsequently, the LLMs evaluate their own generated responses and learn to produce ``better'' responses based on self-judgment. Finally, the model is retrained based on the self-generated responses to distill the whole process. By analyzing our proposed method, we also find interesting connections between Self-Criticism and goal-conditioned reinforcement learning, and pseudo-labeling. Experimental results demonstrate that this method achieves nearly identical performance to RLHF in terms of both human evaluation and evaluation by other LLMs, with only a minimal alignment tax."
}

@article{zhao2023qtsumm,
  title={QTSumm: Query-focused summarization over tabular data},
  author={Zhao, Yilun and Qi, Zhenting and Nan, Linyong and Mi, Boyu and Liu, Yixin and Zou, Weijin and Han, Simeng and Chen, Ruizhe and Tang, Xiangru and Xu, Yumo and others},
  journal={arXiv preprint arXiv:2305.14303},
  url={https://arxiv.org/abs/2305.14303},
  year={2023}
}

@inproceedings{zhao-etal-2023-loft,
    selected = {true},
    title = "{L}o{FT}: Enhancing Faithfulness and Diversity for Table-to-Text Generation via Logic Form Control",
    author = "Zhao*, Yilun  and
      Qi*, Zhenting  and
      Nan, Linyong  and
      Flores, Lorenzo Jaime  and
      Radev, Dragomir",
    editor = "Vlachos, Andreas  and
      Augenstein, Isabelle",
    booktitle = "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.eacl-main.40/",
    doi = "10.18653/v1/2023.eacl-main.40",
    pages = "554--561",
    preview={loft.png},
    pdf = "https://aclanthology.org/2023.eacl-main.40.pdf",
    abstract = "Logical Table-to-Text (LT2T) generation is tasked with generating logically faithful sentences from tables. There currently exists two challenges in the field: 1) Faithfulness: how to generate sentences that are factually correct given the table content; 2) Diversity: how to generate multiple sentences that offer different perspectives on the table. This work proposes LoFT, which utilizes logic forms as fact verifiers and content planners to control LT2T generation. Experimental results on the LogicNLG dataset demonstrate that LoFT is the first model that addresses unfaithfulness and lack of diversity issues simultaneously. Our code is publicly available at \url{https://github.com/Yale-LILY/LoFT}."
}

@INPROCEEDINGS{10098024,
  author={Qi, Zhenting and Zhu, Ruike and Fu, Zheyu and Chai, Wenhao and Kindratenko, Volodymyr},
  booktitle={2022 IEEE 34th International Conference on Tools with Artificial Intelligence (ICTAI)}, 
  title={Weakly Supervised Two-Stage Training Scheme for Deep Video Fight Detection Model}, 
  year={2022},
  volume={},
  number={},
  pages={677-685},
  keywords={Training;Deep learning;Codes;Surveillance;Streaming media;Feature extraction;Generators;Computer Vision;Weakly Supervised Learning;Self-Training;Video Anomaly Detection;Video Fight Detection},
  doi={10.1109/ICTAI56018.2022.00105}
}

@inproceedings{qi-etal-2023-pillow,
    selected = {true},
    title = "{PILLOW}: Enhancing Efficient Instruction Fine-tuning via Prompt Matching",
    author = "Qi, Zhenting  and
      Tan, Xiaoyu  and
      Shi, Shaojie  and
      Qu, Chao  and
      Xu, Yinghui  and
      Qi, Yuan",
    editor = "Wang, Mingxuan  and
      Zitouni, Imed",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Industry Track",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-industry.45/",
    doi = "10.18653/v1/2023.emnlp-industry.45",
    pages = "471--482",
    preview={pillow.png},
    pdf = "https://aclanthology.org/2023.emnlp-industry.45.pdf",
    abstract = "Instruction fine-tuning has conventionally been employed to adapt Large Language Models (LLMs) to a variety of diverse tasks. Nonetheless, this technique often necessitates substantial computational resources, making it impractical for deployment by individuals or small-scale entities. Recently, Low-Rank Adaptation (LoRA) has become a promising alternative, offering tuning capabilities with reduced resource overhead. However, attaining satisfactory performance through the fine-tuning of LoRA is a non-trivial challenge. In this paper, we propose PILLOW, which aims to improve LoRA{'}s performance by leveraging LLM{'}s in-context learning capability through prompt matching via reinforcement learning in resource-constrained environments. Specifically, PILLOW incorporates a matching network that selects prompts from a user-defined pool, concatenates the optimal prompts given the user instruction, and performs inference using the LoRA-fine-tuned LLMs. Compared with typical instruction fine-tuning methods, PILLOW exhibits commensurate performance on various evaluation metrics, utilizing only consumer-grade GPU resources and exhibiting a large increase in training efficiency."
}

@inproceedings{qi-etal-2023-safer,
    selected={true},
    title = "{S}a{FER}: A Robust and Efficient Framework for Fine-tuning {BERT}-based Classifier with Noisy Labels",
    author = "Qi, Zhenting  and
      Tan, Xiaoyu  and
      Qu, Chao  and
      Xu, Yinghui  and
      Qi, Yuan",
    editor = "Sitaram, Sunayana  and
      Beigman Klebanov, Beata  and
      Williams, Jason D",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-industry.38/",
    doi = "10.18653/v1/2023.acl-industry.38",
    pages = "390--403",
    preview={safer.png},
    pdf = "https://aclanthology.org/2023.acl-industry.38.pdf",
    abstract = "Learning on noisy datasets is a challenging problem when pre-trained language models are applied to real-world text classification tasks. In numerous industrial applications, acquiring task-specific datasets with 100{\%} accurate labels is difficult, thus many datasets are accompanied by label noise at different levels. Previous work has shown that existing noise-handling methods could not improve the peak performance of BERT on noisy datasets, and might even deteriorate it. In this paper, we propose SaFER, a robust and efficient fine-tuning framework for BERT-based text classifiers, combating label noises without access to any clean data for training or validation. Utilizing a label-agnostic early-stopping strategy and self-supervised learning, our proposed framework achieves superior performance in terms of both accuracy and speed on multiple text classification benchmarks. The trained model is finally fully deployed in several industrial biomedical literature mining tasks and demonstrates high effectiveness and efficiency."
}

@inproceedings{zhao-etal-2023-openrt,
    title = "{O}pen{RT}: An Open-source Framework for Reasoning Over Tabular Data",
    author = "Zhao, Yilun  and
      Mi, Boyu  and
      Qi, Zhenting  and
      Nan, Linyong  and
      Guo, Minghao  and
      Cohan, Arman  and
      Radev, Dragomir",
    editor = "Bollegala, Danushka  and
      Huang, Ruihong  and
      Ritter, Alan",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-demo.32/",
    doi = "10.18653/v1/2023.acl-demo.32",
    pages = "336--347",
    abstract = "There are a growing number of table pre-training methods proposed for reasoning over tabular data (e.g., question answering, fact checking, and faithful text generation). However, most existing methods are benchmarked solely on a limited number of datasets, varying in configuration, which leads to a lack of unified, standardized, fair, and comprehensive comparison between methods. This paper presents OpenRT, the first open-source framework for reasoning over tabular data, to reproduce existing table pre-training models for performance comparison and develop new models quickly. We implemented and compared six table pre-training models on four question answering, one fact checking, and one faithful text generation datasets. Moreover, to enable the community to easily construct new table reasoning datasets, we developed TaRAT, an annotation tool which supports multi-person collaborative annotations for various kinds of table reasoning tasks. The researchers are able to deploy the newly-constructed dataset to OpenRT and compare the performances of different baseline systems."
}

@article{han2024p,
  title={P-FOLIO: Evaluating and Improving Logical Reasoning with Abundant Human-Written Reasoning Chains},
  author={Han, Simeng and Yu, Aaron and Shen, Rui and Qi, Zhenting and Riddell, Martin and Zhou, Wenfei and Qiao, Yujie and Zhao, Yilun and Yavuz, Semih and Liu, Ye and others},
  journal={arXiv preprint arXiv:2410.09207},
  url={https://arxiv.org/abs/2410.09207},
  year={2024}
}

@article{du2024constrained,
  title={Constrained Human-AI Cooperation: An Inclusive Embodied Social Intelligence Challenge},
  author={Du, Weihua and Lyu, Qiushi and Shan, Jiaming and Qi, Zhenting and Zhang, Hongxin and Chen, Sunli and Peng, Andi and Shu, Tianmin and Lee, Kwonjoon and Dariush, Behzad and others},
  journal={arXiv preprint arXiv:2411.01796},
  url={https://arxiv.org/abs/2411.01796},
  year={2024}
}

@article{pawelczyk2024generalizing,
  title={Generalizing Trust: Weak-to-Strong Trustworthiness in Language Models},
  author={Pawelczyk, Martin and Sun, Lillian and Qi, Zhenting and Kumar, Aounon and Lakkaraju, Himabindu},
  journal={arXiv preprint arXiv:2501.00418},
  url={https://arxiv.org/abs/2501.00418},
  year={2024}
}

@article{zeng2025satori,
  title={Satori-SWE: Evolutionary Test-Time Scaling for Sample-Efficient Software Engineering},
  author={Zeng, Guangtao and Shen, Maohao and Chen, Delin and Qi, Zhenting and Das, Subhro and Gutfreund, Dan and Cox, David and Wornell, Gregory and Lu, Wei and Hong, Zhang-Wei and others},
  journal={arXiv preprint arXiv:2505.23604},
  url={https://arxiv.org/abs/2505.23604},
  year={2025}
}

@article{qi2025evolm,
  selected = {true},
  title={EvoLM: In Search of Lost Language Model Training Dynamics},
  author={Qi, Zhenting and Nie, Fan and Alahi, Alexandre and Zou, James and Lakkaraju, Himabindu and Du, Yilun and Xing, Eric and Kakade, Sham and Zhang, Hanlin},
  journal={The Thirty-Ninth Annual Conference on Neural Information Processing Systems (NeurIPS)},
  url={https://arxiv.org/abs/2506.16029},
  pdf={https://arxiv.org/pdf/2506.16029},
  year={2025}
}

@article{xiong2025measuring,
  selected = {true},
  title={Measuring the Faithfulness of Thinking Drafts in Large Reasoning Models},
  author={Xiong, Zidi and Shan, Chen and Qi, Zhenting and Lakkaraju, Himabindu},
  journal={The Thirty-Ninth Annual Conference on Neural Information Processing Systems (NeurIPS)},
  url={https://arxiv.org/abs/2505.13774},
  year={2025}
}

@article{zhao2023robut,
  title={RobuT: A systematic study of table QA robustness against human-annotated adversarial perturbations},
  author={Zhao, Yilun and Zhao, Chen and Nan, Linyong and Qi, Zhenting and Zhang, Wenlin and Tang, Xiangru and Mi, Boyu and Radev, Dragomir},
  journal={arXiv preprint arXiv:2306.14321},
  url={https://arxiv.org/abs/2306.14321},
  year={2023}
}

@inproceedings{han-etal-2024-folio,
    selected = {true},
    title = "{FOLIO}: Natural Language Reasoning with First-Order Logic",
    author = "Han, Simeng  and
      Schoelkopf, Hailey  and
      Zhao, Yilun  and
      Qi, Zhenting  and
      Riddell, Martin  and
      Zhou, Wenfei  and
      Coady, James  and
      Peng, David  and
      Qiao, Yujie  and
      Benson, Luke  and
      Sun, Lucy  and
      Wardle-Solano, Alexander  and
      Szab{\'o}, Hannah  and
      Zubova, Ekaterina  and
      Burtell, Matthew  and
      Fan, Jonathan  and
      Liu, Yixin  and
      Wong, Brian  and
      Sailor, Malcolm  and
      Ni, Ansong  and
      Nan, Linyong  and
      Kasai, Jungo  and
      Yu, Tao  and
      Zhang, Rui  and
      Fabbri, Alexander  and
      Kryscinski, Wojciech Maciej  and
      Yavuz, Semih  and
      Liu, Ye  and
      Lin, Xi Victoria  and
      Joty, Shafiq  and
      Zhou, Yingbo  and
      Xiong, Caiming  and
      Ying, Rex  and
      Cohan, Arman  and
      Radev, Dragomir",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.1229/",
    doi = "10.18653/v1/2024.emnlp-main.1229",
    pages = "22017--22031",
    preview={folio.png},
    google_scholar_id={u-x6o8ySG0sC},
    arxiv={2209.00840},
    pdf = "https://arxiv.org/pdf/2209.00840",
    abstract = "Large language models (LLMs) have achieved remarkable performance on a variety of natural language understanding tasks. However, existing benchmarks are inadequate in measuring the complex logical reasoning capabilities of a model. We present FOLIO, a human-annotated, logically complex and diverse dataset for reasoning in natural language (NL), equipped with first-order logic (FOL) annotations. FOLIO consists of 1,430 examples (unique conclusions), each paired with one of 487 sets of premises used to deductively reason for the validity of each conclusion. The logical correctness of the premises and conclusions is ensured by their FOL annotations, which are automatically verified by an FOL inference engine. In addition to the main NL reasoning task, NL-FOL pairs in FOLIO constitute a new NL-FOL translation dataset. Our experiments on FOLIO systematically evaluate the FOL reasoning ability of supervised fine-tuning on medium-sized language models. For both NL reasoning and NL-FOL translation, we benchmark multiple state-of-the-art language models. Our results show that a subset of FOLIO remains a challenge for one of the most capable Large Language Model (LLM) publicly available, GPT-4."
}