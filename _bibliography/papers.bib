---
---

@string{aps = {American Physical Society,}}

@inproceedings{han-etal-2024-folio,
    selected = {true},
    title = "{FOLIO}: Natural Language Reasoning with First-Order Logic",
    author = "Han, Simeng  and
      Schoelkopf, Hailey  and
      Zhao, Yilun  and
      Qi, Zhenting  and
      Riddell, Martin  and
      Zhou, Wenfei  and
      Coady, James  and
      Peng, David  and
      Qiao, Yujie  and
      Benson, Luke  and
      Sun, Lucy  and
      Wardle-Solano, Alexander  and
      Szab{\'o}, Hannah  and
      Zubova, Ekaterina  and
      Burtell, Matthew  and
      Fan, Jonathan  and
      Liu, Yixin  and
      Wong, Brian  and
      Sailor, Malcolm  and
      Ni, Ansong  and
      Nan, Linyong  and
      Kasai, Jungo  and
      Yu, Tao  and
      Zhang, Rui  and
      Fabbri, Alexander  and
      Kryscinski, Wojciech Maciej  and
      Yavuz, Semih  and
      Liu, Ye  and
      Lin, Xi Victoria  and
      Joty, Shafiq  and
      Zhou, Yingbo  and
      Xiong, Caiming  and
      Ying, Rex  and
      Cohan, Arman  and
      Radev, Dragomir",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.1229/",
    doi = "10.18653/v1/2024.emnlp-main.1229",
    pages = "22017--22031",
    abstract = "Large language models (LLMs) have achieved remarkable performance on a variety of natural language understanding tasks. However, existing benchmarks are inadequate in measuring the complex logical reasoning capabilities of a model. We present FOLIO, a human-annotated, logically complex and diverse dataset for reasoning in natural language (NL), equipped with first-order logic (FOL) annotations. FOLIO consists of 1,430 examples (unique conclusions), each paired with one of 487 sets of premises used to deductively reason for the validity of each conclusion. The logical correctness of the premises and conclusions is ensured by their FOL annotations, which are automatically verified by an FOL inference engine. In addition to the main NL reasoning task, NL-FOL pairs in FOLIO constitute a new NL-FOL translation dataset. Our experiments on FOLIO systematically evaluate the FOL reasoning ability of supervised fine-tuning on medium-sized language models. For both NL reasoning and NL-FOL translation, we benchmark multiple state-of-the-art language models. Our results show that a subset of FOLIO remains a challenge for one of the most capable Large Language Model (LLM) publicly available, GPT-4."
}

@inproceedings{qi2025mutual,
  selected = {true},
  title={Mutual Reasoning Makes Smaller {LLM}s Stronger Problem-Solver},
  author={Zhenting Qi and Mingyuan MA and Jiahang Xu and Li Lyna Zhang and Fan Yang and Mao Yang},
  booktitle={The Thirteenth International Conference on Learning Representations},
  year={2025},
  url={https://openreview.net/forum?id=6aHUmotXaw}
}

@article{zhao2023robut,
  title={RobuT: A systematic study of table QA robustness against human-annotated adversarial perturbations},
  author={Zhao, Yilun and Zhao, Chen and Nan, Linyong and Qi, Zhenting and Zhang, Wenlin and Tang, Xiangru and Mi, Boyu and Radev, Dragomir},
  journal={arXiv preprint arXiv:2306.14321},
  url={https://arxiv.org/abs/2306.14321},
  year={2023}
}

@inproceedings{qi2025follow,
  title={Follow My Instruction and Spill the Beans: Scalable Data Extraction from Retrieval-Augmented Generation Systems},
  author={Zhenting Qi and Hanlin Zhang and Eric P. Xing and Sham M. Kakade and Himabindu Lakkaraju},
  booktitle={The Thirteenth International Conference on Learning Representations},
  year={2025},
  url={https://openreview.net/forum?id=Y4aWwRh25b}
}

@article{zhao2022reastap,
  title={ReasTAP: Injecting table reasoning skills during pre-training via synthetic reasoning examples},
  author={Zhao, Yilun and Nan, Linyong and Qi, Zhenting and Zhang, Rui and Radev, Dragomir},
  journal={arXiv preprint arXiv:2210.12374},
  url={https://arxiv.org/abs/2210.12374},
  year={2022}
}

@inproceedings{tan-etal-2023-self,
    title = "Self-Criticism: Aligning Large Language Models with their Understanding of Helpfulness, Honesty, and Harmlessness",
    author = "Tan, Xiaoyu  and
      Shi, Shaojie  and
      Qiu, Xihe  and
      Qu, Chao  and
      Qi, Zhenting  and
      Xu, Yinghui  and
      Qi, Yuan",
    editor = "Wang, Mingxuan  and
      Zitouni, Imed",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Industry Track",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-industry.62/",
    doi = "10.18653/v1/2023.emnlp-industry.62",
    pages = "650--662",
    abstract = "Recently, there has been a notable surge in the significance of large language models (LLMs) that engage in conversational-style interactions, such as ChatGPT and Claude, as they contribute significantly to the progress of artificial general intelligence (AGI). Typically, these models undergo a two-phase fine-tuning process: instruction fine-tuning (IF) and reinforcement learning from human feedback (RLHF). These methods aim to align the LLMs to be helpful, honest, and harmless (HHH). However, RLHF, which incorporates independent reward models trained on high-quality human feedback datasets, incurs high costs in terms of hardware resources and human efforts. Therefore, we explore the possibility of aligning LLMs with their own understanding of HHH through IF and in-context learning (ICL). In this study, we propose a novel framework called Self-Criticism, which allows LLMs to align themselves with HHH based on the definition they learned from a large-scale text corpus. We begin by employing IF on a given instruction set and learning HHH discrimination through few-shot ICL. Subsequently, the LLMs evaluate their own generated responses and learn to produce ``better'' responses based on self-judgment. Finally, the model is retrained based on the self-generated responses to distill the whole process. By analyzing our proposed method, we also find interesting connections between Self-Criticism and goal-conditioned reinforcement learning, and pseudo-labeling. Experimental results demonstrate that this method achieves nearly identical performance to RLHF in terms of both human evaluation and evaluation by other LLMs, with only a minimal alignment tax."
}

@article{zhao2023qtsumm,
  title={QTSumm: Query-focused summarization over tabular data},
  author={Zhao, Yilun and Qi, Zhenting and Nan, Linyong and Mi, Boyu and Liu, Yixin and Zou, Weijin and Han, Simeng and Chen, Ruizhe and Tang, Xiangru and Xu, Yumo and others},
  journal={arXiv preprint arXiv:2305.14303},
  url={https://arxiv.org/abs/2305.14303},
  year={2023}
}

@article{zhao2023loft,
  title={Loft: Enhancing faithfulness and diversity for table-to-text generation via logic form control},
  author={Zhao*, Yilun and Qi*, Zhenting and Nan, Linyong and Flores, Lorenzo Jaime Yu and Radev, Dragomir},
  journal={arXiv preprint arXiv:2302.02962},
  url={https://arxiv.org/abs/2302.02962},
  selected={true},
  year={2023}
}

@INPROCEEDINGS{10098024,
  author={Qi, Zhenting and Zhu, Ruike and Fu, Zheyu and Chai, Wenhao and Kindratenko, Volodymyr},
  booktitle={2022 IEEE 34th International Conference on Tools with Artificial Intelligence (ICTAI)}, 
  title={Weakly Supervised Two-Stage Training Scheme for Deep Video Fight Detection Model}, 
  year={2022},
  volume={},
  number={},
  pages={677-685},
  keywords={Training;Deep learning;Codes;Surveillance;Streaming media;Feature extraction;Generators;Computer Vision;Weakly Supervised Learning;Self-Training;Video Anomaly Detection;Video Fight Detection},
  doi={10.1109/ICTAI56018.2022.00105}
}

@article{qi2023pillow,
  title={Pillow: Enhancing efficient instruction fine-tuning via prompt matching},
  author={Qi, Zhenting and Tan, Xiaoyu and Shi, Shaojie and Qu, Chao and Xu, Yinghui and Qi, Yuan},
  journal={arXiv preprint arXiv:2312.05621},
  url={https://arxiv.org/abs/2312.05621},
  selected={true},
  year={2023}
}

@article{shen2025satori,
  title={Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search},
  author={Shen*, Maohao and Zeng*, Guangtao and Qi*, Zhenting and Hong, Zhang-Wei and Chen, Zhenfang and Lu, Wei and Wornell, Gregory and Das, Subhro and Cox, David and Gan, Chuang},
  journal={arXiv preprint arXiv:2502.02508},
  url={https://arxiv.org/abs/2502.02508},
  selected={true},
  year={2025}
}

@inproceedings{qi-etal-2023-safer,
    title = "{S}a{FER}: A Robust and Efficient Framework for Fine-tuning {BERT}-based Classifier with Noisy Labels",
    author = "Qi, Zhenting  and
      Tan, Xiaoyu  and
      Qu, Chao  and
      Xu, Yinghui  and
      Qi, Yuan",
    editor = "Sitaram, Sunayana  and
      Beigman Klebanov, Beata  and
      Williams, Jason D",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-industry.38/",
    doi = "10.18653/v1/2023.acl-industry.38",
    pages = "390--403",
    selected={true},
    abstract = "Learning on noisy datasets is a challenging problem when pre-trained language models are applied to real-world text classification tasks. In numerous industrial applications, acquiring task-specific datasets with 100{\%} accurate labels is difficult, thus many datasets are accompanied by label noise at different levels. Previous work has shown that existing noise-handling methods could not improve the peak performance of BERT on noisy datasets, and might even deteriorate it. In this paper, we propose SaFER, a robust and efficient fine-tuning framework for BERT-based text classifiers, combating label noises without access to any clean data for training or validation. Utilizing a label-agnostic early-stopping strategy and self-supervised learning, our proposed framework achieves superior performance in terms of both accuracy and speed on multiple text classification benchmarks. The trained model is finally fully deployed in several industrial biomedical literature mining tasks and demonstrates high effectiveness and efficiency."
}

@inproceedings{zhao-etal-2023-openrt,
    title = "{O}pen{RT}: An Open-source Framework for Reasoning Over Tabular Data",
    author = "Zhao, Yilun  and
      Mi, Boyu  and
      Qi, Zhenting  and
      Nan, Linyong  and
      Guo, Minghao  and
      Cohan, Arman  and
      Radev, Dragomir",
    editor = "Bollegala, Danushka  and
      Huang, Ruihong  and
      Ritter, Alan",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-demo.32/",
    doi = "10.18653/v1/2023.acl-demo.32",
    pages = "336--347",
    abstract = "There are a growing number of table pre-training methods proposed for reasoning over tabular data (e.g., question answering, fact checking, and faithful text generation). However, most existing methods are benchmarked solely on a limited number of datasets, varying in configuration, which leads to a lack of unified, standardized, fair, and comprehensive comparison between methods. This paper presents OpenRT, the first open-source framework for reasoning over tabular data, to reproduce existing table pre-training models for performance comparison and develop new models quickly. We implemented and compared six table pre-training models on four question answering, one fact checking, and one faithful text generation datasets. Moreover, to enable the community to easily construct new table reasoning datasets, we developed TaRAT, an annotation tool which supports multi-person collaborative annotations for various kinds of table reasoning tasks. The researchers are able to deploy the newly-constructed dataset to OpenRT and compare the performances of different baseline systems."
}

@inproceedings{qi2025quantifying,
  selected = {true},
  title={Quantifying Generalization Complexity for Large Language Models},
  author={Zhenting Qi and Hongyin Luo and Xuliang Huang and Zhuokai Zhao and Yibo Jiang and Xiangjun Fan and Himabindu Lakkaraju and James R. Glass},
  booktitle={The Thirteenth International Conference on Learning Representations},
  year={2025},
  url={https://openreview.net/forum?id=jpSLXoRKnH}
}

@article{han2024p,
  title={P-FOLIO: Evaluating and Improving Logical Reasoning with Abundant Human-Written Reasoning Chains},
  author={Han, Simeng and Yu, Aaron and Shen, Rui and Qi, Zhenting and Riddell, Martin and Zhou, Wenfei and Qiao, Yujie and Zhao, Yilun and Yavuz, Semih and Liu, Ye and others},
  journal={arXiv preprint arXiv:2410.09207},
  url={https://arxiv.org/abs/2410.09207},
  year={2024}
}

@article{du2024constrained,
  title={Constrained Human-AI Cooperation: An Inclusive Embodied Social Intelligence Challenge},
  author={Du, Weihua and Lyu, Qiushi and Shan, Jiaming and Qi, Zhenting and Zhang, Hongxin and Chen, Sunli and Peng, Andi and Shu, Tianmin and Lee, Kwonjoon and Dariush, Behzad and others},
  journal={arXiv preprint arXiv:2411.01796},
  url={https://arxiv.org/abs/2411.01796},
  year={2024}
}

@article{pawelczyk2024generalizing,
  title={Generalizing Trust: Weak-to-Strong Trustworthiness in Language Models},
  author={Pawelczyk, Martin and Sun, Lillian and Qi, Zhenting and Kumar, Aounon and Lakkaraju, Himabindu},
  journal={arXiv preprint arXiv:2501.00418},
  url={https://arxiv.org/abs/2501.00418},
  year={2024}
}

@article{zeng2025satori,
  title={Satori-SWE: Evolutionary Test-Time Scaling for Sample-Efficient Software Engineering},
  author={Zeng, Guangtao and Shen, Maohao and Chen, Delin and Qi, Zhenting and Das, Subhro and Gutfreund, Dan and Cox, David and Wornell, Gregory and Lu, Wei and Hong, Zhang-Wei and others},
  journal={arXiv preprint arXiv:2505.23604},
  url={https://arxiv.org/abs/2505.23604},
  year={2025}
}

@article{xiong2025measuring,
  title={Measuring the Faithfulness of Thinking Drafts in Large Reasoning Models},
  author={Xiong, Zidi and Shan, Chen and Qi, Zhenting and Lakkaraju, Himabindu},
  journal={arXiv preprint arXiv:2505.13774},
  url={https://arxiv.org/abs/2505.13774},
  year={2025}
}
