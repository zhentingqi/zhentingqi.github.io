---
---

@string{aps = {American Physical Society,}}

@article{han2022folio,
  title={Folio: Natural language reasoning with first-order logic},
  author={Han, Simeng and Schoelkopf, Hailey and Zhao, Yilun and Qi, Zhenting and Riddell, Martin and Zhou, Wenfei and Coady, James and Peng, David and Qiao, Yujie and Benson, Luke and others},
  journal={arXiv preprint arXiv:2209.00840},
  selected={true},
  year={2022}
}

@article{qi2024mutual,
  title={Mutual reasoning makes smaller llms stronger problem-solvers},
  author={Qi, Zhenting and Ma, Mingyuan and Xu, Jiahang and Zhang, Li Lyna and Yang, Fan and Yang, Mao},
  journal={arXiv preprint arXiv:2408.06195},
  selected={true},
  year={2024}
}

@article{zhao2023robut,
  title={RobuT: A systematic study of table QA robustness against human-annotated adversarial perturbations},
  author={Zhao, Yilun and Zhao, Chen and Nan, Linyong and Qi, Zhenting and Zhang, Wenlin and Tang, Xiangru and Mi, Boyu and Radev, Dragomir},
  journal={arXiv preprint arXiv:2306.14321},
  year={2023}
}

@article{qi2024follow,
  title={Follow my instruction and spill the beans: Scalable data extraction from retrieval-augmented generation systems},
  author={Qi, Zhenting and Zhang, Hanlin and Xing, Eric and Kakade, Sham and Lakkaraju, Himabindu},
  journal={arXiv preprint arXiv:2402.17840},
  selected={true},
  year={2024}
}

@article{zhao2022reastap,
  title={ReasTAP: Injecting table reasoning skills during pre-training via synthetic reasoning examples},
  author={Zhao, Yilun and Nan, Linyong and Qi, Zhenting and Zhang, Rui and Radev, Dragomir},
  journal={arXiv preprint arXiv:2210.12374},
  year={2022}
}

@inproceedings{tan-etal-2023-self,
    title = "Self-Criticism: Aligning Large Language Models with their Understanding of Helpfulness, Honesty, and Harmlessness",
    author = "Tan, Xiaoyu  and
      Shi, Shaojie  and
      Qiu, Xihe  and
      Qu, Chao  and
      Qi, Zhenting  and
      Xu, Yinghui  and
      Qi, Yuan",
    editor = "Wang, Mingxuan  and
      Zitouni, Imed",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Industry Track",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-industry.62/",
    doi = "10.18653/v1/2023.emnlp-industry.62",
    pages = "650--662",
    abstract = "Recently, there has been a notable surge in the significance of large language models (LLMs) that engage in conversational-style interactions, such as ChatGPT and Claude, as they contribute significantly to the progress of artificial general intelligence (AGI). Typically, these models undergo a two-phase fine-tuning process: instruction fine-tuning (IF) and reinforcement learning from human feedback (RLHF). These methods aim to align the LLMs to be helpful, honest, and harmless (HHH). However, RLHF, which incorporates independent reward models trained on high-quality human feedback datasets, incurs high costs in terms of hardware resources and human efforts. Therefore, we explore the possibility of aligning LLMs with their own understanding of HHH through IF and in-context learning (ICL). In this study, we propose a novel framework called Self-Criticism, which allows LLMs to align themselves with HHH based on the definition they learned from a large-scale text corpus. We begin by employing IF on a given instruction set and learning HHH discrimination through few-shot ICL. Subsequently, the LLMs evaluate their own generated responses and learn to produce ``better'' responses based on self-judgment. Finally, the model is retrained based on the self-generated responses to distill the whole process. By analyzing our proposed method, we also find interesting connections between Self-Criticism and goal-conditioned reinforcement learning, and pseudo-labeling. Experimental results demonstrate that this method achieves nearly identical performance to RLHF in terms of both human evaluation and evaluation by other LLMs, with only a minimal alignment tax."
}

@article{zhao2023qtsumm,
  title={QTSumm: Query-focused summarization over tabular data},
  author={Zhao, Yilun and Qi, Zhenting and Nan, Linyong and Mi, Boyu and Liu, Yixin and Zou, Weijin and Han, Simeng and Chen, Ruizhe and Tang, Xiangru and Xu, Yumo and others},
  journal={arXiv preprint arXiv:2305.14303},
  year={2023}
}

@article{zhao2023loft,
  title={Loft: Enhancing faithfulness and diversity for table-to-text generation via logic form control},
  author={Zhao, Yilun and Qi, Zhenting and Nan, Linyong and Flores, Lorenzo Jaime Yu and Radev, Dragomir},
  journal={arXiv preprint arXiv:2302.02962},
  selected={true},
  year={2023}
}

@INPROCEEDINGS{10098024,
  author={Qi, Zhenting and Zhu, Ruike and Fu, Zheyu and Chai, Wenhao and Kindratenko, Volodymyr},
  booktitle={2022 IEEE 34th International Conference on Tools with Artificial Intelligence (ICTAI)}, 
  title={Weakly Supervised Two-Stage Training Scheme for Deep Video Fight Detection Model}, 
  year={2022},
  volume={},
  number={},
  pages={677-685},
  keywords={Training;Deep learning;Codes;Surveillance;Streaming media;Feature extraction;Generators;Computer Vision;Weakly Supervised Learning;Self-Training;Video Anomaly Detection;Video Fight Detection},
  doi={10.1109/ICTAI56018.2022.00105}
}

@article{qi2023pillow,
  title={Pillow: Enhancing efficient instruction fine-tuning via prompt matching},
  author={Qi, Zhenting and Tan, Xiaoyu and Shi, Shaojie and Qu, Chao and Xu, Yinghui and Qi, Yuan},
  journal={arXiv preprint arXiv:2312.05621},
  selected={true},
  year={2023}
}

@article{shen2025satori,
  title={Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search},
  author={Shen, Maohao and Zeng, Guangtao and Qi, Zhenting and Hong, Zhang-Wei and Chen, Zhenfang and Lu, Wei and Wornell, Gregory and Das, Subhro and Cox, David and Gan, Chuang},
  journal={arXiv preprint arXiv:2502.02508},
  selected={true},
  year={2025}
}

@inproceedings{qi-etal-2023-safer,
    title = "{S}a{FER}: A Robust and Efficient Framework for Fine-tuning {BERT}-based Classifier with Noisy Labels",
    author = "Qi, Zhenting  and
      Tan, Xiaoyu  and
      Qu, Chao  and
      Xu, Yinghui  and
      Qi, Yuan",
    editor = "Sitaram, Sunayana  and
      Beigman Klebanov, Beata  and
      Williams, Jason D",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-industry.38/",
    doi = "10.18653/v1/2023.acl-industry.38",
    pages = "390--403",
    selected={true},
    abstract = "Learning on noisy datasets is a challenging problem when pre-trained language models are applied to real-world text classification tasks. In numerous industrial applications, acquiring task-specific datasets with 100{\%} accurate labels is difficult, thus many datasets are accompanied by label noise at different levels. Previous work has shown that existing noise-handling methods could not improve the peak performance of BERT on noisy datasets, and might even deteriorate it. In this paper, we propose SaFER, a robust and efficient fine-tuning framework for BERT-based text classifiers, combating label noises without access to any clean data for training or validation. Utilizing a label-agnostic early-stopping strategy and self-supervised learning, our proposed framework achieves superior performance in terms of both accuracy and speed on multiple text classification benchmarks. The trained model is finally fully deployed in several industrial biomedical literature mining tasks and demonstrates high effectiveness and efficiency."
}

@inproceedings{zhao-etal-2023-openrt,
    title = "{O}pen{RT}: An Open-source Framework for Reasoning Over Tabular Data",
    author = "Zhao, Yilun  and
      Mi, Boyu  and
      Qi, Zhenting  and
      Nan, Linyong  and
      Guo, Minghao  and
      Cohan, Arman  and
      Radev, Dragomir",
    editor = "Bollegala, Danushka  and
      Huang, Ruihong  and
      Ritter, Alan",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-demo.32/",
    doi = "10.18653/v1/2023.acl-demo.32",
    pages = "336--347",
    abstract = "There are a growing number of table pre-training methods proposed for reasoning over tabular data (e.g., question answering, fact checking, and faithful text generation). However, most existing methods are benchmarked solely on a limited number of datasets, varying in configuration, which leads to a lack of unified, standardized, fair, and comprehensive comparison between methods. This paper presents OpenRT, the first open-source framework for reasoning over tabular data, to reproduce existing table pre-training models for performance comparison and develop new models quickly. We implemented and compared six table pre-training models on four question answering, one fact checking, and one faithful text generation datasets. Moreover, to enable the community to easily construct new table reasoning datasets, we developed TaRAT, an annotation tool which supports multi-person collaborative annotations for various kinds of table reasoning tasks. The researchers are able to deploy the newly-constructed dataset to OpenRT and compare the performances of different baseline systems."
}

@article{qi2024quantifying,
  title={Quantifying generalization complexity for large language models},
  author={Qi, Zhenting and Luo, Hongyin and Huang, Xuliang and Zhao, Zhuokai and Jiang, Yibo and Fan, Xiangjun and Lakkaraju, Himabindu and Glass, James},
  journal={arXiv preprint arXiv:2410.01769},
  selected={true},
  year={2024}
}

@article{han2024p,
  title={P-FOLIO: Evaluating and Improving Logical Reasoning with Abundant Human-Written Reasoning Chains},
  author={Han, Simeng and Yu, Aaron and Shen, Rui and Qi, Zhenting and Riddell, Martin and Zhou, Wenfei and Qiao, Yujie and Zhao, Yilun and Yavuz, Semih and Liu, Ye and others},
  journal={arXiv preprint arXiv:2410.09207},
  year={2024}
}

@article{du2024constrained,
  title={Constrained Human-AI Cooperation: An Inclusive Embodied Social Intelligence Challenge},
  author={Du, Weihua and Lyu, Qiushi and Shan, Jiaming and Qi, Zhenting and Zhang, Hongxin and Chen, Sunli and Peng, Andi and Shu, Tianmin and Lee, Kwonjoon and Dariush, Behzad and others},
  journal={arXiv preprint arXiv:2411.01796},
  year={2024}
}

@article{pawelczyk2024generalizing,
  title={Generalizing Trust: Weak-to-Strong Trustworthiness in Language Models},
  author={Pawelczyk, Martin and Sun, Lillian and Qi, Zhenting and Kumar, Aounon and Lakkaraju, Himabindu},
  journal={arXiv preprint arXiv:2501.00418},
  year={2024}
}

@article{zeng2025satori,
  title={Satori-SWE: Evolutionary Test-Time Scaling for Sample-Efficient Software Engineering},
  author={Zeng, Guangtao and Shen, Maohao and Chen, Delin and Qi, Zhenting and Das, Subhro and Gutfreund, Dan and Cox, David and Wornell, Gregory and Lu, Wei and Hong, Zhang-Wei and others},
  journal={arXiv preprint arXiv:2505.23604},
  year={2025}
}

@article{xiong2025measuring,
  title={Measuring the Faithfulness of Thinking Drafts in Large Reasoning Models},
  author={Xiong, Zidi and Shan, Chen and Qi, Zhenting and Lakkaraju, Himabindu},
  journal={arXiv preprint arXiv:2505.13774},
  year={2025}
}
