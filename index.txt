1:"$Sreact.fragment"
2:I[3719,["457","static/chunks/457-8b8d97145837e580.js","874","static/chunks/874-6cc630662f3664af.js","862","static/chunks/862-15038af301665bb3.js","177","static/chunks/app/layout-d7cbb4646de1c3b3.js"],"ThemeProvider"]
3:I[768,["457","static/chunks/457-8b8d97145837e580.js","874","static/chunks/874-6cc630662f3664af.js","862","static/chunks/862-15038af301665bb3.js","177","static/chunks/app/layout-d7cbb4646de1c3b3.js"],"default"]
4:I[7555,[],""]
5:I[1295,[],""]
6:I[2548,["457","static/chunks/457-8b8d97145837e580.js","874","static/chunks/874-6cc630662f3664af.js","862","static/chunks/862-15038af301665bb3.js","177","static/chunks/app/layout-d7cbb4646de1c3b3.js"],"default"]
7:I[2637,["457","static/chunks/457-8b8d97145837e580.js","485","static/chunks/485-487001f78a0503db.js","874","static/chunks/874-6cc630662f3664af.js","748","static/chunks/748-c442066f2b36fa6c.js","974","static/chunks/app/page-1e259e5be13ce12d.js"],"default"]
8:I[9507,["457","static/chunks/457-8b8d97145837e580.js","485","static/chunks/485-487001f78a0503db.js","874","static/chunks/874-6cc630662f3664af.js","748","static/chunks/748-c442066f2b36fa6c.js","974","static/chunks/app/page-1e259e5be13ce12d.js"],"default"]
a:I[1990,["457","static/chunks/457-8b8d97145837e580.js","485","static/chunks/485-487001f78a0503db.js","874","static/chunks/874-6cc630662f3664af.js","748","static/chunks/748-c442066f2b36fa6c.js","974","static/chunks/app/page-1e259e5be13ce12d.js"],"default"]
b:I[5218,["457","static/chunks/457-8b8d97145837e580.js","485","static/chunks/485-487001f78a0503db.js","874","static/chunks/874-6cc630662f3664af.js","748","static/chunks/748-c442066f2b36fa6c.js","974","static/chunks/app/page-1e259e5be13ce12d.js"],"default"]
14:I[9665,[],"MetadataBoundary"]
16:I[9665,[],"OutletBoundary"]
19:I[4911,[],"AsyncMetadataOutlet"]
1b:I[9665,[],"ViewportBoundary"]
1d:I[6614,[],""]
:HL["/_next/static/css/66c6e803a86cbc2f.css","style"]
9:T740,Welcome! I am a 1st year Computer Science Ph.D. student at [Harvard University](https://www.harvard.edu/), where I am honored to be co-advised by Prof. [Yilun Du](https://yilundu.github.io/) and Prof. [Hima Lakkaraju](https://himalakkaraju.github.io/). 

My research centers around developing intelligent and reliable AI systems that benefit human society. Motivated by this, I am generally interested in the following topics (w/o particular order):

- **Reasoning**
    - Understanding and enhancing reasoning capabilities in foundation models
    - Developing AI systems that generalize effectively to OOD scenarios
    - Training (multi-)agents for compositional reasoning tasks

- **Reliability**
    - Improving understanding of foundation models and AI systems
    - Enhancing controllability and robustness
    - Designing scalable computational methods for reliability while advancing capabilities

I've had the privilege of working closely with many distinguished researchers, including (the late) Prof. [Dragomir R. Radev](http://www.cs.yale.edu/homes/radev/) at Yale, Prof. [Volodymyr Kindratenko](https://ece.illinois.edu/about/directory/faculty/kindrtnk) at UIUC, Dr. [Li Lyna Zhang](https://www.microsoft.com/en-us/research/people/lzhani/) at [Microsoft Research Asia](https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/), Prof. [Eric Xing](https://mbzuai.ac.ae/study/faculty/professor-eric-xing/) at CMU, and Prof. [James Glass](https://sls.csail.mit.edu/people/glass.shtml) at MIT.

For more information about my research, please see [Google Scholar](https://scholar.google.com/citations?hl=en&user=WZ00HCUAAAAJ), [Semantic Scholar](https://www.semanticscholar.org/author/Zhenting-Qi/2186056193), or [DBLP](https://dblp.org/pid/329/2118.html). Please feel free to reach out at: `zhentingqi [at] g [dot] harvard [dot] edu`.c:T4a8,Modern language model (LM) training has been divided into multiple stages, making it difficult for downstream developers to evaluate the impact of design choices made at each stage. We present EvoLM, a model suite that enables systematic and transparent analysis of LMs' training dynamics across pre-training, continued pre-training, supervised fine-tuning, and reinforcement learning. We train over 100 LMs with 1B and 4B parameters from scratch, and evaluate both upstream (language modeling) and downstream (problem-solving) capabilities, including considerations of both in-domain and out-of-domain generalization. Key insights highlight the diminishing returns from excessive pre-training and post-training, the importance and practices of mitigating forgetting during domain-specific continued pre-training, the crucial role of continued pre-training in bridging pre-training and post-training phases, and various intricate trade-offs when configuring supervised fine-tuning and reinforcement learning. To facilitate open research and reproducibility, we release all pre-trained and post-trained models, training datasets for all stages, and our entire training and evaluation pipeline.d:T664,@inproceedings{evolm2025,
  title = {EvoLM: In Search of Lost Language Model Training Dynamics},
  author = {Zhenting Qi and Fan Nie and Alexandre Alahi and James Zou and Himabindu Lakkaraju and Yilun Du and Eric Xing and Sham Kakade and Hanlin Zhang},
  year = {2025},
  month = {dec},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  note = {Oral},
  abstract = {Modern language model (LM) training has been divided into multiple stages, making it difficult for downstream developers to evaluate the impact of design choices made at each stage. We present EvoLM, a model suite that enables systematic and transparent analysis of LMs' training dynamics across pre-training, continued pre-training, supervised fine-tuning, and reinforcement learning. We train over 100 LMs with 1B and 4B parameters from scratch, and evaluate both upstream (language modeling) and downstream (problem-solving) capabilities, including considerations of both in-domain and out-of-domain generalization. Key insights highlight the diminishing returns from excessive pre-training and post-training, the importance and practices of mitigating forgetting during domain-specific continued pre-training, the crucial role of continued pre-training in bridging pre-training and post-training phases, and various intricate trade-offs when configuring supervised fine-tuning and reinforcement learning. To facilitate open research and reproducibility, we release all pre-trained and post-trained models, training datasets for all stages, and our entire training and evaluation pipeline.},
  arxiv = {https://arxiv.org/abs/2506.16029}
}e:T57b,Large language models (LLMs) have demonstrated remarkable reasoning capabilities across diverse domains. Recent studies have shown that increasing test-time computation enhances LLMs' reasoning capabilities. This typically involves extensive sampling at inference time guided by an external LLM verifier, resulting in a two-player system. Despite external guidance, the effectiveness of this system demonstrates the potential of a single LLM to tackle complex tasks. Thus, we pose a new research problem: Can we internalize the searching capabilities to fundamentally enhance the reasoning abilities of a single LLM? This work explores an orthogonal direction focusing on post-training LLMs for autoregressive searching (i.e., an extended reasoning process with self-reflection and self-exploration of new strategies). To achieve this, we propose the Chain-of-Action-Thought (COAT) reasoning and a two-stage training paradigm: 1) a small-scale format tuning stage to internalize the COAT reasoning format and 2) a large-scale self-improvement stage leveraging reinforcement learning. Our approach results in Satori, a 7B LLM trained on open-source models and data. Extensive empirical evaluations demonstrate that Satori achieves state-of-the-art performance on mathematical reasoning benchmarks while exhibits strong generalization to out-of-domain tasks. Code, data, and models are fully open-sourced.f:T762,@inproceedings{satori2025,
  title = {Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search},
  author = {Maohao Shen and Guangtao Zeng and Zhenting Qi and Zhang-Wei Hong and Zhenfang Chen and Wei Lu and Gregory Wornell and Subhro Das and David Cox and Chuang Gan},
  year = {2025},
  month = {jul},
  booktitle = {International Conference on Machine Learning (ICML)},
  abstract = {Large language models (LLMs) have demonstrated remarkable reasoning capabilities across diverse domains. Recent studies have shown that increasing test-time computation enhances LLMs' reasoning capabilities. This typically involves extensive sampling at inference time guided by an external LLM verifier, resulting in a two-player system. Despite external guidance, the effectiveness of this system demonstrates the potential of a single LLM to tackle complex tasks. Thus, we pose a new research problem: Can we internalize the searching capabilities to fundamentally enhance the reasoning abilities of a single LLM? This work explores an orthogonal direction focusing on post-training LLMs for autoregressive searching (i.e., an extended reasoning process with self-reflection and self-exploration of new strategies). To achieve this, we propose the Chain-of-Action-Thought (COAT) reasoning and a two-stage training paradigm: 1) a small-scale format tuning stage to internalize the COAT reasoning format and 2) a large-scale self-improvement stage leveraging reinforcement learning. Our approach results in Satori, a 7B LLM trained on open-source models and data. Extensive empirical evaluations demonstrate that Satori achieves state-of-the-art performance on mathematical reasoning benchmarks while exhibits strong generalization to out-of-domain tasks. Code, data, and models are fully open-sourced.},
  arxiv = {https://arxiv.org/abs/2502.02508}
}10:T589,@inproceedings{mutual-reasoning2025,
  title = {rStar: Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solvers},
  author = {Zhenting Qi and Mingyuan Ma and Jiahang Xu and Li Lyna Zhang and Fan Yang and Mao Yang},
  year = {2025},
  month = {may},
  booktitle = {International Conference on Learning Representations (ICLR)},
  abstract = {This paper introduces rStar, a self-play mutual reasoning approach that significantly improves reasoning capabilities of small language models (SLMs) without fine-tuning or superior models. rStar decouples reasoning into a self-play mutual generation-discrimination process. First, a target SLM augments the Monte Carlo Tree Search (MCTS) with a rich set of human-like reasoning actions to construct higher quality reasoning trajectories. Next, another SLM, with capabilities similar to the target SLM, acts as a discriminator to verify each trajectory generated by the target SLM. The mutually agreed reasoning trajectories are considered mutual consistent, thus are more likely to be correct. Extensive experiments across five SLMs demonstrate rStar can effectively solve diverse reasoning problems, including GSM8K, GSM-Hard, MATH, SVAMP, and StrategyQA. Remarkably, rStar boosts GSM8K accuracy from 12.51\% to 63.91\% for LLaMA2-7B, from 36.46\% to 81.88\% for Mistral-7B, from 74.53\% to 91.13\% for LLaMA3-8B-Instruct.},
  arxiv = {https://arxiv.org/abs/2408.06195}
}11:T4ab,@inproceedings{data-extraction-rag2025,
  title = {Follow My Instruction and Spill the Beans: Scalable Data Extraction from Retrieval-Augmented Generation Systems},
  author = {Zhenting Qi and Hanlin Zhang and Eric Xing and Sham Kakade and Himabindu Lakkaraju},
  year = {2025},
  month = {may},
  booktitle = {International Conference on Learning Representations (ICLR)},
  abstract = {This paper presents a novel approach to scalable data extraction from retrieval-augmented generation (RAG) systems. By leveraging instruction-following capabilities of large language models, we design a framework that systematically generates prompts to guide LLMs in revealing embedded information, enabling scalable data extraction without direct access to the underlying retrieval mechanisms. Our method demonstrates effectiveness across various RAG architectures, highlighting potential vulnerabilities and emphasizing the need for robust data protection strategies in LLM applications. Through extensive experiments, we show that our approach can efficiently extract valuable information from RAG systems, raising important security and privacy concerns.},
  arxiv = {https://arxiv.org/abs/2402.17840}
}12:T5e5,While large language models (LLMs) have shown exceptional capabilities in understanding complex queries and performing sophisticated tasks, their generalization abilities are often deeply entangled with memorization, necessitating more precise evaluation. To address this challenge, we introduce Scylla, a dynamic evaluation framework that quantitatively measures the generalization abilities of LLMs. Scylla disentangles generalization from memorization via assessing model performance on both in-distribution (ID) and out-of-distribution (OOD) data through 20 tasks across 5 levels of complexity. Through extensive experiments, we uncover a non-monotonic relationship between task complexity and the performance gap between ID and OOD data, which we term the generalization valley. Specifically, this phenomenon reveals a critical threshold - referred to as critical complexity - where reliance on non-generalizable behavior peaks, indicating the upper bound of LLMs' generalization capabilities. As model size increases, the critical complexity shifts toward higher levels of task complexity, suggesting that larger models can handle more complex reasoning tasks before over-relying on memorization. Leveraging Scylla and the concept of critical complexity, we benchmark 28 LLMs including both open-sourced models such as LLaMA and Qwen families, and close-sourced models like Claude and GPT, providing a more robust evaluation and establishing a clearer understanding of LLMs' generalization capabilities.13:T7a4,@inproceedings{quantifying-generalization2025,
  title = {Quantifying Generalization Complexity for Large Language Models},
  author = {Zhenting Qi and Hongyin Luo and Xuliang Huang and Zhuokai Zhao and Yibo Jiang and Xiangjun Fan and Himabindu Lakkaraju and James Glass},
  year = {2025},
  month = {may},
  booktitle = {International Conference on Learning Representations (ICLR)},
  abstract = {While large language models (LLMs) have shown exceptional capabilities in understanding complex queries and performing sophisticated tasks, their generalization abilities are often deeply entangled with memorization, necessitating more precise evaluation. To address this challenge, we introduce Scylla, a dynamic evaluation framework that quantitatively measures the generalization abilities of LLMs. Scylla disentangles generalization from memorization via assessing model performance on both in-distribution (ID) and out-of-distribution (OOD) data through 20 tasks across 5 levels of complexity. Through extensive experiments, we uncover a non-monotonic relationship between task complexity and the performance gap between ID and OOD data, which we term the generalization valley. Specifically, this phenomenon reveals a critical threshold - referred to as critical complexity - where reliance on non-generalizable behavior peaks, indicating the upper bound of LLMs' generalization capabilities. As model size increases, the critical complexity shifts toward higher levels of task complexity, suggesting that larger models can handle more complex reasoning tasks before over-relying on memorization. Leveraging Scylla and the concept of critical complexity, we benchmark 28 LLMs including both open-sourced models such as LLaMA and Qwen families, and close-sourced models like Claude and GPT, providing a more robust evaluation and establishing a clearer understanding of LLMs' generalization capabilities.},
  arxiv = {https://arxiv.org/abs/2410.01769}
}0:{"P":null,"b":"q949WACoPgBxPEnVJjdV8","p":"","c":["",""],"i":false,"f":[[["",{"children":["__PAGE__",{}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/66c6e803a86cbc2f.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","className":"scroll-smooth","suppressHydrationWarning":true,"children":[["$","head",null,{"children":[["$","link",null,{"rel":"icon","href":"/favicon.svg","type":"image/svg+xml"}],["$","link",null,{"rel":"dns-prefetch","href":"https://google-fonts.jialeliu.com"}],["$","link",null,{"rel":"preconnect","href":"https://google-fonts.jialeliu.com","crossOrigin":""}],["$","link",null,{"rel":"preload","as":"style","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap"}],["$","link",null,{"rel":"stylesheet","id":"gfonts-css","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap","media":"print"}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n              (function(){\n                var l = document.getElementById('gfonts-css');\n                if (!l) return;\n                if (l.media !== 'all') {\n                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });\n                }\n              })();\n            "}}],["$","noscript",null,{"children":["$","link",null,{"rel":"stylesheet","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap"}]}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n              try {\n                const theme = localStorage.getItem('theme-storage');\n                const parsed = theme ? JSON.parse(theme) : null;\n                const setting = parsed?.state?.theme || 'system';\n                const prefersDark = typeof window !== 'undefined' && window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;\n                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));\n                var root = document.documentElement;\n                root.classList.add(effective);\n                root.setAttribute('data-theme', effective);\n              } catch (e) {\n                var root = document.documentElement;\n                root.classList.add('light');\n                root.setAttribute('data-theme', 'light');\n              }\n            "}}]]}],["$","body",null,{"className":"font-sans antialiased","children":["$","$L2",null,{"children":[["$","$L3",null,{"items":[{"title":"About","type":"page","target":"about","href":"/"},{"title":"Publications","type":"page","target":"publications","href":"/publications"}],"siteTitle":"Zhenting Qi","enableOnePageMode":false}],["$","main",null,{"className":"min-h-screen pt-16 lg:pt-20","children":["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}],["$","$L6",null,{"lastUpdated":"December 1, 2025"}]]}]}]]}]]}],{"children":["__PAGE__",["$","$1","c",{"children":[["$","div",null,{"className":"max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-8 bg-background min-h-screen","children":["$","div",null,{"className":"grid grid-cols-1 lg:grid-cols-3 gap-12","children":[["$","div",null,{"className":"lg:col-span-1","children":["$","$L7",null,{"author":{"name":"Zhenting Qi","title":"Ph.D. Student in Computer Science","institution":"Harvard University","avatar":"/bio.jpg"},"social":{"email":"zhentingqi@g.harvard.edu","location":"Cambridge, MA, USA","location_url":"https://maps.google.com","location_details":["Harvard University","Cambridge, MA, USA"],"google_scholar":"https://scholar.google.com/citations?hl=en&user=WZ00HCUAAAAJ","orcid":"","github":"https://github.com/zhentingqi","linkedin":"https://www.linkedin.com/in/zhentingqi","instagram":"https://www.instagram.com/heptacol/"}}]}],["$","div",null,{"className":"lg:col-span-2 space-y-8","children":[["$","section","about",{"id":"about","className":"scroll-mt-24 space-y-8","children":[[["$","$L8","about",{"content":"$9","title":"About"}],["$","$La","news",{"items":[{"date":"2025-11","content":"Will be joining **Meta FAIR** (Menlo Park office) as a Research Intern, working on multi-agent training."},{"date":"2025-09","content":"Our paper [_EvoLM: In Search of Lost Language Model Training Dynamics_](https://arxiv.org/abs/2506.16029) has been accepted to NeurIPS 2025 (oral)."},{"date":"2025-05","content":"Our paper [_Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search_](https://arxiv.org/abs/2502.02508) has been accepted to ICML 2025."},{"date":"2025-05","content":"Will be joining **Google DeepMind** (Mountain View office) as a Student Researcher, working on language model post-training."},{"date":"2025-04","content":"I will continue my research journey at **Harvard** as a PhD student!"},{"date":"2025-01","content":"Our papers [_Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solvers_](https://arxiv.org/abs/2408.06195), [_Quantifying Generalization Complexity for Large Language Models_](https://arxiv.org/abs/2410.01769), [_Follow My Instruction and Spill the Beans: Scalable Data Extraction from Retrieval-Augmented Generation Systems_](https://arxiv.org/abs/2402.17840) have been accepted to ICLR 2025."}],"title":"News"}],["$","$Lb","featured_publications",{"publications":[{"id":"evolm2025","title":"EvoLM: In Search of Lost Language Model Training Dynamics","authors":[{"name":"Zhenting Qi","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false},{"name":"Fan Nie","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Alexandre Alahi","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"James Zou","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Himabindu Lakkaraju","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yilun Du","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Eric Xing","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Sham Kakade","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Hanlin Zhang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2025,"month":"12","type":"conference","status":"published","tags":[],"keywords":"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:2:props:publications:0:tags","researchArea":"machine-learning","journal":"","conference":"Advances in Neural Information Processing Systems (NeurIPS)","abstract":"$c","description":"We developed a comprehensive model suite for analyzing language model training dynamics across pre-training, continued pre-training, supervised fine-tuning, and reinforcement learning stages.","selected":true,"awards":["Oral Presentation"],"arxivId":"2506.16029","url":"https://arxiv.org/abs/2506.16029","pdfUrl":"https://arxiv.org/pdf/2506.16029.pdf","bibtex":"$d"},{"id":"satori2025","title":"Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search","authors":[{"name":"Maohao Shen","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Guangtao Zeng","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Zhenting Qi","isHighlighted":true,"isCorresponding":true,"isCoAuthor":false},{"name":"Zhang-Wei Hong","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zhenfang Chen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Wei Lu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Gregory Wornell","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Subhro Das","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"David Cox","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Chuang Gan","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2025,"month":"7","type":"conference","status":"published","tags":[],"keywords":"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:2:props:publications:1:tags","researchArea":"machine-learning","journal":"","conference":"International Conference on Machine Learning (ICML)","abstract":"$e","description":"We introduced the COAT reasoning framework to enhance LLM reasoning via autoregressive search with self-reflection and self-exploration.","selected":true,"arxivId":"2502.02508","url":"https://arxiv.org/abs/2502.02508","pdfUrl":"https://arxiv.org/pdf/2502.02508.pdf","bibtex":"$f"},{"id":"mutual-reasoning2025","title":"rStar: Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solvers","authors":[{"name":"Zhenting Qi","isHighlighted":true,"isCorresponding":true,"isCoAuthor":false},{"name":"Mingyuan Ma","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Jiahang Xu","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Li Lyna Zhang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Fan Yang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Mao Yang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2025,"month":"5","type":"conference","status":"published","tags":[],"keywords":"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:2:props:publications:2:tags","researchArea":"machine-learning","journal":"","conference":"International Conference on Learning Representations (ICLR)","abstract":"This paper introduces rStar, a self-play mutual reasoning approach that significantly improves reasoning capabilities of small language models (SLMs) without fine-tuning or superior models. rStar decouples reasoning into a self-play mutual generation-discrimination process. First, a target SLM augments the Monte Carlo Tree Search (MCTS) with a rich set of human-like reasoning actions to construct higher quality reasoning trajectories. Next, another SLM, with capabilities similar to the target SLM, acts as a discriminator to verify each trajectory generated by the target SLM. The mutually agreed reasoning trajectories are considered mutual consistent, thus are more likely to be correct. Extensive experiments across five SLMs demonstrate rStar can effectively solve diverse reasoning problems, including GSM8K, GSM-Hard, MATH, SVAMP, and StrategyQA. Remarkably, rStar boosts GSM8K accuracy from 12.51% to 63.91% for LLaMA2-7B, from 36.46% to 81.88% for Mistral-7B, from 74.53% to 91.13% for LLaMA3-8B-Instruct.","description":"We introduced rStar, a self-play mutual reasoning approach that enhances reasoning capabilities of small language models without fine-tuning or superior models.","selected":true,"arxivId":"2408.06195","url":"https://arxiv.org/abs/2408.06195","pdfUrl":"https://arxiv.org/pdf/2408.06195.pdf","bibtex":"$10"},{"id":"data-extraction-rag2025","title":"Follow My Instruction and Spill the Beans: Scalable Data Extraction from Retrieval-Augmented Generation Systems","authors":[{"name":"Zhenting Qi","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false},{"name":"Hanlin Zhang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Eric Xing","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Sham Kakade","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Himabindu Lakkaraju","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2025,"month":"5","type":"conference","status":"published","tags":[],"keywords":"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:2:props:publications:3:tags","researchArea":"machine-learning","journal":"","conference":"International Conference on Learning Representations (ICLR)","abstract":"This paper presents a novel approach to scalable data extraction from retrieval-augmented generation (RAG) systems. By leveraging instruction-following capabilities of large language models, we design a framework that systematically generates prompts to guide LLMs in revealing embedded information, enabling scalable data extraction without direct access to the underlying retrieval mechanisms. Our method demonstrates effectiveness across various RAG architectures, highlighting potential vulnerabilities and emphasizing the need for robust data protection strategies in LLM applications. Through extensive experiments, we show that our approach can efficiently extract valuable information from RAG systems, raising important security and privacy concerns.","description":"We developed a scalable method for extracting data from RAG systems using LLMs' instruction-following capabilities.","selected":true,"arxivId":"2402.17840","url":"https://arxiv.org/abs/2402.17840","pdfUrl":"https://arxiv.org/pdf/2402.17840.pdf","bibtex":"$11"},{"id":"quantifying-generalization2025","title":"Quantifying Generalization Complexity for Large Language Models","authors":[{"name":"Zhenting Qi","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false},{"name":"Hongyin Luo","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Xuliang Huang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zhuokai Zhao","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yibo Jiang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Xiangjun Fan","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Himabindu Lakkaraju","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"James Glass","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2025,"month":"5","type":"conference","status":"published","tags":[],"keywords":"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:2:props:publications:4:tags","researchArea":"machine-learning","journal":"","conference":"International Conference on Learning Representations (ICLR)","abstract":"$12","description":"We introduced Scylla, a dynamic evaluation framework that quantitatively measures LLMs' generalization abilities by disentangling generalization from memorization.","selected":true,"arxivId":"2410.01769","url":"https://arxiv.org/abs/2410.01769","pdfUrl":"https://arxiv.org/pdf/2410.01769.pdf","bibtex":"$13"}],"title":"Selected Publications","enableOnePageMode":false}]],false,false,false]}]]}]]}]}],["$","$L14",null,{"children":"$L15"}],null,["$","$L16",null,{"children":["$L17","$L18",["$","$L19",null,{"promise":"$@1a"}]]}]]}],{},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","rwMGY3ORfJeA7EJx-dFx1",{"children":[["$","$L1b",null,{"children":"$L1c"}],null]}],null]}],false]],"m":"$undefined","G":["$1d","$undefined"],"s":false,"S":true}
1e:"$Sreact.suspense"
1f:I[4911,[],"AsyncMetadata"]
15:["$","$1e",null,{"fallback":null,"children":["$","$L1f",null,{"promise":"$@20"}]}]
18:null
1c:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
17:null
20:{"metadata":[["$","title","0",{"children":"Zhenting Qi"}],["$","meta","1",{"name":"description","content":"Computer Science Ph.D. student at Harvard University, researching intelligent and reliable AI systems"}],["$","meta","2",{"name":"author","content":"Zhenting Qi"}],["$","meta","3",{"name":"keywords","content":"Zhenting Qi,PhD,Research,Harvard University"}],["$","meta","4",{"name":"creator","content":"Zhenting Qi"}],["$","meta","5",{"name":"publisher","content":"Zhenting Qi"}],["$","meta","6",{"property":"og:title","content":"Zhenting Qi"}],["$","meta","7",{"property":"og:description","content":"Computer Science Ph.D. student at Harvard University, researching intelligent and reliable AI systems"}],["$","meta","8",{"property":"og:site_name","content":"Zhenting Qi's Academic Website"}],["$","meta","9",{"property":"og:locale","content":"en_US"}],["$","meta","10",{"property":"og:type","content":"website"}],["$","meta","11",{"name":"twitter:card","content":"summary"}],["$","meta","12",{"name":"twitter:title","content":"Zhenting Qi"}],["$","meta","13",{"name":"twitter:description","content":"Computer Science Ph.D. student at Harvard University, researching intelligent and reliable AI systems"}],["$","link","14",{"rel":"icon","href":"/favicon.svg"}]],"error":null,"digest":"$undefined"}
1a:{"metadata":"$20:metadata","error":null,"digest":"$undefined"}
