1:"$Sreact.fragment"
2:I[3719,["457","static/chunks/457-8b8d97145837e580.js","874","static/chunks/874-6cc630662f3664af.js","862","static/chunks/862-15038af301665bb3.js","177","static/chunks/app/layout-aaf17853ce9bc59e.js"],"ThemeProvider"]
3:I[768,["457","static/chunks/457-8b8d97145837e580.js","874","static/chunks/874-6cc630662f3664af.js","862","static/chunks/862-15038af301665bb3.js","177","static/chunks/app/layout-aaf17853ce9bc59e.js"],"default"]
4:I[7555,[],""]
5:I[1295,[],""]
6:I[2548,["457","static/chunks/457-8b8d97145837e580.js","874","static/chunks/874-6cc630662f3664af.js","862","static/chunks/862-15038af301665bb3.js","177","static/chunks/app/layout-aaf17853ce9bc59e.js"],"default"]
8:I[9665,[],"MetadataBoundary"]
a:I[9665,[],"OutletBoundary"]
d:I[4911,[],"AsyncMetadataOutlet"]
f:I[9665,[],"ViewportBoundary"]
11:I[6614,[],""]
:HL["/_next/static/css/1dd32553fc9feb8b.css","style"]
0:{"P":null,"b":"Z2zNg5oRfaUq6meuM-etJ","p":"","c":["","publications",""],"i":false,"f":[[["",{"children":[["slug","publications","d"],{"children":["__PAGE__",{}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/1dd32553fc9feb8b.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","className":"scroll-smooth","suppressHydrationWarning":true,"children":[["$","head",null,{"children":[["$","link",null,{"rel":"icon","href":"/favicon.svg","type":"image/svg+xml"}],["$","link",null,{"rel":"dns-prefetch","href":"https://google-fonts.jialeliu.com"}],["$","link",null,{"rel":"preconnect","href":"https://google-fonts.jialeliu.com","crossOrigin":""}],["$","link",null,{"rel":"preload","as":"style","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap"}],["$","link",null,{"rel":"stylesheet","id":"gfonts-css","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap","media":"print"}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n              (function(){\n                var l = document.getElementById('gfonts-css');\n                if (!l) return;\n                if (l.media !== 'all') {\n                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });\n                }\n              })();\n            "}}],["$","noscript",null,{"children":["$","link",null,{"rel":"stylesheet","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap"}]}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n              try {\n                const theme = localStorage.getItem('theme-storage');\n                const parsed = theme ? JSON.parse(theme) : null;\n                const setting = parsed?.state?.theme || 'system';\n                const prefersDark = typeof window !== 'undefined' && window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;\n                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));\n                var root = document.documentElement;\n                root.classList.add(effective);\n                root.setAttribute('data-theme', effective);\n              } catch (e) {\n                var root = document.documentElement;\n                root.classList.add('light');\n                root.setAttribute('data-theme', 'light');\n              }\n            "}}]]}],["$","body",null,{"className":"font-sans antialiased","children":["$","$L2",null,{"children":[["$","$L3",null,{"items":[{"title":"About","type":"page","target":"about","href":"/"},{"title":"Publications","type":"page","target":"publications","href":"/publications"}],"siteTitle":"Zhenting Qi","enableOnePageMode":false}],["$","main",null,{"className":"min-h-screen pt-16 lg:pt-20","children":["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}],["$","$L6",null,{"lastUpdated":"December 1, 2025"}]]}]}]]}]]}],{"children":[["slug","publications","d"],["$","$1","c",{"children":[null,["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["__PAGE__",["$","$1","c",{"children":["$L7",["$","$L8",null,{"children":"$L9"}],null,["$","$La",null,{"children":["$Lb","$Lc",["$","$Ld",null,{"promise":"$@e"}]]}]]}],{},null,false]},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","02iVyfckUDniFzQYNXUHi",{"children":[["$","$Lf",null,{"children":"$L10"}],null]}],null]}],false]],"m":"$undefined","G":["$11","$undefined"],"s":false,"S":true}
12:"$Sreact.suspense"
13:I[4911,[],"AsyncMetadata"]
15:I[6669,["457","static/chunks/457-8b8d97145837e580.js","485","static/chunks/485-487001f78a0503db.js","748","static/chunks/748-de23e927c4feb2c1.js","182","static/chunks/app/%5Bslug%5D/page-45e6a89798ec03fc.js"],"default"]
9:["$","$12",null,{"fallback":null,"children":["$","$L13",null,{"promise":"$@14"}]}]
16:T4a8,Modern language model (LM) training has been divided into multiple stages, making it difficult for downstream developers to evaluate the impact of design choices made at each stage. We present EvoLM, a model suite that enables systematic and transparent analysis of LMs' training dynamics across pre-training, continued pre-training, supervised fine-tuning, and reinforcement learning. We train over 100 LMs with 1B and 4B parameters from scratch, and evaluate both upstream (language modeling) and downstream (problem-solving) capabilities, including considerations of both in-domain and out-of-domain generalization. Key insights highlight the diminishing returns from excessive pre-training and post-training, the importance and practices of mitigating forgetting during domain-specific continued pre-training, the crucial role of continued pre-training in bridging pre-training and post-training phases, and various intricate trade-offs when configuring supervised fine-tuning and reinforcement learning. To facilitate open research and reproducibility, we release all pre-trained and post-trained models, training datasets for all stages, and our entire training and evaluation pipeline.17:T664,@inproceedings{evolm2025,
  title = {EvoLM: In Search of Lost Language Model Training Dynamics},
  author = {Zhenting Qi and Fan Nie and Alexandre Alahi and James Zou and Himabindu Lakkaraju and Yilun Du and Eric Xing and Sham Kakade and Hanlin Zhang},
  year = {2025},
  month = {dec},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  note = {Oral},
  abstract = {Modern language model (LM) training has been divided into multiple stages, making it difficult for downstream developers to evaluate the impact of design choices made at each stage. We present EvoLM, a model suite that enables systematic and transparent analysis of LMs' training dynamics across pre-training, continued pre-training, supervised fine-tuning, and reinforcement learning. We train over 100 LMs with 1B and 4B parameters from scratch, and evaluate both upstream (language modeling) and downstream (problem-solving) capabilities, including considerations of both in-domain and out-of-domain generalization. Key insights highlight the diminishing returns from excessive pre-training and post-training, the importance and practices of mitigating forgetting during domain-specific continued pre-training, the crucial role of continued pre-training in bridging pre-training and post-training phases, and various intricate trade-offs when configuring supervised fine-tuning and reinforcement learning. To facilitate open research and reproducibility, we release all pre-trained and post-trained models, training datasets for all stages, and our entire training and evaluation pipeline.},
  arxiv = {https://arxiv.org/abs/2506.16029}
}18:T4e9,Large Reasoning Models (LRMs) have significantly enhanced their capabilities in complex problem-solving by introducing a thinking draft that enables multi-path Chain-of-Thought explorations before producing final answers. Ensuring the faithfulness of these intermediate reasoning processes is crucial for reliable monitoring, interpretation, and effective control. In this paper, we propose a systematic counterfactual intervention framework to rigorously evaluate thinking draft faithfulness. Our approach focuses on two complementary dimensions: (1) Intra-Draft Faithfulness, which assesses whether individual reasoning steps causally influence subsequent steps and the final draft conclusion through counterfactual step insertions; and (2) Draft-to-Answer Faithfulness, which evaluates whether final answers are logically consistent with and dependent on the thinking draft, by perturbing the draft's concluding logic. We conduct extensive experiments across six state-of-the-art LRMs. Our findings show that current LRMs demonstrate selective faithfulness to intermediate reasoning steps and frequently fail to faithfully align with the draft conclusions. These results underscore the need for more faithful and interpretable reasoning in advanced LRMs.19:T668,@inproceedings{faithful-thinking-drafts2025,
  title = {Measuring the Faithfulness of Thinking Drafts in Large Reasoning Models},
  author = {Zidi Xiong and Shan Chen and Zhenting Qi and Himabindu Lakkaraju},
  year = {2025},
  month = {dec},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  abstract = {Large Reasoning Models (LRMs) have significantly enhanced their capabilities in complex problem-solving by introducing a thinking draft that enables multi-path Chain-of-Thought explorations before producing final answers. Ensuring the faithfulness of these intermediate reasoning processes is crucial for reliable monitoring, interpretation, and effective control. In this paper, we propose a systematic counterfactual intervention framework to rigorously evaluate thinking draft faithfulness. Our approach focuses on two complementary dimensions: (1) Intra-Draft Faithfulness, which assesses whether individual reasoning steps causally influence subsequent steps and the final draft conclusion through counterfactual step insertions; and (2) Draft-to-Answer Faithfulness, which evaluates whether final answers are logically consistent with and dependent on the thinking draft, by perturbing the draft's concluding logic. We conduct extensive experiments across six state-of-the-art LRMs. Our findings show that current LRMs demonstrate selective faithfulness to intermediate reasoning steps and frequently fail to faithfully align with the draft conclusions. These results underscore the need for more faithful and interpretable reasoning in advanced LRMs.},
  arxiv = {https://arxiv.org/abs/2505.13774}
}1a:T57b,Large language models (LLMs) have demonstrated remarkable reasoning capabilities across diverse domains. Recent studies have shown that increasing test-time computation enhances LLMs' reasoning capabilities. This typically involves extensive sampling at inference time guided by an external LLM verifier, resulting in a two-player system. Despite external guidance, the effectiveness of this system demonstrates the potential of a single LLM to tackle complex tasks. Thus, we pose a new research problem: Can we internalize the searching capabilities to fundamentally enhance the reasoning abilities of a single LLM? This work explores an orthogonal direction focusing on post-training LLMs for autoregressive searching (i.e., an extended reasoning process with self-reflection and self-exploration of new strategies). To achieve this, we propose the Chain-of-Action-Thought (COAT) reasoning and a two-stage training paradigm: 1) a small-scale format tuning stage to internalize the COAT reasoning format and 2) a large-scale self-improvement stage leveraging reinforcement learning. Our approach results in Satori, a 7B LLM trained on open-source models and data. Extensive empirical evaluations demonstrate that Satori achieves state-of-the-art performance on mathematical reasoning benchmarks while exhibits strong generalization to out-of-domain tasks. Code, data, and models are fully open-sourced.1b:T762,@inproceedings{satori2025,
  title = {Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search},
  author = {Maohao Shen and Guangtao Zeng and Zhenting Qi and Zhang-Wei Hong and Zhenfang Chen and Wei Lu and Gregory Wornell and Subhro Das and David Cox and Chuang Gan},
  year = {2025},
  month = {jul},
  booktitle = {International Conference on Machine Learning (ICML)},
  abstract = {Large language models (LLMs) have demonstrated remarkable reasoning capabilities across diverse domains. Recent studies have shown that increasing test-time computation enhances LLMs' reasoning capabilities. This typically involves extensive sampling at inference time guided by an external LLM verifier, resulting in a two-player system. Despite external guidance, the effectiveness of this system demonstrates the potential of a single LLM to tackle complex tasks. Thus, we pose a new research problem: Can we internalize the searching capabilities to fundamentally enhance the reasoning abilities of a single LLM? This work explores an orthogonal direction focusing on post-training LLMs for autoregressive searching (i.e., an extended reasoning process with self-reflection and self-exploration of new strategies). To achieve this, we propose the Chain-of-Action-Thought (COAT) reasoning and a two-stage training paradigm: 1) a small-scale format tuning stage to internalize the COAT reasoning format and 2) a large-scale self-improvement stage leveraging reinforcement learning. Our approach results in Satori, a 7B LLM trained on open-source models and data. Extensive empirical evaluations demonstrate that Satori achieves state-of-the-art performance on mathematical reasoning benchmarks while exhibits strong generalization to out-of-domain tasks. Code, data, and models are fully open-sourced.},
  arxiv = {https://arxiv.org/abs/2502.02508}
}1c:T589,@inproceedings{mutual-reasoning2025,
  title = {rStar: Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solvers},
  author = {Zhenting Qi and Mingyuan Ma and Jiahang Xu and Li Lyna Zhang and Fan Yang and Mao Yang},
  year = {2025},
  month = {may},
  booktitle = {International Conference on Learning Representations (ICLR)},
  abstract = {This paper introduces rStar, a self-play mutual reasoning approach that significantly improves reasoning capabilities of small language models (SLMs) without fine-tuning or superior models. rStar decouples reasoning into a self-play mutual generation-discrimination process. First, a target SLM augments the Monte Carlo Tree Search (MCTS) with a rich set of human-like reasoning actions to construct higher quality reasoning trajectories. Next, another SLM, with capabilities similar to the target SLM, acts as a discriminator to verify each trajectory generated by the target SLM. The mutually agreed reasoning trajectories are considered mutual consistent, thus are more likely to be correct. Extensive experiments across five SLMs demonstrate rStar can effectively solve diverse reasoning problems, including GSM8K, GSM-Hard, MATH, SVAMP, and StrategyQA. Remarkably, rStar boosts GSM8K accuracy from 12.51\% to 63.91\% for LLaMA2-7B, from 36.46\% to 81.88\% for Mistral-7B, from 74.53\% to 91.13\% for LLaMA3-8B-Instruct.},
  arxiv = {https://arxiv.org/abs/2408.06195}
}1d:T4ab,@inproceedings{data-extraction-rag2025,
  title = {Follow My Instruction and Spill the Beans: Scalable Data Extraction from Retrieval-Augmented Generation Systems},
  author = {Zhenting Qi and Hanlin Zhang and Eric Xing and Sham Kakade and Himabindu Lakkaraju},
  year = {2025},
  month = {may},
  booktitle = {International Conference on Learning Representations (ICLR)},
  abstract = {This paper presents a novel approach to scalable data extraction from retrieval-augmented generation (RAG) systems. By leveraging instruction-following capabilities of large language models, we design a framework that systematically generates prompts to guide LLMs in revealing embedded information, enabling scalable data extraction without direct access to the underlying retrieval mechanisms. Our method demonstrates effectiveness across various RAG architectures, highlighting potential vulnerabilities and emphasizing the need for robust data protection strategies in LLM applications. Through extensive experiments, we show that our approach can efficiently extract valuable information from RAG systems, raising important security and privacy concerns.},
  arxiv = {https://arxiv.org/abs/2402.17840}
}1e:T5e5,While large language models (LLMs) have shown exceptional capabilities in understanding complex queries and performing sophisticated tasks, their generalization abilities are often deeply entangled with memorization, necessitating more precise evaluation. To address this challenge, we introduce Scylla, a dynamic evaluation framework that quantitatively measures the generalization abilities of LLMs. Scylla disentangles generalization from memorization via assessing model performance on both in-distribution (ID) and out-of-distribution (OOD) data through 20 tasks across 5 levels of complexity. Through extensive experiments, we uncover a non-monotonic relationship between task complexity and the performance gap between ID and OOD data, which we term the generalization valley. Specifically, this phenomenon reveals a critical threshold - referred to as critical complexity - where reliance on non-generalizable behavior peaks, indicating the upper bound of LLMs' generalization capabilities. As model size increases, the critical complexity shifts toward higher levels of task complexity, suggesting that larger models can handle more complex reasoning tasks before over-relying on memorization. Leveraging Scylla and the concept of critical complexity, we benchmark 28 LLMs including both open-sourced models such as LLaMA and Qwen families, and close-sourced models like Claude and GPT, providing a more robust evaluation and establishing a clearer understanding of LLMs' generalization capabilities.1f:T7a4,@inproceedings{quantifying-generalization2025,
  title = {Quantifying Generalization Complexity for Large Language Models},
  author = {Zhenting Qi and Hongyin Luo and Xuliang Huang and Zhuokai Zhao and Yibo Jiang and Xiangjun Fan and Himabindu Lakkaraju and James Glass},
  year = {2025},
  month = {may},
  booktitle = {International Conference on Learning Representations (ICLR)},
  abstract = {While large language models (LLMs) have shown exceptional capabilities in understanding complex queries and performing sophisticated tasks, their generalization abilities are often deeply entangled with memorization, necessitating more precise evaluation. To address this challenge, we introduce Scylla, a dynamic evaluation framework that quantitatively measures the generalization abilities of LLMs. Scylla disentangles generalization from memorization via assessing model performance on both in-distribution (ID) and out-of-distribution (OOD) data through 20 tasks across 5 levels of complexity. Through extensive experiments, we uncover a non-monotonic relationship between task complexity and the performance gap between ID and OOD data, which we term the generalization valley. Specifically, this phenomenon reveals a critical threshold - referred to as critical complexity - where reliance on non-generalizable behavior peaks, indicating the upper bound of LLMs' generalization capabilities. As model size increases, the critical complexity shifts toward higher levels of task complexity, suggesting that larger models can handle more complex reasoning tasks before over-relying on memorization. Leveraging Scylla and the concept of critical complexity, we benchmark 28 LLMs including both open-sourced models such as LLaMA and Qwen families, and close-sourced models like Claude and GPT, providing a more robust evaluation and establishing a clearer understanding of LLMs' generalization capabilities.},
  arxiv = {https://arxiv.org/abs/2410.01769}
}20:T55d,@inproceedings{folio2024,
  title = {FOLIO: Natural Language Reasoning with First-Order Logic},
  author = {Simeng Han and Hailey Schoelkopf and Yilun Zhao and Zhenting Qi and Martin Riddell and Wenfei Zhou and James Coady and David Peng and Yujie Qiao and Luke Benson and Lucy Sun and Alex Wardle-Solano and Hannah Szabo and Ekaterina Zubova and Matthew Burtell and Jonathan Fan and Yixin Liu and Brian Wong and Malcolm Sailor and Ansong Ni and Linyong Nan and Jungo Kasai and Tao Yu and Rui Zhang and Alexander R. Fabbri and Wojciech Kryscinski and Semih Yavuz and Ye Liu and Xi Victoria Lin and Shafiq Joty and Yingbo Zhou and Caiming Xiong and Rex Ying and Arman Cohan and Dragomir Radev},
  year = {2024},
  month = {dec},
  booktitle = {Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  abstract = {FOLIO presents a dataset and benchmark for evaluating natural language reasoning capabilities of AI systems using First-Order Logic (FOL). The dataset comprises a wide range of reasoning tasks that require understanding and applying FOL principles to natural language statements. FOLIO serves as a valuable resource for developing and assessing models capable of logical reasoning in natural language contexts, advancing research in integrating formal logic with natural language understanding.},
  arxiv = {https://arxiv.org/abs/2209.00840}
}21:T472,@inproceedings{p-folio2024,
  title = {P-FOLIO: Evaluating and Improving Logical Reasoning with Abundant Human-Written Reasoning Chains},
  author = {Simeng Han and Aaron Yu and Rui Shen and Zhenting Qi and Martin Riddell and Wenfei Zhou and Yujie Qiao and Yilun Zhao and Semih Yavuz and Ye Liu and Shafiq Joty and Yingbo Zhou and Caiming Xiong and Dragomir Radev and Rex Ying and Arman Cohan},
  year = {2024},
  month = {dec},
  booktitle = {Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  note = {Findings},
  abstract = {P-FOLIO extends the FOLIO dataset by providing abundant human-written reasoning chains for evaluating and improving logical reasoning capabilities. The dataset includes detailed reasoning processes that demonstrate how humans approach logical reasoning tasks, providing valuable training data and evaluation benchmarks for developing more capable reasoning models. P-FOLIO enables researchers to better understand and improve the logical reasoning capabilities of AI systems through access to high-quality human reasoning demonstrations.},
  arxiv = {https://arxiv.org/abs/2410.09207}
}22:T450,@inproceedings{constrained-human-ai-cooperation2024,
  title = {Constrained Human-AI Cooperation: An Inclusive Embodied Social Intelligence Challenge},
  author = {Weihua Du and Qiushi Lyu and Jiaming Shan and Zhenting Qi and Hongxin Zhang and Sunli Chen and Andi Peng and Tianmin Shu and Kwonjoon Lee and Behzad Dariush and Chuang Gan},
  year = {2024},
  month = {dec},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  note = {Dataset and Benchmark Track},
  abstract = {We present the Constrained Human-AI Cooperation (CHAIC) challenge, an inclusive benchmark designed to advance research in embodied social intelligence. The challenge presents scenarios where AI agents must collaborate with humans under specific constraints, testing their ability to understand and adapt to human behaviors and social norms. The benchmark includes a diverse set of tasks designed to evaluate various aspects of social intelligence in embodied AI systems, including communication, cooperation, and adaptation in constrained environments.},
  arxiv = {https://arxiv.org/abs/2411.01796}
}23:T451,@inproceedings{self-criticism2023,
  title = {Self-Criticism: Aligning Large Language Models with their Understanding of Helpfulness, Honesty, and Harmlessness},
  author = {Xiaoyu Tan and Shaojie Shi and Xihe Qiu and Chao Qu and Zhenting Qi and Yinghui Xu and Yuan Qi},
  year = {2023},
  month = {dec},
  booktitle = {Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  note = {Industry Track (Oral)},
  abstract = {Self-Criticism presents a novel approach to align large language models with their understanding of helpfulness, honesty, and harmlessness through self-evaluation mechanisms. The method enables models to critique and improve their own outputs by applying their understanding of these core values. By incorporating self-criticism into the training and inference process, we demonstrate improved alignment with human values and better adherence to ethical guidelines. The approach leverages the model's own understanding of what constitutes helpful, honest, and harmless behavior to guide its responses.},
  url = {https://aclanthology.org/2023.emnlp-industry.62/}
}24:T415,@inproceedings{safer2023,
  title = {SaFER: A Robust and Efficient Framework for Fine-tuning BERT-based Classifier with Noisy Labels},
  author = {Zhenting Qi and Xiaoyu Tan and Chao Qu and Yinghui Xu and Yuan Qi},
  year = {2023},
  month = {jul},
  booktitle = {Annual Meeting of the Association for Computational Linguistics (ACL)},
  note = {Industry Track},
  abstract = {SaFER introduces a robust and efficient framework for fine-tuning BERT-based classifiers when dealing with noisy labels. The framework addresses the challenge of label noise in real-world datasets by incorporating noise-robust training strategies and efficient fine-tuning techniques. SaFER demonstrates improved performance on noisy datasets while maintaining computational efficiency, making it suitable for practical applications where label quality cannot be guaranteed. The approach combines noise detection, label correction, and robust training methods to achieve reliable classification performance.},
  url = {https://aclanthology.org/2023.acl-industry.38/}
}25:T424,@inproceedings{loft2023,
  title = {LoFT: Enhancing Faithfulness and Diversity for Table-to-Text Generation via Logic Form Control},
  author = {Yilun Zhao and Zhenting Qi and Linyong Nan and Lorenzo Jaime Flores and Dragomir Radev},
  year = {2023},
  month = {feb},
  booktitle = {Conference of the European Chapter of the Association for Computational Linguistics (EACL)},
  note = {Short Paper (Oral)},
  abstract = {LoFT introduces a novel approach to enhance faithfulness and diversity in table-to-text generation through logic form control. The method leverages logical representations to guide the generation process, ensuring that generated text accurately reflects the underlying table data while maintaining diversity. By incorporating logic form constraints, LoFT addresses common issues in table-to-text generation such as factual errors and repetitive outputs. The approach demonstrates improved faithfulness to source tables and increased diversity in generated text compared to baseline methods.},
  arxiv = {https://arxiv.org/abs/2302.02962}
}26:T401,@inproceedings{reastap2022,
  title = {ReasTAP: Injecting Table Reasoning Skills During Pre-training via Synthetic Reasoning Examples},
  author = {Yilun Zhao and Linyong Nan and Zhenting Qi and Rui Zhang and Dragomir Radev},
  year = {2022},
  month = {dec},
  booktitle = {Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  abstract = {ReasTAP introduces a method to inject table reasoning skills during pre-training by generating and incorporating synthetic reasoning examples. The approach creates diverse reasoning scenarios that teach models to understand and reason over tabular data during the pre-training phase. By exposing models to synthetic reasoning examples, ReasTAP improves their ability to handle table-related tasks without requiring extensive task-specific fine-tuning. The method demonstrates improved performance on downstream table reasoning tasks, showing the effectiveness of incorporating reasoning skills during pre-training.},
  arxiv = {https://arxiv.org/abs/2210.12374}
}7:["$","div",null,{"className":"max-w-4xl mx-auto px-4 sm:px-6 lg:px-8 py-12","children":[["$","$L15",null,{"config":{"type":"publication","title":"Publications","description":"A collection of my research work.","source":"publications.bib"},"publications":[{"id":"evolm2025","title":"EvoLM: In Search of Lost Language Model Training Dynamics","authors":[{"name":"Zhenting Qi","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Fan Nie","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Alexandre Alahi","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"James Zou","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Himabindu Lakkaraju","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yilun Du","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Eric Xing","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Sham Kakade","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Hanlin Zhang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2025,"month":"12","type":"conference","status":"published","tags":[],"keywords":"$7:props:children:0:props:publications:0:tags","researchArea":"machine-learning","journal":"","conference":"Advances in Neural Information Processing Systems (NeurIPS)","abstract":"$16","description":"We developed a comprehensive model suite for analyzing language model training dynamics across pre-training, continued pre-training, supervised fine-tuning, and reinforcement learning stages.","selected":true,"awards":["Oral Presentation"],"arxivId":"2506.16029","url":"https://arxiv.org/abs/2506.16029","pdfUrl":"https://arxiv.org/pdf/2506.16029.pdf","bibtex":"$17"},{"id":"faithful-thinking-drafts2025","title":"Measuring the Faithfulness of Thinking Drafts in Large Reasoning Models","authors":[{"name":"Zidi Xiong","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Shan Chen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zhenting Qi","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Himabindu Lakkaraju","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2025,"month":"12","type":"conference","status":"published","tags":[],"keywords":"$7:props:children:0:props:publications:1:tags","researchArea":"machine-learning","journal":"","conference":"Advances in Neural Information Processing Systems (NeurIPS)","abstract":"$18","description":"We introduced a systematic framework to evaluate the faithfulness of thinking drafts in Large Reasoning Models using counterfactual interventions.","selected":false,"arxivId":"2505.13774","url":"https://arxiv.org/abs/2505.13774","pdfUrl":"https://arxiv.org/pdf/2505.13774.pdf","bibtex":"$19"},{"id":"satori2025","title":"Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search","authors":[{"name":"Maohao Shen","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Guangtao Zeng","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Zhenting Qi","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Zhang-Wei Hong","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zhenfang Chen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Wei Lu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Gregory Wornell","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Subhro Das","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"David Cox","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Chuang Gan","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2025,"month":"7","type":"conference","status":"published","tags":[],"keywords":"$7:props:children:0:props:publications:2:tags","researchArea":"machine-learning","journal":"","conference":"International Conference on Machine Learning (ICML)","abstract":"$1a","description":"We introduced the COAT reasoning framework to enhance LLM reasoning via autoregressive search with self-reflection and self-exploration.","selected":true,"arxivId":"2502.02508","url":"https://arxiv.org/abs/2502.02508","pdfUrl":"https://arxiv.org/pdf/2502.02508.pdf","bibtex":"$1b"},{"id":"mutual-reasoning2025","title":"rStar: Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solvers","authors":[{"name":"Zhenting Qi","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Mingyuan Ma","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Jiahang Xu","isHighlighted":false,"isCorresponding":true,"isCoAuthor":false},{"name":"Li Lyna Zhang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Fan Yang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Mao Yang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2025,"month":"5","type":"conference","status":"published","tags":[],"keywords":"$7:props:children:0:props:publications:3:tags","researchArea":"machine-learning","journal":"","conference":"International Conference on Learning Representations (ICLR)","abstract":"This paper introduces rStar, a self-play mutual reasoning approach that significantly improves reasoning capabilities of small language models (SLMs) without fine-tuning or superior models. rStar decouples reasoning into a self-play mutual generation-discrimination process. First, a target SLM augments the Monte Carlo Tree Search (MCTS) with a rich set of human-like reasoning actions to construct higher quality reasoning trajectories. Next, another SLM, with capabilities similar to the target SLM, acts as a discriminator to verify each trajectory generated by the target SLM. The mutually agreed reasoning trajectories are considered mutual consistent, thus are more likely to be correct. Extensive experiments across five SLMs demonstrate rStar can effectively solve diverse reasoning problems, including GSM8K, GSM-Hard, MATH, SVAMP, and StrategyQA. Remarkably, rStar boosts GSM8K accuracy from 12.51% to 63.91% for LLaMA2-7B, from 36.46% to 81.88% for Mistral-7B, from 74.53% to 91.13% for LLaMA3-8B-Instruct.","description":"We introduced rStar, a self-play mutual reasoning approach that enhances reasoning capabilities of small language models without fine-tuning or superior models.","selected":true,"arxivId":"2408.06195","url":"https://arxiv.org/abs/2408.06195","pdfUrl":"https://arxiv.org/pdf/2408.06195.pdf","bibtex":"$1c"},{"id":"data-extraction-rag2025","title":"Follow My Instruction and Spill the Beans: Scalable Data Extraction from Retrieval-Augmented Generation Systems","authors":[{"name":"Zhenting Qi","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Hanlin Zhang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Eric Xing","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Sham Kakade","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Himabindu Lakkaraju","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2025,"month":"5","type":"conference","status":"published","tags":[],"keywords":"$7:props:children:0:props:publications:4:tags","researchArea":"machine-learning","journal":"","conference":"International Conference on Learning Representations (ICLR)","abstract":"This paper presents a novel approach to scalable data extraction from retrieval-augmented generation (RAG) systems. By leveraging instruction-following capabilities of large language models, we design a framework that systematically generates prompts to guide LLMs in revealing embedded information, enabling scalable data extraction without direct access to the underlying retrieval mechanisms. Our method demonstrates effectiveness across various RAG architectures, highlighting potential vulnerabilities and emphasizing the need for robust data protection strategies in LLM applications. Through extensive experiments, we show that our approach can efficiently extract valuable information from RAG systems, raising important security and privacy concerns.","description":"We developed a scalable method for extracting data from RAG systems using LLMs' instruction-following capabilities.","selected":true,"arxivId":"2402.17840","url":"https://arxiv.org/abs/2402.17840","pdfUrl":"https://arxiv.org/pdf/2402.17840.pdf","bibtex":"$1d"},{"id":"quantifying-generalization2025","title":"Quantifying Generalization Complexity for Large Language Models","authors":[{"name":"Zhenting Qi","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Hongyin Luo","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Xuliang Huang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zhuokai Zhao","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yibo Jiang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Xiangjun Fan","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Himabindu Lakkaraju","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"James Glass","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2025,"month":"5","type":"conference","status":"published","tags":[],"keywords":"$7:props:children:0:props:publications:5:tags","researchArea":"machine-learning","journal":"","conference":"International Conference on Learning Representations (ICLR)","abstract":"$1e","description":"We introduced Scylla, a dynamic evaluation framework that quantitatively measures LLMs' generalization abilities by disentangling generalization from memorization.","selected":true,"arxivId":"2410.01769","url":"https://arxiv.org/abs/2410.01769","pdfUrl":"https://arxiv.org/pdf/2410.01769.pdf","bibtex":"$1f"},{"id":"folio2024","title":"FOLIO: Natural Language Reasoning with First-Order Logic","authors":[{"name":"Simeng Han","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Hailey Schoelkopf","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yilun Zhao","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zhenting Qi","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Martin Riddell","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Wenfei Zhou","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"James Coady","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"David Peng","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yujie Qiao","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Luke Benson","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Lucy Sun","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Alex Wardle-Solano","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Hannah Szabo","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Ekaterina Zubova","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Matthew Burtell","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Jonathan Fan","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yixin Liu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Brian Wong","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Malcolm Sailor","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Ansong Ni","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Linyong Nan","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Jungo Kasai","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Tao Yu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Rui Zhang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Alexander R. Fabbri","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Wojciech Kryscinski","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Semih Yavuz","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Ye Liu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Xi Victoria Lin","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Shafiq Joty","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yingbo Zhou","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Caiming Xiong","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Rex Ying","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Arman Cohan","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Dragomir Radev","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2024,"month":"12","type":"conference","status":"published","tags":[],"keywords":"$7:props:children:0:props:publications:6:tags","researchArea":"machine-learning","journal":"","conference":"Conference on Empirical Methods in Natural Language Processing (EMNLP)","abstract":"FOLIO presents a dataset and benchmark for evaluating natural language reasoning capabilities of AI systems using First-Order Logic (FOL). The dataset comprises a wide range of reasoning tasks that require understanding and applying FOL principles to natural language statements. FOLIO serves as a valuable resource for developing and assessing models capable of logical reasoning in natural language contexts, advancing research in integrating formal logic with natural language understanding.","description":"We developed a comprehensive dataset and benchmark for natural language reasoning using First-Order Logic.","selected":false,"arxivId":"2209.00840","url":"https://arxiv.org/abs/2209.00840","pdfUrl":"https://arxiv.org/pdf/2209.00840.pdf","bibtex":"$20"},{"id":"p-folio2024","title":"P-FOLIO: Evaluating and Improving Logical Reasoning with Abundant Human-Written Reasoning Chains","authors":[{"name":"Simeng Han","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Aaron Yu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Rui Shen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zhenting Qi","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Martin Riddell","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Wenfei Zhou","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yujie Qiao","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yilun Zhao","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Semih Yavuz","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Ye Liu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Shafiq Joty","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yingbo Zhou","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Caiming Xiong","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Dragomir Radev","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Rex Ying","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Arman Cohan","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2024,"month":"12","type":"conference","status":"published","tags":[],"keywords":"$7:props:children:0:props:publications:7:tags","researchArea":"machine-learning","journal":"","conference":"Conference on Empirical Methods in Natural Language Processing (EMNLP)","abstract":"P-FOLIO extends the FOLIO dataset by providing abundant human-written reasoning chains for evaluating and improving logical reasoning capabilities. The dataset includes detailed reasoning processes that demonstrate how humans approach logical reasoning tasks, providing valuable training data and evaluation benchmarks for developing more capable reasoning models. P-FOLIO enables researchers to better understand and improve the logical reasoning capabilities of AI systems through access to high-quality human reasoning demonstrations.","description":"We extended FOLIO with abundant human-written reasoning chains, providing detailed reasoning processes for logical reasoning tasks.","selected":false,"arxivId":"2410.09207","url":"https://arxiv.org/abs/2410.09207","pdfUrl":"https://arxiv.org/pdf/2410.09207.pdf","bibtex":"$21"},{"id":"constrained-human-ai-cooperation2024","title":"Constrained Human-AI Cooperation: An Inclusive Embodied Social Intelligence Challenge","authors":[{"name":"Weihua Du","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Qiushi Lyu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Jiaming Shan","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zhenting Qi","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Hongxin Zhang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Sunli Chen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Andi Peng","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Tianmin Shu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Kwonjoon Lee","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Behzad Dariush","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Chuang Gan","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2024,"month":"12","type":"conference","status":"published","tags":[],"keywords":"$7:props:children:0:props:publications:8:tags","researchArea":"machine-learning","journal":"","conference":"Advances in Neural Information Processing Systems (NeurIPS)","abstract":"We present the Constrained Human-AI Cooperation (CHAIC) challenge, an inclusive benchmark designed to advance research in embodied social intelligence. The challenge presents scenarios where AI agents must collaborate with humans under specific constraints, testing their ability to understand and adapt to human behaviors and social norms. The benchmark includes a diverse set of tasks designed to evaluate various aspects of social intelligence in embodied AI systems, including communication, cooperation, and adaptation in constrained environments.","description":"We introduced a comprehensive benchmark challenge for advancing research in embodied social intelligence through constrained human-AI cooperation scenarios.","selected":false,"arxivId":"2411.01796","url":"https://arxiv.org/abs/2411.01796","pdfUrl":"https://arxiv.org/pdf/2411.01796.pdf","bibtex":"$22"},{"id":"pillow2023","title":"PILLOW: Enhancing Efficient Instruction Fine-tuning via Prompt Matching","authors":[{"name":"Zhenting Qi","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Xiaoyu Tan","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Shaojie Shi","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Chao Qu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yinghui Xu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yuan Qi","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2023,"month":"12","type":"conference","status":"published","tags":[],"keywords":"$7:props:children:0:props:publications:9:tags","researchArea":"machine-learning","journal":"","conference":"Conference on Empirical Methods in Natural Language Processing (EMNLP)","abstract":"PILLOW introduces a novel approach to enhance efficient instruction fine-tuning through prompt matching. The method leverages prompt similarity and matching techniques to improve the effectiveness of instruction fine-tuning while maintaining computational efficiency. By identifying and matching similar prompts during training, PILLOW enables more efficient use of training data and better generalization to new instructions. The approach demonstrates significant improvements in instruction following capabilities while reducing computational costs compared to standard fine-tuning methods.","description":"We introduced a prompt matching framework to enhance the efficiency of instruction fine-tuning.","selected":false,"awards":["Oral Presentation"],"arxivId":"2312.05621","url":"https://arxiv.org/abs/2312.05621","pdfUrl":"https://arxiv.org/pdf/2312.05621.pdf","bibtex":"@inproceedings{pillow2023,\n  title = {PILLOW: Enhancing Efficient Instruction Fine-tuning via Prompt Matching},\n  author = {Zhenting Qi and Xiaoyu Tan and Shaojie Shi and Chao Qu and Yinghui Xu and Yuan Qi},\n  year = {2023},\n  month = {dec},\n  booktitle = {Conference on Empirical Methods in Natural Language Processing (EMNLP)},\n  note = {Industry Track (Oral)},\n  abstract = {PILLOW introduces a novel approach to enhance efficient instruction fine-tuning through prompt matching. The method leverages prompt similarity and matching techniques to improve the effectiveness of instruction fine-tuning while maintaining computational efficiency. By identifying and matching similar prompts during training, PILLOW enables more efficient use of training data and better generalization to new instructions. The approach demonstrates significant improvements in instruction following capabilities while reducing computational costs compared to standard fine-tuning methods.},\n  arxiv = {https://arxiv.org/abs/2312.05621}\n}"},{"id":"qtsumm2023","title":"QTSumm: A New Benchmark for Query-Focused Table Summarization","authors":[{"name":"Yilun Zhao","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zhenting Qi","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Linyong Nan","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Boyu Mi","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yixin Liu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Weijin Zou","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Simeng Han","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Xiangru Tang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yumo Xu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Arman Cohan","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Dragomir Radev","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2023,"month":"12","type":"conference","status":"published","tags":[],"keywords":"$7:props:children:0:props:publications:10:tags","researchArea":"machine-learning","journal":"","conference":"Conference on Empirical Methods in Natural Language Processing (EMNLP)","abstract":"QTSumm introduces a new benchmark for query-focused table summarization, a task that requires generating summaries of tabular data in response to specific queries. The benchmark includes diverse tables and queries, covering various domains and complexity levels. QTSumm enables systematic evaluation of models' abilities to understand tabular data, interpret queries, and generate relevant summaries. The dataset provides a foundation for advancing research in table understanding and query-focused summarization.","description":"We introduced a comprehensive benchmark dataset for query-focused table summarization.","selected":false,"arxivId":"2305.14303","url":"https://arxiv.org/abs/2305.14303","pdfUrl":"https://arxiv.org/pdf/2305.14303.pdf","bibtex":"@inproceedings{qtsumm2023,\n  title = {QTSumm: A New Benchmark for Query-Focused Table Summarization},\n  author = {Yilun Zhao and Zhenting Qi and Linyong Nan and Boyu Mi and Yixin Liu and Weijin Zou and Simeng Han and Xiangru Tang and Yumo Xu and Arman Cohan and Dragomir Radev},\n  year = {2023},\n  month = {dec},\n  booktitle = {Conference on Empirical Methods in Natural Language Processing (EMNLP)},\n  abstract = {QTSumm introduces a new benchmark for query-focused table summarization, a task that requires generating summaries of tabular data in response to specific queries. The benchmark includes diverse tables and queries, covering various domains and complexity levels. QTSumm enables systematic evaluation of models' abilities to understand tabular data, interpret queries, and generate relevant summaries. The dataset provides a foundation for advancing research in table understanding and query-focused summarization.},\n  arxiv = {https://arxiv.org/abs/2305.14303}\n}"},{"id":"self-criticism2023","title":"Self-Criticism: Aligning Large Language Models with their Understanding of Helpfulness, Honesty, and Harmlessness","authors":[{"name":"Xiaoyu Tan","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Shaojie Shi","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Xihe Qiu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Chao Qu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zhenting Qi","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yinghui Xu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yuan Qi","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2023,"month":"12","type":"conference","status":"published","tags":[],"keywords":"$7:props:children:0:props:publications:11:tags","researchArea":"machine-learning","journal":"","conference":"Conference on Empirical Methods in Natural Language Processing (EMNLP)","abstract":"Self-Criticism presents a novel approach to align large language models with their understanding of helpfulness, honesty, and harmlessness through self-evaluation mechanisms. The method enables models to critique and improve their own outputs by applying their understanding of these core values. By incorporating self-criticism into the training and inference process, we demonstrate improved alignment with human values and better adherence to ethical guidelines. The approach leverages the model's own understanding of what constitutes helpful, honest, and harmless behavior to guide its responses.","description":"We introduced a self-criticism framework that enables models to evaluate and improve their outputs based on their understanding of helpfulness, honesty, and harmlessness.","selected":false,"awards":["Oral Presentation"],"url":"https://aclanthology.org/2023.emnlp-industry.62/","bibtex":"$23"},{"id":"openrt2023","title":"OpenRT: An Open-source Framework for Reasoning Over Tabular Data","authors":[{"name":"Yilun Zhao","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Boyu Mi","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zhenting Qi","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Linyong Nan","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Minghao Guo","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Arman Cohan","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Dragomir Radev","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2023,"month":"7","type":"conference","status":"published","tags":[],"keywords":"$7:props:children:0:props:publications:12:tags","researchArea":"machine-learning","journal":"","conference":"Annual Meeting of the Association for Computational Linguistics (ACL)","abstract":"OpenRT presents an open-source framework for reasoning over tabular data, providing tools and infrastructure for systematic evaluation and development of table reasoning models. The framework includes data processing utilities, model interfaces, evaluation metrics, and benchmarking capabilities. OpenRT enables researchers and practitioners to easily develop, evaluate, and compare different approaches to table reasoning, advancing the field through open collaboration and reproducible research.","description":"We developed and released an open-source framework for reasoning over tabular data.","selected":false,"url":"https://aclanthology.org/2023.acl-demo.32/","bibtex":"@inproceedings{openrt2023,\n  title = {OpenRT: An Open-source Framework for Reasoning Over Tabular Data},\n  author = {Yilun Zhao and Boyu Mi and Zhenting Qi and Linyong Nan and Minghao Guo and Arman Cohan and Dragomir Radev},\n  year = {2023},\n  month = {jul},\n  booktitle = {Annual Meeting of the Association for Computational Linguistics (ACL)},\n  note = {System Demonstration Track},\n  abstract = {OpenRT presents an open-source framework for reasoning over tabular data, providing tools and infrastructure for systematic evaluation and development of table reasoning models. The framework includes data processing utilities, model interfaces, evaluation metrics, and benchmarking capabilities. OpenRT enables researchers and practitioners to easily develop, evaluate, and compare different approaches to table reasoning, advancing the field through open collaboration and reproducible research.},\n  url = {https://aclanthology.org/2023.acl-demo.32/}\n}"},{"id":"robut2023","title":"RobuT: A Systematic Study of Table QA Robustness Against Human-Annotated Adversarial Perturbations","authors":[{"name":"Yilun Zhao","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Chen Zhao","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Linyong Nan","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zhenting Qi","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Wenlin Zhang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Xiangru Tang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Boyu Mi","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Dragomir Radev","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2023,"month":"7","type":"conference","status":"published","tags":[],"keywords":"$7:props:children:0:props:publications:13:tags","researchArea":"machine-learning","journal":"","conference":"Annual Meeting of the Association for Computational Linguistics (ACL)","abstract":"RobuT presents a systematic study of table question answering (QA) robustness against human-annotated adversarial perturbations. We investigate how various types of perturbations affect model performance and identify vulnerabilities in current table QA systems. The study includes comprehensive analysis of different perturbation strategies, evaluation of model robustness across multiple benchmarks, and insights into failure modes. Our findings reveal significant robustness challenges in table QA systems and provide guidance for developing more robust models.","description":"We conducted a systematic study of table QA robustness against human-annotated adversarial perturbations.","selected":false,"arxivId":"2306.14321","url":"https://arxiv.org/abs/2306.14321","pdfUrl":"https://arxiv.org/pdf/2306.14321.pdf","bibtex":"@inproceedings{robut2023,\n  title = {RobuT: A Systematic Study of Table QA Robustness Against Human-Annotated Adversarial Perturbations},\n  author = {Yilun Zhao and Chen Zhao and Linyong Nan and Zhenting Qi and Wenlin Zhang and Xiangru Tang and Boyu Mi and Dragomir Radev},\n  year = {2023},\n  month = {jul},\n  booktitle = {Annual Meeting of the Association for Computational Linguistics (ACL)},\n  abstract = {RobuT presents a systematic study of table question answering (QA) robustness against human-annotated adversarial perturbations. We investigate how various types of perturbations affect model performance and identify vulnerabilities in current table QA systems. The study includes comprehensive analysis of different perturbation strategies, evaluation of model robustness across multiple benchmarks, and insights into failure modes. Our findings reveal significant robustness challenges in table QA systems and provide guidance for developing more robust models.},\n  arxiv = {https://arxiv.org/abs/2306.14321}\n}"},{"id":"safer2023","title":"SaFER: A Robust and Efficient Framework for Fine-tuning BERT-based Classifier with Noisy Labels","authors":[{"name":"Zhenting Qi","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Xiaoyu Tan","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Chao Qu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yinghui Xu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yuan Qi","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2023,"month":"7","type":"conference","status":"published","tags":[],"keywords":"$7:props:children:0:props:publications:14:tags","researchArea":"machine-learning","journal":"","conference":"Annual Meeting of the Association for Computational Linguistics (ACL)","abstract":"SaFER introduces a robust and efficient framework for fine-tuning BERT-based classifiers when dealing with noisy labels. The framework addresses the challenge of label noise in real-world datasets by incorporating noise-robust training strategies and efficient fine-tuning techniques. SaFER demonstrates improved performance on noisy datasets while maintaining computational efficiency, making it suitable for practical applications where label quality cannot be guaranteed. The approach combines noise detection, label correction, and robust training methods to achieve reliable classification performance.","description":"We developed a robust framework for fine-tuning BERT-based classifiers in the presence of noisy labels.","selected":false,"url":"https://aclanthology.org/2023.acl-industry.38/","bibtex":"$24"},{"id":"loft2023","title":"LoFT: Enhancing Faithfulness and Diversity for Table-to-Text Generation via Logic Form Control","authors":[{"name":"Yilun Zhao","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zhenting Qi","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Linyong Nan","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Lorenzo Jaime Flores","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Dragomir Radev","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2023,"month":"2","type":"conference","status":"published","tags":[],"keywords":"$7:props:children:0:props:publications:15:tags","researchArea":"machine-learning","journal":"","conference":"Conference of the European Chapter of the Association for Computational Linguistics (EACL)","abstract":"LoFT introduces a novel approach to enhance faithfulness and diversity in table-to-text generation through logic form control. The method leverages logical representations to guide the generation process, ensuring that generated text accurately reflects the underlying table data while maintaining diversity. By incorporating logic form constraints, LoFT addresses common issues in table-to-text generation such as factual errors and repetitive outputs. The approach demonstrates improved faithfulness to source tables and increased diversity in generated text compared to baseline methods.","description":"We introduced logic form control mechanisms to guide table-to-text generation and ensure faithfulness to source data.","selected":false,"awards":["Oral Presentation"],"arxivId":"2302.02962","url":"https://arxiv.org/abs/2302.02962","pdfUrl":"https://arxiv.org/pdf/2302.02962.pdf","bibtex":"$25"},{"id":"reastap2022","title":"ReasTAP: Injecting Table Reasoning Skills During Pre-training via Synthetic Reasoning Examples","authors":[{"name":"Yilun Zhao","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Linyong Nan","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zhenting Qi","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Rui Zhang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Dragomir Radev","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2022,"month":"12","type":"conference","status":"published","tags":[],"keywords":"$7:props:children:0:props:publications:16:tags","researchArea":"machine-learning","journal":"","conference":"Conference on Empirical Methods in Natural Language Processing (EMNLP)","abstract":"ReasTAP introduces a method to inject table reasoning skills during pre-training by generating and incorporating synthetic reasoning examples. The approach creates diverse reasoning scenarios that teach models to understand and reason over tabular data during the pre-training phase. By exposing models to synthetic reasoning examples, ReasTAP improves their ability to handle table-related tasks without requiring extensive task-specific fine-tuning. The method demonstrates improved performance on downstream table reasoning tasks, showing the effectiveness of incorporating reasoning skills during pre-training.","description":"We developed methods to generate synthetic reasoning examples for table understanding tasks and integrated table reasoning skills into the pre-training phase.","selected":false,"arxivId":"2210.12374","url":"https://arxiv.org/abs/2210.12374","pdfUrl":"https://arxiv.org/pdf/2210.12374.pdf","bibtex":"$26"}]}],false,false]}]
c:null
10:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
b:null
14:{"metadata":[["$","title","0",{"children":"Publications | Zhenting Qi"}],["$","meta","1",{"name":"description","content":"A collection of my research work."}],["$","meta","2",{"name":"author","content":"Zhenting Qi"}],["$","meta","3",{"name":"keywords","content":"Zhenting Qi,PhD,Research,Harvard University"}],["$","meta","4",{"name":"creator","content":"Zhenting Qi"}],["$","meta","5",{"name":"publisher","content":"Zhenting Qi"}],["$","meta","6",{"property":"og:title","content":"Zhenting Qi"}],["$","meta","7",{"property":"og:description","content":"Computer Science Ph.D. student at Harvard University, researching intelligent and reliable AI systems"}],["$","meta","8",{"property":"og:site_name","content":"Zhenting Qi's Academic Website"}],["$","meta","9",{"property":"og:locale","content":"en_US"}],["$","meta","10",{"property":"og:type","content":"website"}],["$","meta","11",{"name":"twitter:card","content":"summary"}],["$","meta","12",{"name":"twitter:title","content":"Zhenting Qi"}],["$","meta","13",{"name":"twitter:description","content":"Computer Science Ph.D. student at Harvard University, researching intelligent and reliable AI systems"}],["$","link","14",{"rel":"icon","href":"/favicon.svg"}]],"error":null,"digest":"$undefined"}
e:{"metadata":"$14:metadata","error":null,"digest":"$undefined"}
