<!DOCTYPE html><html lang="en" class="scroll-smooth"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/_next/static/css/59336caf1270a3e3.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-679b75d1c4c2027c.js"/><script src="/_next/static/chunks/4bd1b696-70b6d399998de86a.js" async=""></script><script src="/_next/static/chunks/684-5aaa8290a129f299.js" async=""></script><script src="/_next/static/chunks/main-app-912ad050d553b0aa.js" async=""></script><script src="/_next/static/chunks/457-8b8d97145837e580.js" async=""></script><script src="/_next/static/chunks/874-6cc630662f3664af.js" async=""></script><script src="/_next/static/chunks/654-f769e5130ee40d3d.js" async=""></script><script src="/_next/static/chunks/app/layout-94e79e3770a5aead.js" async=""></script><script src="/_next/static/chunks/485-487001f78a0503db.js" async=""></script><script src="/_next/static/chunks/748-de23e927c4feb2c1.js" async=""></script><script src="/_next/static/chunks/app/%5Bslug%5D/page-45e6a89798ec03fc.js" async=""></script><link rel="icon" href="/favicon.svg" type="image/svg+xml"/><link rel="dns-prefetch" href="https://google-fonts.jialeliu.com"/><link rel="preconnect" href="https://google-fonts.jialeliu.com" crossorigin=""/><link rel="preload" as="style" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap"/><link rel="stylesheet" id="gfonts-css" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap" media="print"/><noscript><link rel="stylesheet" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap"/></noscript><script>
              // Always use light theme
              var root = document.documentElement;
              root.classList.remove('dark');
              root.classList.add('light');
              root.setAttribute('data-theme', 'light');
            </script><title>Publications | Zhenting Qi</title><meta name="description" content="A collection of my research work."/><meta name="author" content="Zhenting Qi"/><meta name="keywords" content="Zhenting Qi,PhD,Research,Harvard University"/><meta name="creator" content="Zhenting Qi"/><meta name="publisher" content="Zhenting Qi"/><meta property="og:title" content="Zhenting Qi"/><meta property="og:description" content="Computer Science Ph.D. student at Harvard University, researching intelligent and reliable AI systems"/><meta property="og:site_name" content="Zhenting Qi&#x27;s Academic Website"/><meta property="og:locale" content="en_US"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Zhenting Qi"/><meta name="twitter:description" content="Computer Science Ph.D. student at Harvard University, researching intelligent and reliable AI systems"/><link rel="icon" href="/favicon.svg"/><script>document.querySelectorAll('body link[rel="icon"], body link[rel="apple-touch-icon"]').forEach(el => document.head.appendChild(el))</script><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="font-sans antialiased"><div style="visibility:hidden"><nav class="fixed top-0 left-0 right-0 z-50" data-headlessui-state=""><div class="transition-all duration-300 ease-out bg-transparent" style="transform:translateY(-100px)"><div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8"><div class="flex justify-between items-center h-16 lg:h-20"><div class="flex-shrink-0" tabindex="0"><a class="flex items-center space-x-3 text-xl lg:text-2xl font-serif font-semibold text-primary hover:text-accent transition-colors duration-200" href="/"><img alt="Harvard University" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" class="object-contain" style="color:transparent" src="/harvard.jpg"/><span class="text-neutral-300 dark:text-neutral-600">|</span><span>Zhenting Qi</span></a></div><div class="hidden lg:block"><div class="ml-10 flex items-center space-x-8"><div class="flex items-baseline space-x-8"><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/"><span class="relative z-10">About</span></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-primary" href="/publications/"><span class="relative z-10">Publications</span><div class="absolute inset-0 bg-accent/10 rounded-lg"></div></a></div></div></div><div class="lg:hidden flex items-center space-x-2"><button class="inline-flex items-center justify-center p-2 rounded-md text-neutral-600 hover:text-primary hover:bg-neutral-100 dark:hover:bg-neutral-800 focus:outline-none focus:ring-2 focus:ring-inset focus:ring-accent transition-colors duration-200" id="headlessui-disclosure-button-¬´R75lb¬ª" type="button" aria-expanded="false" data-headlessui-state=""><span class="sr-only">Open main menu</span><div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="block h-6 w-6"><path stroke-linecap="round" stroke-linejoin="round" d="M3.75 6.75h16.5M3.75 12h16.5m-16.5 5.25h16.5"></path></svg></div></button></div></div></div></div></nav><main class="min-h-screen pt-16 lg:pt-20"><div class="max-w-4xl mx-auto px-4 sm:px-6 lg:px-8 py-12"><div style="opacity:0;transform:translateY(20px)"><div class="mb-8"><h1 class="text-4xl font-serif font-bold text-primary mb-4">Publications</h1><p class="text-lg text-neutral-600 dark:text-neutral-500 max-w-2xl">A collection of my research work.</p></div><div class="mb-8 space-y-4"><div class="flex flex-col sm:flex-row gap-4"><div class="relative flex-grow"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="absolute left-3 top-1/2 transform -translate-y-1/2 h-5 w-5 text-neutral-400"><path stroke-linecap="round" stroke-linejoin="round" d="m21 21-5.197-5.197m0 0A7.5 7.5 0 1 0 5.196 5.196a7.5 7.5 0 0 0 10.607 10.607Z"></path></svg><input type="text" placeholder="Search publications..." class="w-full pl-10 pr-4 py-2 rounded-lg border border-neutral-200 dark:border-neutral-800 bg-white dark:bg-neutral-900 focus:ring-2 focus:ring-accent focus:border-transparent transition-all duration-200" value=""/></div><button class="flex items-center justify-center px-4 py-2 rounded-lg border transition-all duration-200 bg-white dark:bg-neutral-900 border-neutral-200 dark:border-neutral-800 text-neutral-600 hover:border-accent hover:text-accent"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-5 w-5 mr-2"><path stroke-linecap="round" stroke-linejoin="round" d="M12 3c2.755 0 5.455.232 8.083.678.533.09.917.556.917 1.096v1.044a2.25 2.25 0 0 1-.659 1.591l-5.432 5.432a2.25 2.25 0 0 0-.659 1.591v2.927a2.25 2.25 0 0 1-1.244 2.013L9.75 21v-6.568a2.25 2.25 0 0 0-.659-1.591L3.659 7.409A2.25 2.25 0 0 1 3 5.818V4.774c0-.54.384-1.006.917-1.096A48.32 48.32 0 0 1 12 3Z"></path></svg>Filters</button></div></div><div class="space-y-6"><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><div class="flex items-start justify-between gap-2 mb-2"><h3 class="text-xl font-semibold text-primary leading-tight flex-1">EvoLM: In Search of Lost Language Model Training Dynamics</h3><div class="flex flex-wrap gap-1 flex-shrink-0"><span class="inline-block px-2 py-1 rounded border-2 border-accent bg-accent/10 text-accent text-xs font-semibold whitespace-nowrap">üèÜ <!-- -->Oral Presentation</span></div></div><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Zhenting Qi</span>, </span><span><span class="">Fan Nie</span>, </span><span><span class="">Alexandre Alahi</span>, </span><span><span class="">James Zou</span>, </span><span><span class="">Himabindu Lakkaraju</span>, </span><span><span class="">Yilun Du</span>, </span><span><span class="">Eric Xing</span>, </span><span><span class="">Sham Kakade</span>, </span><span><span class="">Hanlin Zhang</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">Advances in Neural Information Processing Systems (NeurIPS)<!-- --> <!-- -->2025</p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">We developed a comprehensive model suite for analyzing language model training dynamics across pre-training, continued pre-training, supervised fine-tuning, and reinforcement learning stages.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://arxiv.org/abs/2506.16029" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">arXiv</a><a href="https://arxiv.org/pdf/2506.16029.pdf" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">PDF</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><div class="flex items-start justify-between gap-2 mb-2"><h3 class="text-xl font-semibold text-primary leading-tight flex-1">Measuring the Faithfulness of Thinking Drafts in Large Reasoning Models</h3></div><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Zidi Xiong</span>, </span><span><span class="">Shan Chen</span>, </span><span><span class="">Zhenting Qi</span>, </span><span><span class="">Himabindu Lakkaraju</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">Advances in Neural Information Processing Systems (NeurIPS)<!-- --> <!-- -->2025</p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">We introduced a systematic framework to evaluate the faithfulness of thinking drafts in Large Reasoning Models using counterfactual interventions.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://arxiv.org/abs/2505.13774" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">arXiv</a><a href="https://arxiv.org/pdf/2505.13774.pdf" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">PDF</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><div class="flex items-start justify-between gap-2 mb-2"><h3 class="text-xl font-semibold text-primary leading-tight flex-1">Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search</h3></div><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Maohao Shen</span><sup class="ml-0 text-neutral-600 dark:text-neutral-400">‚Ä†</sup>, </span><span><span class="">Guangtao Zeng</span><sup class="ml-0 text-neutral-600 dark:text-neutral-400">‚Ä†</sup>, </span><span><span class="">Zhenting Qi</span><sup class="ml-0 text-neutral-600 dark:text-neutral-400">‚Ä†</sup>, </span><span><span class="">Zhang-Wei Hong</span>, </span><span><span class="">Zhenfang Chen</span>, </span><span><span class="">Wei Lu</span>, </span><span><span class="">Gregory Wornell</span>, </span><span><span class="">Subhro Das</span>, </span><span><span class="">David Cox</span>, </span><span><span class="">Chuang Gan</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">International Conference on Machine Learning (ICML)<!-- --> <!-- -->2025</p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">We introduced the COAT reasoning framework to enhance LLM reasoning via autoregressive search with self-reflection and self-exploration.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://arxiv.org/abs/2502.02508" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">arXiv</a><a href="https://arxiv.org/pdf/2502.02508.pdf" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">PDF</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><div class="flex items-start justify-between gap-2 mb-2"><h3 class="text-xl font-semibold text-primary leading-tight flex-1">rStar: Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solvers</h3></div><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Zhenting Qi</span><sup class="ml-0 text-neutral-600 dark:text-neutral-400">‚Ä†</sup>, </span><span><span class="">Mingyuan Ma</span><sup class="ml-0 text-neutral-600 dark:text-neutral-400">‚Ä†</sup>, </span><span><span class="">Jiahang Xu</span><sup class="ml-0 text-neutral-600 dark:text-neutral-400">‚Ä†</sup>, </span><span><span class="">Li Lyna Zhang</span>, </span><span><span class="">Fan Yang</span>, </span><span><span class="">Mao Yang</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">International Conference on Learning Representations (ICLR)<!-- --> <!-- -->2025</p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">We introduced rStar, a self-play mutual reasoning approach that enhances reasoning capabilities of small language models without fine-tuning or superior models.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://arxiv.org/abs/2408.06195" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">arXiv</a><a href="https://arxiv.org/pdf/2408.06195.pdf" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">PDF</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><div class="flex items-start justify-between gap-2 mb-2"><h3 class="text-xl font-semibold text-primary leading-tight flex-1">Follow My Instruction and Spill the Beans: Scalable Data Extraction from Retrieval-Augmented Generation Systems</h3></div><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Zhenting Qi</span>, </span><span><span class="">Hanlin Zhang</span>, </span><span><span class="">Eric Xing</span>, </span><span><span class="">Sham Kakade</span>, </span><span><span class="">Himabindu Lakkaraju</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">International Conference on Learning Representations (ICLR)<!-- --> <!-- -->2025</p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">We developed a scalable method for extracting data from RAG systems using LLMs&#x27; instruction-following capabilities.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://arxiv.org/abs/2402.17840" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">arXiv</a><a href="https://arxiv.org/pdf/2402.17840.pdf" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">PDF</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><div class="flex items-start justify-between gap-2 mb-2"><h3 class="text-xl font-semibold text-primary leading-tight flex-1">Quantifying Generalization Complexity for Large Language Models</h3></div><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Zhenting Qi</span>, </span><span><span class="">Hongyin Luo</span>, </span><span><span class="">Xuliang Huang</span>, </span><span><span class="">Zhuokai Zhao</span>, </span><span><span class="">Yibo Jiang</span>, </span><span><span class="">Xiangjun Fan</span>, </span><span><span class="">Himabindu Lakkaraju</span>, </span><span><span class="">James Glass</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">International Conference on Learning Representations (ICLR)<!-- --> <!-- -->2025</p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">We introduced Scylla, a dynamic evaluation framework that quantitatively measures LLMs&#x27; generalization abilities by disentangling generalization from memorization.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://arxiv.org/abs/2410.01769" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">arXiv</a><a href="https://arxiv.org/pdf/2410.01769.pdf" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">PDF</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><div class="flex items-start justify-between gap-2 mb-2"><h3 class="text-xl font-semibold text-primary leading-tight flex-1">FOLIO: Natural Language Reasoning with First-Order Logic</h3></div><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Simeng Han</span>, </span><span><span class="">Hailey Schoelkopf</span>, </span><span><span class="">Yilun Zhao</span>, </span><span><span class="">Zhenting Qi</span>, </span><span><span class="">Martin Riddell</span>, </span><span><span class="">Wenfei Zhou</span>, </span><span><span class="">James Coady</span>, </span><span><span class="">David Peng</span>, </span><span><span class="">Yujie Qiao</span>, </span><span><span class="">Luke Benson</span>, </span><span><span class="">Lucy Sun</span>, </span><span><span class="">Alex Wardle-Solano</span>, </span><span><span class="">Hannah Szabo</span>, </span><span><span class="">Ekaterina Zubova</span>, </span><span><span class="">Matthew Burtell</span>, </span><span><span class="">Jonathan Fan</span>, </span><span><span class="">Yixin Liu</span>, </span><span><span class="">Brian Wong</span>, </span><span><span class="">Malcolm Sailor</span>, </span><span><span class="">Ansong Ni</span>, </span><span><span class="">Linyong Nan</span>, </span><span><span class="">Jungo Kasai</span>, </span><span><span class="">Tao Yu</span>, </span><span><span class="">Rui Zhang</span>, </span><span><span class="">Alexander R. Fabbri</span>, </span><span><span class="">Wojciech Kryscinski</span>, </span><span><span class="">Semih Yavuz</span>, </span><span><span class="">Ye Liu</span>, </span><span><span class="">Xi Victoria Lin</span>, </span><span><span class="">Shafiq Joty</span>, </span><span><span class="">Yingbo Zhou</span>, </span><span><span class="">Caiming Xiong</span>, </span><span><span class="">Rex Ying</span>, </span><span><span class="">Arman Cohan</span>, </span><span><span class="">Dragomir Radev</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">Conference on Empirical Methods in Natural Language Processing (EMNLP)<!-- --> <!-- -->2024</p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">We developed a comprehensive dataset and benchmark for natural language reasoning using First-Order Logic.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://arxiv.org/abs/2209.00840" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">arXiv</a><a href="https://arxiv.org/pdf/2209.00840.pdf" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">PDF</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><div class="flex items-start justify-between gap-2 mb-2"><h3 class="text-xl font-semibold text-primary leading-tight flex-1">P-FOLIO: Evaluating and Improving Logical Reasoning with Abundant Human-Written Reasoning Chains</h3></div><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Simeng Han</span>, </span><span><span class="">Aaron Yu</span>, </span><span><span class="">Rui Shen</span>, </span><span><span class="">Zhenting Qi</span>, </span><span><span class="">Martin Riddell</span>, </span><span><span class="">Wenfei Zhou</span>, </span><span><span class="">Yujie Qiao</span>, </span><span><span class="">Yilun Zhao</span>, </span><span><span class="">Semih Yavuz</span>, </span><span><span class="">Ye Liu</span>, </span><span><span class="">Shafiq Joty</span>, </span><span><span class="">Yingbo Zhou</span>, </span><span><span class="">Caiming Xiong</span>, </span><span><span class="">Dragomir Radev</span>, </span><span><span class="">Rex Ying</span>, </span><span><span class="">Arman Cohan</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">Conference on Empirical Methods in Natural Language Processing (EMNLP)<!-- --> <!-- -->2024</p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">We extended FOLIO with abundant human-written reasoning chains, providing detailed reasoning processes for logical reasoning tasks.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://arxiv.org/abs/2410.09207" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">arXiv</a><a href="https://arxiv.org/pdf/2410.09207.pdf" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">PDF</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><div class="flex items-start justify-between gap-2 mb-2"><h3 class="text-xl font-semibold text-primary leading-tight flex-1">Constrained Human-AI Cooperation: An Inclusive Embodied Social Intelligence Challenge</h3></div><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Weihua Du</span>, </span><span><span class="">Qiushi Lyu</span>, </span><span><span class="">Jiaming Shan</span>, </span><span><span class="">Zhenting Qi</span>, </span><span><span class="">Hongxin Zhang</span>, </span><span><span class="">Sunli Chen</span>, </span><span><span class="">Andi Peng</span>, </span><span><span class="">Tianmin Shu</span>, </span><span><span class="">Kwonjoon Lee</span>, </span><span><span class="">Behzad Dariush</span>, </span><span><span class="">Chuang Gan</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">Advances in Neural Information Processing Systems (NeurIPS)<!-- --> <!-- -->2024</p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">We introduced a comprehensive benchmark challenge for advancing research in embodied social intelligence through constrained human-AI cooperation scenarios.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://arxiv.org/abs/2411.01796" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">arXiv</a><a href="https://arxiv.org/pdf/2411.01796.pdf" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">PDF</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><div class="flex items-start justify-between gap-2 mb-2"><h3 class="text-xl font-semibold text-primary leading-tight flex-1">PILLOW: Enhancing Efficient Instruction Fine-tuning via Prompt Matching</h3><div class="flex flex-wrap gap-1 flex-shrink-0"><span class="inline-block px-2 py-1 rounded border-2 border-accent bg-accent/10 text-accent text-xs font-semibold whitespace-nowrap">üèÜ <!-- -->Oral Presentation</span></div></div><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Zhenting Qi</span>, </span><span><span class="">Xiaoyu Tan</span>, </span><span><span class="">Shaojie Shi</span>, </span><span><span class="">Chao Qu</span>, </span><span><span class="">Yinghui Xu</span>, </span><span><span class="">Yuan Qi</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">Conference on Empirical Methods in Natural Language Processing (EMNLP)<!-- --> <!-- -->2023</p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">We introduced a prompt matching framework to enhance the efficiency of instruction fine-tuning.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://arxiv.org/abs/2312.05621" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">arXiv</a><a href="https://arxiv.org/pdf/2312.05621.pdf" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">PDF</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><div class="flex items-start justify-between gap-2 mb-2"><h3 class="text-xl font-semibold text-primary leading-tight flex-1">QTSumm: A New Benchmark for Query-Focused Table Summarization</h3></div><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Yilun Zhao</span>, </span><span><span class="">Zhenting Qi</span>, </span><span><span class="">Linyong Nan</span>, </span><span><span class="">Boyu Mi</span>, </span><span><span class="">Yixin Liu</span>, </span><span><span class="">Weijin Zou</span>, </span><span><span class="">Simeng Han</span>, </span><span><span class="">Xiangru Tang</span>, </span><span><span class="">Yumo Xu</span>, </span><span><span class="">Arman Cohan</span>, </span><span><span class="">Dragomir Radev</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">Conference on Empirical Methods in Natural Language Processing (EMNLP)<!-- --> <!-- -->2023</p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">We introduced a comprehensive benchmark dataset for query-focused table summarization.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://arxiv.org/abs/2305.14303" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">arXiv</a><a href="https://arxiv.org/pdf/2305.14303.pdf" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">PDF</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><div class="flex items-start justify-between gap-2 mb-2"><h3 class="text-xl font-semibold text-primary leading-tight flex-1">Self-Criticism: Aligning Large Language Models with their Understanding of Helpfulness, Honesty, and Harmlessness</h3><div class="flex flex-wrap gap-1 flex-shrink-0"><span class="inline-block px-2 py-1 rounded border-2 border-accent bg-accent/10 text-accent text-xs font-semibold whitespace-nowrap">üèÜ <!-- -->Oral Presentation</span></div></div><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Xiaoyu Tan</span>, </span><span><span class="">Shaojie Shi</span>, </span><span><span class="">Xihe Qiu</span>, </span><span><span class="">Chao Qu</span>, </span><span><span class="">Zhenting Qi</span>, </span><span><span class="">Yinghui Xu</span>, </span><span><span class="">Yuan Qi</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">Conference on Empirical Methods in Natural Language Processing (EMNLP)<!-- --> <!-- -->2023</p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">We introduced a self-criticism framework that enables models to evaluate and improve their outputs based on their understanding of helpfulness, honesty, and harmlessness.</p><div class="flex flex-wrap gap-2 mt-auto"><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><div class="flex items-start justify-between gap-2 mb-2"><h3 class="text-xl font-semibold text-primary leading-tight flex-1">OpenRT: An Open-source Framework for Reasoning Over Tabular Data</h3></div><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Yilun Zhao</span>, </span><span><span class="">Boyu Mi</span>, </span><span><span class="">Zhenting Qi</span>, </span><span><span class="">Linyong Nan</span>, </span><span><span class="">Minghao Guo</span>, </span><span><span class="">Arman Cohan</span>, </span><span><span class="">Dragomir Radev</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">Annual Meeting of the Association for Computational Linguistics (ACL)<!-- --> <!-- -->2023</p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">We developed and released an open-source framework for reasoning over tabular data.</p><div class="flex flex-wrap gap-2 mt-auto"><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><div class="flex items-start justify-between gap-2 mb-2"><h3 class="text-xl font-semibold text-primary leading-tight flex-1">RobuT: A Systematic Study of Table QA Robustness Against Human-Annotated Adversarial Perturbations</h3></div><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Yilun Zhao</span>, </span><span><span class="">Chen Zhao</span>, </span><span><span class="">Linyong Nan</span>, </span><span><span class="">Zhenting Qi</span>, </span><span><span class="">Wenlin Zhang</span>, </span><span><span class="">Xiangru Tang</span>, </span><span><span class="">Boyu Mi</span>, </span><span><span class="">Dragomir Radev</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">Annual Meeting of the Association for Computational Linguistics (ACL)<!-- --> <!-- -->2023</p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">We conducted a systematic study of table QA robustness against human-annotated adversarial perturbations.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://arxiv.org/abs/2306.14321" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">arXiv</a><a href="https://arxiv.org/pdf/2306.14321.pdf" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">PDF</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><div class="flex items-start justify-between gap-2 mb-2"><h3 class="text-xl font-semibold text-primary leading-tight flex-1">SaFER: A Robust and Efficient Framework for Fine-tuning BERT-based Classifier with Noisy Labels</h3></div><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Zhenting Qi</span>, </span><span><span class="">Xiaoyu Tan</span>, </span><span><span class="">Chao Qu</span>, </span><span><span class="">Yinghui Xu</span>, </span><span><span class="">Yuan Qi</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">Annual Meeting of the Association for Computational Linguistics (ACL)<!-- --> <!-- -->2023</p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">We developed a robust framework for fine-tuning BERT-based classifiers in the presence of noisy labels.</p><div class="flex flex-wrap gap-2 mt-auto"><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><div class="flex items-start justify-between gap-2 mb-2"><h3 class="text-xl font-semibold text-primary leading-tight flex-1">LoFT: Enhancing Faithfulness and Diversity for Table-to-Text Generation via Logic Form Control</h3><div class="flex flex-wrap gap-1 flex-shrink-0"><span class="inline-block px-2 py-1 rounded border-2 border-accent bg-accent/10 text-accent text-xs font-semibold whitespace-nowrap">üèÜ <!-- -->Oral Presentation</span></div></div><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Yilun Zhao</span>, </span><span><span class="">Zhenting Qi</span>, </span><span><span class="">Linyong Nan</span>, </span><span><span class="">Lorenzo Jaime Flores</span>, </span><span><span class="">Dragomir Radev</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">Conference of the European Chapter of the Association for Computational Linguistics (EACL)<!-- --> <!-- -->2023</p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">We introduced logic form control mechanisms to guide table-to-text generation and ensure faithfulness to source data.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://arxiv.org/abs/2302.02962" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">arXiv</a><a href="https://arxiv.org/pdf/2302.02962.pdf" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">PDF</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><div class="flex items-start justify-between gap-2 mb-2"><h3 class="text-xl font-semibold text-primary leading-tight flex-1">ReasTAP: Injecting Table Reasoning Skills During Pre-training via Synthetic Reasoning Examples</h3></div><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Yilun Zhao</span>, </span><span><span class="">Linyong Nan</span>, </span><span><span class="">Zhenting Qi</span>, </span><span><span class="">Rui Zhang</span>, </span><span><span class="">Dragomir Radev</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">Conference on Empirical Methods in Natural Language Processing (EMNLP)<!-- --> <!-- -->2022</p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">We developed methods to generate synthetic reasoning examples for table understanding tasks and integrated table reasoning skills into the pre-training phase.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://arxiv.org/abs/2210.12374" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">arXiv</a><a href="https://arxiv.org/pdf/2210.12374.pdf" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">PDF</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div></div></div></div><!--$--><!--/$--><!--$--><!--/$--></main><footer class="border-t border-neutral-200/50 bg-neutral-50/50 dark:bg-neutral-900/50 dark:border-neutral-700/50"><div class="max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-6"><div class="flex flex-col sm:flex-row justify-between items-center gap-2"><p class="text-xs text-neutral-500">Last updated: <!-- -->December 1, 2025</p><p class="text-xs text-neutral-500 flex items-center"><a href="https://github.com/xyjoey/PRISM" target="_blank" rel="noopener noreferrer">Built with PRISM</a><span class="ml-2">üöÄ</span></p></div></div></footer></div><script src="/_next/static/chunks/webpack-679b75d1c4c2027c.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[9147,[\"457\",\"static/chunks/457-8b8d97145837e580.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"654\",\"static/chunks/654-f769e5130ee40d3d.js\",\"177\",\"static/chunks/app/layout-94e79e3770a5aead.js\"],\"default\"]\n3:I[5783,[\"457\",\"static/chunks/457-8b8d97145837e580.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"654\",\"static/chunks/654-f769e5130ee40d3d.js\",\"177\",\"static/chunks/app/layout-94e79e3770a5aead.js\"],\"ThemeProvider\"]\n4:I[9891,[\"457\",\"static/chunks/457-8b8d97145837e580.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"654\",\"static/chunks/654-f769e5130ee40d3d.js\",\"177\",\"static/chunks/app/layout-94e79e3770a5aead.js\"],\"default\"]\n5:I[7555,[],\"\"]\n6:I[1295,[],\"\"]\n7:I[2548,[\"457\",\"static/chunks/457-8b8d97145837e580.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"654\",\"static/chunks/654-f769e5130ee40d3d.js\",\"177\",\"static/chunks/app/layout-94e79e3770a5aead.js\"],\"default\"]\n9:I[9665,[],\"MetadataBoundary\"]\nb:I[9665,[],\"OutletBoundary\"]\ne:I[4911,[],\"AsyncMetadataOutlet\"]\n10:I[9665,[],\"ViewportBoundary\"]\n12:I[6614,[],\"\"]\n:HL[\"/_next/static/css/59336caf1270a3e3.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"AzNp4ZfIUpYGBZyWQsN2l\",\"p\":\"\",\"c\":[\"\",\"publications\",\"\"],\"i\":false,\"f\":[[[\"\",{\"children\":[[\"slug\",\"publications\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/59336caf1270a3e3.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"scroll-smooth\",\"suppressHydrationWarning\":true,\"children\":[[\"$\",\"head\",null,{\"children\":[[\"$\",\"link\",null,{\"rel\":\"icon\",\"href\":\"/favicon.svg\",\"type\":\"image/svg+xml\"}],[\"$\",\"link\",null,{\"rel\":\"dns-prefetch\",\"href\":\"https://google-fonts.jialeliu.com\"}],[\"$\",\"link\",null,{\"rel\":\"preconnect\",\"href\":\"https://google-fonts.jialeliu.com\",\"crossOrigin\":\"\"}],[\"$\",\"link\",null,{\"rel\":\"preload\",\"as\":\"style\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\"}],[\"$\",\"link\",null,{\"rel\":\"stylesheet\",\"id\":\"gfonts-css\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\",\"media\":\"print\"}],[\"$\",\"noscript\",null,{\"children\":[\"$\",\"link\",null,{\"rel\":\"stylesheet\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\"}]}],[\"$\",\"script\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"\\n              // Always use light theme\\n              var root = document.documentElement;\\n              root.classList.remove('dark');\\n              root.classList.add('light');\\n              root.setAttribute('data-theme', 'light');\\n            \"}}]]}],[\"$\",\"body\",null,{\"className\":\"font-sans antialiased\",\"children\":[[\"$\",\"$L2\",null,{}],[\"$\",\"$L3\",null,{\"children\":[[\"$\",\"$L4\",null,{\"items\":[{\"title\":\"About\",\"type\":\"page\",\"target\":\"about\",\"href\":\"/\"},{\"title\":\"Publications\",\"type\":\"page\",\"target\":\"publications\",\"href\":\"/publications\"}],\"siteTitle\":\"Zhenting Qi\",\"enableOnePageMode\":false}],[\"$\",\"main\",null,{\"className\":\"min-h-screen pt-16 lg:pt-20\",\"children\":[\"$\",\"$L5\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L6\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}],[\"$\",\"$L7\",null,{\"lastUpdated\":\"December 1, 2025\"}]]}]]}]]}]]}],{\"children\":[[\"slug\",\"publications\",\"d\"],[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L5\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L6\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[\"$L8\",[\"$\",\"$L9\",null,{\"children\":\"$La\"}],null,[\"$\",\"$Lb\",null,{\"children\":[\"$Lc\",\"$Ld\",[\"$\",\"$Le\",null,{\"promise\":\"$@f\"}]]}]]}],{},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$1\",\"uaC4FmEkUIZuz0ZwWQAGR\",{\"children\":[[\"$\",\"$L10\",null,{\"children\":\"$L11\"}],null]}],null]}],false]],\"m\":\"$undefined\",\"G\":[\"$12\",\"$undefined\"],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"13:\"$Sreact.suspense\"\n14:I[4911,[],\"AsyncMetadata\"]\n16:I[6669,[\"457\",\"static/chunks/457-8b8d97145837e580.js\",\"485\",\"static/chunks/485-487001f78a0503db.js\",\"748\",\"static/chunks/748-de23e927c4feb2c1.js\",\"182\",\"static/chunks/app/%5Bslug%5D/page-45e6a89798ec03fc.js\"],\"default\"]\na:[\"$\",\"$13\",null,{\"fallback\":null,\"children\":[\"$\",\"$L14\",null,{\"promise\":\"$@15\"}]}]\n17:T4a8,Modern language model (LM) training has been divided into multiple stages, making it difficult for downstream developers to evaluate the impact of design choices made at each stage. We present EvoLM, a model suite that enables systematic and transparent analysis of LMs' training dynamics across pre-training, continued pre-training, supervised fine-tuning, and reinforcement learning. We train over 100 LMs with 1B and 4B parameters from scratch, and evaluate both upstream (language modeling) and downstream (problem-solving) capabilities, including considerations of both in-domain and out-of-domain generalization. Key insights highlight the diminishing returns from excessive pre-training and post-training, the importance and practices of mitigating forgetting during domain-specific continued pre-training, the crucial role of continued pre-training in bridging pre-training and post-training phases, and various intricate trade-offs when configuring supervised fine-tuning and reinforcement learning. To facilitate open research and reproducibility, we release all pre-trained and post-trained models, training datasets for all stages, and our entire training and evaluation pipeline.18:T664,@inproceedings{evolm2025,\n  title = {EvoLM: In Search of Lost Language Model Training Dynamics},\n  author = {Zhenting Qi and Fan Nie and Alexandre Alahi and James Zou and Himabindu Lakkaraju and Yilun Du and Eric Xing and Sham Kakade and Hanlin Zhang},\n  year = {2025},\n  month = {dec},\n  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},\n  note = {Oral},\n  abstract = {Modern language model (LM) training has been divided into multiple stages, making it "])</script><script>self.__next_f.push([1,"difficult for downstream developers to evaluate the impact of design choices made at each stage. We present EvoLM, a model suite that enables systematic and transparent analysis of LMs' training dynamics across pre-training, continued pre-training, supervised fine-tuning, and reinforcement learning. We train over 100 LMs with 1B and 4B parameters from scratch, and evaluate both upstream (language modeling) and downstream (problem-solving) capabilities, including considerations of both in-domain and out-of-domain generalization. Key insights highlight the diminishing returns from excessive pre-training and post-training, the importance and practices of mitigating forgetting during domain-specific continued pre-training, the crucial role of continued pre-training in bridging pre-training and post-training phases, and various intricate trade-offs when configuring supervised fine-tuning and reinforcement learning. To facilitate open research and reproducibility, we release all pre-trained and post-trained models, training datasets for all stages, and our entire training and evaluation pipeline.},\n  arxiv = {https://arxiv.org/abs/2506.16029}\n}19:T4e9,Large Reasoning Models (LRMs) have significantly enhanced their capabilities in complex problem-solving by introducing a thinking draft that enables multi-path Chain-of-Thought explorations before producing final answers. Ensuring the faithfulness of these intermediate reasoning processes is crucial for reliable monitoring, interpretation, and effective control. In this paper, we propose a systematic counterfactual intervention framework to rigorously evaluate thinking draft faithfulness. Our approach focuses on two complementary dimensions: (1) Intra-Draft Faithfulness, which assesses whether individual reasoning steps causally influence subsequent steps and the final draft conclusion through counterfactual step insertions; and (2) Draft-to-Answer Faithfulness, which evaluates whether final answers are logically consistent with and dependent on the thinking draft, by per"])</script><script>self.__next_f.push([1,"turbing the draft's concluding logic. We conduct extensive experiments across six state-of-the-art LRMs. Our findings show that current LRMs demonstrate selective faithfulness to intermediate reasoning steps and frequently fail to faithfully align with the draft conclusions. These results underscore the need for more faithful and interpretable reasoning in advanced LRMs.1a:T668,@inproceedings{faithful-thinking-drafts2025,\n  title = {Measuring the Faithfulness of Thinking Drafts in Large Reasoning Models},\n  author = {Zidi Xiong and Shan Chen and Zhenting Qi and Himabindu Lakkaraju},\n  year = {2025},\n  month = {dec},\n  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},\n  abstract = {Large Reasoning Models (LRMs) have significantly enhanced their capabilities in complex problem-solving by introducing a thinking draft that enables multi-path Chain-of-Thought explorations before producing final answers. Ensuring the faithfulness of these intermediate reasoning processes is crucial for reliable monitoring, interpretation, and effective control. In this paper, we propose a systematic counterfactual intervention framework to rigorously evaluate thinking draft faithfulness. Our approach focuses on two complementary dimensions: (1) Intra-Draft Faithfulness, which assesses whether individual reasoning steps causally influence subsequent steps and the final draft conclusion through counterfactual step insertions; and (2) Draft-to-Answer Faithfulness, which evaluates whether final answers are logically consistent with and dependent on the thinking draft, by perturbing the draft's concluding logic. We conduct extensive experiments across six state-of-the-art LRMs. Our findings show that current LRMs demonstrate selective faithfulness to intermediate reasoning steps and frequently fail to faithfully align with the draft conclusions. These results underscore the need for more faithful and interpretable reasoning in advanced LRMs.},\n  arxiv = {https://arxiv.org/abs/2505.13774}\n}1b:T57b,Large language mode"])</script><script>self.__next_f.push([1,"ls (LLMs) have demonstrated remarkable reasoning capabilities across diverse domains. Recent studies have shown that increasing test-time computation enhances LLMs' reasoning capabilities. This typically involves extensive sampling at inference time guided by an external LLM verifier, resulting in a two-player system. Despite external guidance, the effectiveness of this system demonstrates the potential of a single LLM to tackle complex tasks. Thus, we pose a new research problem: Can we internalize the searching capabilities to fundamentally enhance the reasoning abilities of a single LLM? This work explores an orthogonal direction focusing on post-training LLMs for autoregressive searching (i.e., an extended reasoning process with self-reflection and self-exploration of new strategies). To achieve this, we propose the Chain-of-Action-Thought (COAT) reasoning and a two-stage training paradigm: 1) a small-scale format tuning stage to internalize the COAT reasoning format and 2) a large-scale self-improvement stage leveraging reinforcement learning. Our approach results in Satori, a 7B LLM trained on open-source models and data. Extensive empirical evaluations demonstrate that Satori achieves state-of-the-art performance on mathematical reasoning benchmarks while exhibits strong generalization to out-of-domain tasks. Code, data, and models are fully open-sourced.1c:T762,@inproceedings{satori2025,\n  title = {Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search},\n  author = {Maohao Shen and Guangtao Zeng and Zhenting Qi and Zhang-Wei Hong and Zhenfang Chen and Wei Lu and Gregory Wornell and Subhro Das and David Cox and Chuang Gan},\n  year = {2025},\n  month = {jul},\n  booktitle = {International Conference on Machine Learning (ICML)},\n  abstract = {Large language models (LLMs) have demonstrated remarkable reasoning capabilities across diverse domains. Recent studies have shown that increasing test-time computation enhances LLMs' reasoning capabilities. This typic"])</script><script>self.__next_f.push([1,"ally involves extensive sampling at inference time guided by an external LLM verifier, resulting in a two-player system. Despite external guidance, the effectiveness of this system demonstrates the potential of a single LLM to tackle complex tasks. Thus, we pose a new research problem: Can we internalize the searching capabilities to fundamentally enhance the reasoning abilities of a single LLM? This work explores an orthogonal direction focusing on post-training LLMs for autoregressive searching (i.e., an extended reasoning process with self-reflection and self-exploration of new strategies). To achieve this, we propose the Chain-of-Action-Thought (COAT) reasoning and a two-stage training paradigm: 1) a small-scale format tuning stage to internalize the COAT reasoning format and 2) a large-scale self-improvement stage leveraging reinforcement learning. Our approach results in Satori, a 7B LLM trained on open-source models and data. Extensive empirical evaluations demonstrate that Satori achieves state-of-the-art performance on mathematical reasoning benchmarks while exhibits strong generalization to out-of-domain tasks. Code, data, and models are fully open-sourced.},\n  arxiv = {https://arxiv.org/abs/2502.02508}\n}1d:T589,@inproceedings{mutual-reasoning2025,\n  title = {rStar: Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solvers},\n  author = {Zhenting Qi and Mingyuan Ma and Jiahang Xu and Li Lyna Zhang and Fan Yang and Mao Yang},\n  year = {2025},\n  month = {may},\n  booktitle = {International Conference on Learning Representations (ICLR)},\n  abstract = {This paper introduces rStar, a self-play mutual reasoning approach that significantly improves reasoning capabilities of small language models (SLMs) without fine-tuning or superior models. rStar decouples reasoning into a self-play mutual generation-discrimination process. First, a target SLM augments the Monte Carlo Tree Search (MCTS) with a rich set of human-like reasoning actions to construct higher quality reasoning trajectories. Next, another SLM, with"])</script><script>self.__next_f.push([1," capabilities similar to the target SLM, acts as a discriminator to verify each trajectory generated by the target SLM. The mutually agreed reasoning trajectories are considered mutual consistent, thus are more likely to be correct. Extensive experiments across five SLMs demonstrate rStar can effectively solve diverse reasoning problems, including GSM8K, GSM-Hard, MATH, SVAMP, and StrategyQA. Remarkably, rStar boosts GSM8K accuracy from 12.51\\% to 63.91\\% for LLaMA2-7B, from 36.46\\% to 81.88\\% for Mistral-7B, from 74.53\\% to 91.13\\% for LLaMA3-8B-Instruct.},\n  arxiv = {https://arxiv.org/abs/2408.06195}\n}1e:T4ab,@inproceedings{data-extraction-rag2025,\n  title = {Follow My Instruction and Spill the Beans: Scalable Data Extraction from Retrieval-Augmented Generation Systems},\n  author = {Zhenting Qi and Hanlin Zhang and Eric Xing and Sham Kakade and Himabindu Lakkaraju},\n  year = {2025},\n  month = {may},\n  booktitle = {International Conference on Learning Representations (ICLR)},\n  abstract = {This paper presents a novel approach to scalable data extraction from retrieval-augmented generation (RAG) systems. By leveraging instruction-following capabilities of large language models, we design a framework that systematically generates prompts to guide LLMs in revealing embedded information, enabling scalable data extraction without direct access to the underlying retrieval mechanisms. Our method demonstrates effectiveness across various RAG architectures, highlighting potential vulnerabilities and emphasizing the need for robust data protection strategies in LLM applications. Through extensive experiments, we show that our approach can efficiently extract valuable information from RAG systems, raising important security and privacy concerns.},\n  arxiv = {https://arxiv.org/abs/2402.17840}\n}1f:T5e5,While large language models (LLMs) have shown exceptional capabilities in understanding complex queries and performing sophisticated tasks, their generalization abilities are often deeply entangled with memorization, necessita"])</script><script>self.__next_f.push([1,"ting more precise evaluation. To address this challenge, we introduce Scylla, a dynamic evaluation framework that quantitatively measures the generalization abilities of LLMs. Scylla disentangles generalization from memorization via assessing model performance on both in-distribution (ID) and out-of-distribution (OOD) data through 20 tasks across 5 levels of complexity. Through extensive experiments, we uncover a non-monotonic relationship between task complexity and the performance gap between ID and OOD data, which we term the generalization valley. Specifically, this phenomenon reveals a critical threshold - referred to as critical complexity - where reliance on non-generalizable behavior peaks, indicating the upper bound of LLMs' generalization capabilities. As model size increases, the critical complexity shifts toward higher levels of task complexity, suggesting that larger models can handle more complex reasoning tasks before over-relying on memorization. Leveraging Scylla and the concept of critical complexity, we benchmark 28 LLMs including both open-sourced models such as LLaMA and Qwen families, and close-sourced models like Claude and GPT, providing a more robust evaluation and establishing a clearer understanding of LLMs' generalization capabilities.20:T7a4,@inproceedings{quantifying-generalization2025,\n  title = {Quantifying Generalization Complexity for Large Language Models},\n  author = {Zhenting Qi and Hongyin Luo and Xuliang Huang and Zhuokai Zhao and Yibo Jiang and Xiangjun Fan and Himabindu Lakkaraju and James Glass},\n  year = {2025},\n  month = {may},\n  booktitle = {International Conference on Learning Representations (ICLR)},\n  abstract = {While large language models (LLMs) have shown exceptional capabilities in understanding complex queries and performing sophisticated tasks, their generalization abilities are often deeply entangled with memorization, necessitating more precise evaluation. To address this challenge, we introduce Scylla, a dynamic evaluation framework that quantitatively meas"])</script><script>self.__next_f.push([1,"ures the generalization abilities of LLMs. Scylla disentangles generalization from memorization via assessing model performance on both in-distribution (ID) and out-of-distribution (OOD) data through 20 tasks across 5 levels of complexity. Through extensive experiments, we uncover a non-monotonic relationship between task complexity and the performance gap between ID and OOD data, which we term the generalization valley. Specifically, this phenomenon reveals a critical threshold - referred to as critical complexity - where reliance on non-generalizable behavior peaks, indicating the upper bound of LLMs' generalization capabilities. As model size increases, the critical complexity shifts toward higher levels of task complexity, suggesting that larger models can handle more complex reasoning tasks before over-relying on memorization. Leveraging Scylla and the concept of critical complexity, we benchmark 28 LLMs including both open-sourced models such as LLaMA and Qwen families, and close-sourced models like Claude and GPT, providing a more robust evaluation and establishing a clearer understanding of LLMs' generalization capabilities.},\n  arxiv = {https://arxiv.org/abs/2410.01769}\n}21:T55d,@inproceedings{folio2024,\n  title = {FOLIO: Natural Language Reasoning with First-Order Logic},\n  author = {Simeng Han and Hailey Schoelkopf and Yilun Zhao and Zhenting Qi and Martin Riddell and Wenfei Zhou and James Coady and David Peng and Yujie Qiao and Luke Benson and Lucy Sun and Alex Wardle-Solano and Hannah Szabo and Ekaterina Zubova and Matthew Burtell and Jonathan Fan and Yixin Liu and Brian Wong and Malcolm Sailor and Ansong Ni and Linyong Nan and Jungo Kasai and Tao Yu and Rui Zhang and Alexander R. Fabbri and Wojciech Kryscinski and Semih Yavuz and Ye Liu and Xi Victoria Lin and Shafiq Joty and Yingbo Zhou and Caiming Xiong and Rex Ying and Arman Cohan and Dragomir Radev},\n  year = {2024},\n  month = {dec},\n  booktitle = {Conference on Empirical Methods in Natural Language Processing (EMNLP)},\n  abstract = {FOLIO prese"])</script><script>self.__next_f.push([1,"nts a dataset and benchmark for evaluating natural language reasoning capabilities of AI systems using First-Order Logic (FOL). The dataset comprises a wide range of reasoning tasks that require understanding and applying FOL principles to natural language statements. FOLIO serves as a valuable resource for developing and assessing models capable of logical reasoning in natural language contexts, advancing research in integrating formal logic with natural language understanding.},\n  arxiv = {https://arxiv.org/abs/2209.00840}\n}22:T472,@inproceedings{p-folio2024,\n  title = {P-FOLIO: Evaluating and Improving Logical Reasoning with Abundant Human-Written Reasoning Chains},\n  author = {Simeng Han and Aaron Yu and Rui Shen and Zhenting Qi and Martin Riddell and Wenfei Zhou and Yujie Qiao and Yilun Zhao and Semih Yavuz and Ye Liu and Shafiq Joty and Yingbo Zhou and Caiming Xiong and Dragomir Radev and Rex Ying and Arman Cohan},\n  year = {2024},\n  month = {dec},\n  booktitle = {Conference on Empirical Methods in Natural Language Processing (EMNLP)},\n  note = {Findings},\n  abstract = {P-FOLIO extends the FOLIO dataset by providing abundant human-written reasoning chains for evaluating and improving logical reasoning capabilities. The dataset includes detailed reasoning processes that demonstrate how humans approach logical reasoning tasks, providing valuable training data and evaluation benchmarks for developing more capable reasoning models. P-FOLIO enables researchers to better understand and improve the logical reasoning capabilities of AI systems through access to high-quality human reasoning demonstrations.},\n  arxiv = {https://arxiv.org/abs/2410.09207}\n}23:T450,@inproceedings{constrained-human-ai-cooperation2024,\n  title = {Constrained Human-AI Cooperation: An Inclusive Embodied Social Intelligence Challenge},\n  author = {Weihua Du and Qiushi Lyu and Jiaming Shan and Zhenting Qi and Hongxin Zhang and Sunli Chen and Andi Peng and Tianmin Shu and Kwonjoon Lee and Behzad Dariush and Chuang Gan},\n  year = {2024},\n  month"])</script><script>self.__next_f.push([1," = {dec},\n  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},\n  note = {Dataset and Benchmark Track},\n  abstract = {We present the Constrained Human-AI Cooperation (CHAIC) challenge, an inclusive benchmark designed to advance research in embodied social intelligence. The challenge presents scenarios where AI agents must collaborate with humans under specific constraints, testing their ability to understand and adapt to human behaviors and social norms. The benchmark includes a diverse set of tasks designed to evaluate various aspects of social intelligence in embodied AI systems, including communication, cooperation, and adaptation in constrained environments.},\n  arxiv = {https://arxiv.org/abs/2411.01796}\n}24:T451,@inproceedings{self-criticism2023,\n  title = {Self-Criticism: Aligning Large Language Models with their Understanding of Helpfulness, Honesty, and Harmlessness},\n  author = {Xiaoyu Tan and Shaojie Shi and Xihe Qiu and Chao Qu and Zhenting Qi and Yinghui Xu and Yuan Qi},\n  year = {2023},\n  month = {dec},\n  booktitle = {Conference on Empirical Methods in Natural Language Processing (EMNLP)},\n  note = {Industry Track (Oral)},\n  abstract = {Self-Criticism presents a novel approach to align large language models with their understanding of helpfulness, honesty, and harmlessness through self-evaluation mechanisms. The method enables models to critique and improve their own outputs by applying their understanding of these core values. By incorporating self-criticism into the training and inference process, we demonstrate improved alignment with human values and better adherence to ethical guidelines. The approach leverages the model's own understanding of what constitutes helpful, honest, and harmless behavior to guide its responses.},\n  url = {https://aclanthology.org/2023.emnlp-industry.62/}\n}25:T415,@inproceedings{safer2023,\n  title = {SaFER: A Robust and Efficient Framework for Fine-tuning BERT-based Classifier with Noisy Labels},\n  author = {Zhenting Qi and Xiaoyu Tan and Chao Qu"])</script><script>self.__next_f.push([1," and Yinghui Xu and Yuan Qi},\n  year = {2023},\n  month = {jul},\n  booktitle = {Annual Meeting of the Association for Computational Linguistics (ACL)},\n  note = {Industry Track},\n  abstract = {SaFER introduces a robust and efficient framework for fine-tuning BERT-based classifiers when dealing with noisy labels. The framework addresses the challenge of label noise in real-world datasets by incorporating noise-robust training strategies and efficient fine-tuning techniques. SaFER demonstrates improved performance on noisy datasets while maintaining computational efficiency, making it suitable for practical applications where label quality cannot be guaranteed. The approach combines noise detection, label correction, and robust training methods to achieve reliable classification performance.},\n  url = {https://aclanthology.org/2023.acl-industry.38/}\n}26:T424,@inproceedings{loft2023,\n  title = {LoFT: Enhancing Faithfulness and Diversity for Table-to-Text Generation via Logic Form Control},\n  author = {Yilun Zhao and Zhenting Qi and Linyong Nan and Lorenzo Jaime Flores and Dragomir Radev},\n  year = {2023},\n  month = {feb},\n  booktitle = {Conference of the European Chapter of the Association for Computational Linguistics (EACL)},\n  note = {Short Paper (Oral)},\n  abstract = {LoFT introduces a novel approach to enhance faithfulness and diversity in table-to-text generation through logic form control. The method leverages logical representations to guide the generation process, ensuring that generated text accurately reflects the underlying table data while maintaining diversity. By incorporating logic form constraints, LoFT addresses common issues in table-to-text generation such as factual errors and repetitive outputs. The approach demonstrates improved faithfulness to source tables and increased diversity in generated text compared to baseline methods.},\n  arxiv = {https://arxiv.org/abs/2302.02962}\n}27:T401,@inproceedings{reastap2022,\n  title = {ReasTAP: Injecting Table Reasoning Skills During Pre-training via Synthet"])</script><script>self.__next_f.push([1,"ic Reasoning Examples},\n  author = {Yilun Zhao and Linyong Nan and Zhenting Qi and Rui Zhang and Dragomir Radev},\n  year = {2022},\n  month = {dec},\n  booktitle = {Conference on Empirical Methods in Natural Language Processing (EMNLP)},\n  abstract = {ReasTAP introduces a method to inject table reasoning skills during pre-training by generating and incorporating synthetic reasoning examples. The approach creates diverse reasoning scenarios that teach models to understand and reason over tabular data during the pre-training phase. By exposing models to synthetic reasoning examples, ReasTAP improves their ability to handle table-related tasks without requiring extensive task-specific fine-tuning. The method demonstrates improved performance on downstream table reasoning tasks, showing the effectiveness of incorporating reasoning skills during pre-training.},\n  arxiv = {https://arxiv.org/abs/2210.12374}\n}"])</script><script>self.__next_f.push([1,"8:[\"$\",\"div\",null,{\"className\":\"max-w-4xl mx-auto px-4 sm:px-6 lg:px-8 py-12\",\"children\":[[\"$\",\"$L16\",null,{\"config\":{\"type\":\"publication\",\"title\":\"Publications\",\"description\":\"A collection of my research work.\",\"source\":\"publications.bib\"},\"publications\":[{\"id\":\"evolm2025\",\"title\":\"EvoLM: In Search of Lost Language Model Training Dynamics\",\"authors\":[{\"name\":\"Zhenting Qi\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Fan Nie\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Alexandre Alahi\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"James Zou\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Himabindu Lakkaraju\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yilun Du\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Eric Xing\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Sham Kakade\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Hanlin Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2025,\"month\":\"12\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$8:props:children:0:props:publications:0:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"Advances in Neural Information Processing Systems (NeurIPS)\",\"abstract\":\"$17\",\"description\":\"We developed a comprehensive model suite for analyzing language model training dynamics across pre-training, continued pre-training, supervised fine-tuning, and reinforcement learning stages.\",\"selected\":true,\"awards\":[\"Oral Presentation\"],\"arxivId\":\"2506.16029\",\"url\":\"https://arxiv.org/abs/2506.16029\",\"pdfUrl\":\"https://arxiv.org/pdf/2506.16029.pdf\",\"bibtex\":\"$18\"},{\"id\":\"faithful-thinking-drafts2025\",\"title\":\"Measuring the Faithfulness of Thinking Drafts in Large Reasoning Models\",\"authors\":[{\"name\":\"Zidi Xiong\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Shan Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhenting Qi\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Himabindu Lakkaraju\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2025,\"month\":\"12\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$8:props:children:0:props:publications:1:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"Advances in Neural Information Processing Systems (NeurIPS)\",\"abstract\":\"$19\",\"description\":\"We introduced a systematic framework to evaluate the faithfulness of thinking drafts in Large Reasoning Models using counterfactual interventions.\",\"selected\":false,\"arxivId\":\"2505.13774\",\"url\":\"https://arxiv.org/abs/2505.13774\",\"pdfUrl\":\"https://arxiv.org/pdf/2505.13774.pdf\",\"bibtex\":\"$1a\"},{\"id\":\"satori2025\",\"title\":\"Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search\",\"authors\":[{\"name\":\"Maohao Shen\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Guangtao Zeng\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Zhenting Qi\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Zhang-Wei Hong\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhenfang Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Wei Lu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Gregory Wornell\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Subhro Das\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"David Cox\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chuang Gan\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2025,\"month\":\"7\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$8:props:children:0:props:publications:2:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"International Conference on Machine Learning (ICML)\",\"abstract\":\"$1b\",\"description\":\"We introduced the COAT reasoning framework to enhance LLM reasoning via autoregressive search with self-reflection and self-exploration.\",\"selected\":true,\"arxivId\":\"2502.02508\",\"url\":\"https://arxiv.org/abs/2502.02508\",\"pdfUrl\":\"https://arxiv.org/pdf/2502.02508.pdf\",\"bibtex\":\"$1c\"},{\"id\":\"mutual-reasoning2025\",\"title\":\"rStar: Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solvers\",\"authors\":[{\"name\":\"Zhenting Qi\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Mingyuan Ma\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Jiahang Xu\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Li Lyna Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Fan Yang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Mao Yang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2025,\"month\":\"5\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$8:props:children:0:props:publications:3:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"International Conference on Learning Representations (ICLR)\",\"abstract\":\"This paper introduces rStar, a self-play mutual reasoning approach that significantly improves reasoning capabilities of small language models (SLMs) without fine-tuning or superior models. rStar decouples reasoning into a self-play mutual generation-discrimination process. First, a target SLM augments the Monte Carlo Tree Search (MCTS) with a rich set of human-like reasoning actions to construct higher quality reasoning trajectories. Next, another SLM, with capabilities similar to the target SLM, acts as a discriminator to verify each trajectory generated by the target SLM. The mutually agreed reasoning trajectories are considered mutual consistent, thus are more likely to be correct. Extensive experiments across five SLMs demonstrate rStar can effectively solve diverse reasoning problems, including GSM8K, GSM-Hard, MATH, SVAMP, and StrategyQA. Remarkably, rStar boosts GSM8K accuracy from 12.51% to 63.91% for LLaMA2-7B, from 36.46% to 81.88% for Mistral-7B, from 74.53% to 91.13% for LLaMA3-8B-Instruct.\",\"description\":\"We introduced rStar, a self-play mutual reasoning approach that enhances reasoning capabilities of small language models without fine-tuning or superior models.\",\"selected\":true,\"arxivId\":\"2408.06195\",\"url\":\"https://arxiv.org/abs/2408.06195\",\"pdfUrl\":\"https://arxiv.org/pdf/2408.06195.pdf\",\"bibtex\":\"$1d\"},{\"id\":\"data-extraction-rag2025\",\"title\":\"Follow My Instruction and Spill the Beans: Scalable Data Extraction from Retrieval-Augmented Generation Systems\",\"authors\":[{\"name\":\"Zhenting Qi\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Hanlin Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Eric Xing\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Sham Kakade\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Himabindu Lakkaraju\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2025,\"month\":\"5\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$8:props:children:0:props:publications:4:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"International Conference on Learning Representations (ICLR)\",\"abstract\":\"This paper presents a novel approach to scalable data extraction from retrieval-augmented generation (RAG) systems. By leveraging instruction-following capabilities of large language models, we design a framework that systematically generates prompts to guide LLMs in revealing embedded information, enabling scalable data extraction without direct access to the underlying retrieval mechanisms. Our method demonstrates effectiveness across various RAG architectures, highlighting potential vulnerabilities and emphasizing the need for robust data protection strategies in LLM applications. Through extensive experiments, we show that our approach can efficiently extract valuable information from RAG systems, raising important security and privacy concerns.\",\"description\":\"We developed a scalable method for extracting data from RAG systems using LLMs' instruction-following capabilities.\",\"selected\":true,\"arxivId\":\"2402.17840\",\"url\":\"https://arxiv.org/abs/2402.17840\",\"pdfUrl\":\"https://arxiv.org/pdf/2402.17840.pdf\",\"bibtex\":\"$1e\"},{\"id\":\"quantifying-generalization2025\",\"title\":\"Quantifying Generalization Complexity for Large Language Models\",\"authors\":[{\"name\":\"Zhenting Qi\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Hongyin Luo\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xuliang Huang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhuokai Zhao\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yibo Jiang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xiangjun Fan\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Himabindu Lakkaraju\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"James Glass\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2025,\"month\":\"5\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$8:props:children:0:props:publications:5:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"International Conference on Learning Representations (ICLR)\",\"abstract\":\"$1f\",\"description\":\"We introduced Scylla, a dynamic evaluation framework that quantitatively measures LLMs' generalization abilities by disentangling generalization from memorization.\",\"selected\":true,\"arxivId\":\"2410.01769\",\"url\":\"https://arxiv.org/abs/2410.01769\",\"pdfUrl\":\"https://arxiv.org/pdf/2410.01769.pdf\",\"bibtex\":\"$20\"},{\"id\":\"folio2024\",\"title\":\"FOLIO: Natural Language Reasoning with First-Order Logic\",\"authors\":[{\"name\":\"Simeng Han\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Hailey Schoelkopf\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yilun Zhao\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhenting Qi\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Martin Riddell\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Wenfei Zhou\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"James Coady\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"David Peng\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yujie Qiao\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Luke Benson\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Lucy Sun\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Alex Wardle-Solano\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Hannah Szabo\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Ekaterina Zubova\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Matthew Burtell\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jonathan Fan\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yixin Liu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Brian Wong\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Malcolm Sailor\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Ansong Ni\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Linyong Nan\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jungo Kasai\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Tao Yu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Rui Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Alexander R. Fabbri\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Wojciech Kryscinski\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Semih Yavuz\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Ye Liu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xi Victoria Lin\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Shafiq Joty\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yingbo Zhou\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Caiming Xiong\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Rex Ying\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Arman Cohan\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Dragomir Radev\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2024,\"month\":\"12\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$8:props:children:0:props:publications:6:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"Conference on Empirical Methods in Natural Language Processing (EMNLP)\",\"abstract\":\"FOLIO presents a dataset and benchmark for evaluating natural language reasoning capabilities of AI systems using First-Order Logic (FOL). The dataset comprises a wide range of reasoning tasks that require understanding and applying FOL principles to natural language statements. FOLIO serves as a valuable resource for developing and assessing models capable of logical reasoning in natural language contexts, advancing research in integrating formal logic with natural language understanding.\",\"description\":\"We developed a comprehensive dataset and benchmark for natural language reasoning using First-Order Logic.\",\"selected\":false,\"arxivId\":\"2209.00840\",\"url\":\"https://arxiv.org/abs/2209.00840\",\"pdfUrl\":\"https://arxiv.org/pdf/2209.00840.pdf\",\"bibtex\":\"$21\"},{\"id\":\"p-folio2024\",\"title\":\"P-FOLIO: Evaluating and Improving Logical Reasoning with Abundant Human-Written Reasoning Chains\",\"authors\":[{\"name\":\"Simeng Han\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Aaron Yu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Rui Shen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhenting Qi\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Martin Riddell\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Wenfei Zhou\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yujie Qiao\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yilun Zhao\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Semih Yavuz\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Ye Liu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Shafiq Joty\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yingbo Zhou\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Caiming Xiong\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Dragomir Radev\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Rex Ying\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Arman Cohan\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2024,\"month\":\"12\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$8:props:children:0:props:publications:7:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"Conference on Empirical Methods in Natural Language Processing (EMNLP)\",\"abstract\":\"P-FOLIO extends the FOLIO dataset by providing abundant human-written reasoning chains for evaluating and improving logical reasoning capabilities. The dataset includes detailed reasoning processes that demonstrate how humans approach logical reasoning tasks, providing valuable training data and evaluation benchmarks for developing more capable reasoning models. P-FOLIO enables researchers to better understand and improve the logical reasoning capabilities of AI systems through access to high-quality human reasoning demonstrations.\",\"description\":\"We extended FOLIO with abundant human-written reasoning chains, providing detailed reasoning processes for logical reasoning tasks.\",\"selected\":false,\"arxivId\":\"2410.09207\",\"url\":\"https://arxiv.org/abs/2410.09207\",\"pdfUrl\":\"https://arxiv.org/pdf/2410.09207.pdf\",\"bibtex\":\"$22\"},{\"id\":\"constrained-human-ai-cooperation2024\",\"title\":\"Constrained Human-AI Cooperation: An Inclusive Embodied Social Intelligence Challenge\",\"authors\":[{\"name\":\"Weihua Du\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Qiushi Lyu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jiaming Shan\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhenting Qi\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Hongxin Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Sunli Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Andi Peng\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Tianmin Shu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Kwonjoon Lee\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Behzad Dariush\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chuang Gan\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2024,\"month\":\"12\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$8:props:children:0:props:publications:8:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"Advances in Neural Information Processing Systems (NeurIPS)\",\"abstract\":\"We present the Constrained Human-AI Cooperation (CHAIC) challenge, an inclusive benchmark designed to advance research in embodied social intelligence. The challenge presents scenarios where AI agents must collaborate with humans under specific constraints, testing their ability to understand and adapt to human behaviors and social norms. The benchmark includes a diverse set of tasks designed to evaluate various aspects of social intelligence in embodied AI systems, including communication, cooperation, and adaptation in constrained environments.\",\"description\":\"We introduced a comprehensive benchmark challenge for advancing research in embodied social intelligence through constrained human-AI cooperation scenarios.\",\"selected\":false,\"arxivId\":\"2411.01796\",\"url\":\"https://arxiv.org/abs/2411.01796\",\"pdfUrl\":\"https://arxiv.org/pdf/2411.01796.pdf\",\"bibtex\":\"$23\"},{\"id\":\"pillow2023\",\"title\":\"PILLOW: Enhancing Efficient Instruction Fine-tuning via Prompt Matching\",\"authors\":[{\"name\":\"Zhenting Qi\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xiaoyu Tan\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Shaojie Shi\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chao Qu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yinghui Xu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yuan Qi\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2023,\"month\":\"12\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$8:props:children:0:props:publications:9:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"Conference on Empirical Methods in Natural Language Processing (EMNLP)\",\"abstract\":\"PILLOW introduces a novel approach to enhance efficient instruction fine-tuning through prompt matching. The method leverages prompt similarity and matching techniques to improve the effectiveness of instruction fine-tuning while maintaining computational efficiency. By identifying and matching similar prompts during training, PILLOW enables more efficient use of training data and better generalization to new instructions. The approach demonstrates significant improvements in instruction following capabilities while reducing computational costs compared to standard fine-tuning methods.\",\"description\":\"We introduced a prompt matching framework to enhance the efficiency of instruction fine-tuning.\",\"selected\":false,\"awards\":[\"Oral Presentation\"],\"arxivId\":\"2312.05621\",\"url\":\"https://arxiv.org/abs/2312.05621\",\"pdfUrl\":\"https://arxiv.org/pdf/2312.05621.pdf\",\"bibtex\":\"@inproceedings{pillow2023,\\n  title = {PILLOW: Enhancing Efficient Instruction Fine-tuning via Prompt Matching},\\n  author = {Zhenting Qi and Xiaoyu Tan and Shaojie Shi and Chao Qu and Yinghui Xu and Yuan Qi},\\n  year = {2023},\\n  month = {dec},\\n  booktitle = {Conference on Empirical Methods in Natural Language Processing (EMNLP)},\\n  note = {Industry Track (Oral)},\\n  abstract = {PILLOW introduces a novel approach to enhance efficient instruction fine-tuning through prompt matching. The method leverages prompt similarity and matching techniques to improve the effectiveness of instruction fine-tuning while maintaining computational efficiency. By identifying and matching similar prompts during training, PILLOW enables more efficient use of training data and better generalization to new instructions. The approach demonstrates significant improvements in instruction following capabilities while reducing computational costs compared to standard fine-tuning methods.},\\n  arxiv = {https://arxiv.org/abs/2312.05621}\\n}\"},{\"id\":\"qtsumm2023\",\"title\":\"QTSumm: A New Benchmark for Query-Focused Table Summarization\",\"authors\":[{\"name\":\"Yilun Zhao\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhenting Qi\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Linyong Nan\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Boyu Mi\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yixin Liu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Weijin Zou\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Simeng Han\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xiangru Tang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yumo Xu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Arman Cohan\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Dragomir Radev\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2023,\"month\":\"12\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$8:props:children:0:props:publications:10:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"Conference on Empirical Methods in Natural Language Processing (EMNLP)\",\"abstract\":\"QTSumm introduces a new benchmark for query-focused table summarization, a task that requires generating summaries of tabular data in response to specific queries. The benchmark includes diverse tables and queries, covering various domains and complexity levels. QTSumm enables systematic evaluation of models' abilities to understand tabular data, interpret queries, and generate relevant summaries. The dataset provides a foundation for advancing research in table understanding and query-focused summarization.\",\"description\":\"We introduced a comprehensive benchmark dataset for query-focused table summarization.\",\"selected\":false,\"arxivId\":\"2305.14303\",\"url\":\"https://arxiv.org/abs/2305.14303\",\"pdfUrl\":\"https://arxiv.org/pdf/2305.14303.pdf\",\"bibtex\":\"@inproceedings{qtsumm2023,\\n  title = {QTSumm: A New Benchmark for Query-Focused Table Summarization},\\n  author = {Yilun Zhao and Zhenting Qi and Linyong Nan and Boyu Mi and Yixin Liu and Weijin Zou and Simeng Han and Xiangru Tang and Yumo Xu and Arman Cohan and Dragomir Radev},\\n  year = {2023},\\n  month = {dec},\\n  booktitle = {Conference on Empirical Methods in Natural Language Processing (EMNLP)},\\n  abstract = {QTSumm introduces a new benchmark for query-focused table summarization, a task that requires generating summaries of tabular data in response to specific queries. The benchmark includes diverse tables and queries, covering various domains and complexity levels. QTSumm enables systematic evaluation of models' abilities to understand tabular data, interpret queries, and generate relevant summaries. The dataset provides a foundation for advancing research in table understanding and query-focused summarization.},\\n  arxiv = {https://arxiv.org/abs/2305.14303}\\n}\"},{\"id\":\"self-criticism2023\",\"title\":\"Self-Criticism: Aligning Large Language Models with their Understanding of Helpfulness, Honesty, and Harmlessness\",\"authors\":[{\"name\":\"Xiaoyu Tan\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Shaojie Shi\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xihe Qiu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chao Qu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhenting Qi\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yinghui Xu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yuan Qi\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2023,\"month\":\"12\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$8:props:children:0:props:publications:11:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"Conference on Empirical Methods in Natural Language Processing (EMNLP)\",\"abstract\":\"Self-Criticism presents a novel approach to align large language models with their understanding of helpfulness, honesty, and harmlessness through self-evaluation mechanisms. The method enables models to critique and improve their own outputs by applying their understanding of these core values. By incorporating self-criticism into the training and inference process, we demonstrate improved alignment with human values and better adherence to ethical guidelines. The approach leverages the model's own understanding of what constitutes helpful, honest, and harmless behavior to guide its responses.\",\"description\":\"We introduced a self-criticism framework that enables models to evaluate and improve their outputs based on their understanding of helpfulness, honesty, and harmlessness.\",\"selected\":false,\"awards\":[\"Oral Presentation\"],\"url\":\"https://aclanthology.org/2023.emnlp-industry.62/\",\"bibtex\":\"$24\"},{\"id\":\"openrt2023\",\"title\":\"OpenRT: An Open-source Framework for Reasoning Over Tabular Data\",\"authors\":[{\"name\":\"Yilun Zhao\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Boyu Mi\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhenting Qi\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Linyong Nan\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Minghao Guo\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Arman Cohan\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Dragomir Radev\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2023,\"month\":\"7\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$8:props:children:0:props:publications:12:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"Annual Meeting of the Association for Computational Linguistics (ACL)\",\"abstract\":\"OpenRT presents an open-source framework for reasoning over tabular data, providing tools and infrastructure for systematic evaluation and development of table reasoning models. The framework includes data processing utilities, model interfaces, evaluation metrics, and benchmarking capabilities. OpenRT enables researchers and practitioners to easily develop, evaluate, and compare different approaches to table reasoning, advancing the field through open collaboration and reproducible research.\",\"description\":\"We developed and released an open-source framework for reasoning over tabular data.\",\"selected\":false,\"url\":\"https://aclanthology.org/2023.acl-demo.32/\",\"bibtex\":\"@inproceedings{openrt2023,\\n  title = {OpenRT: An Open-source Framework for Reasoning Over Tabular Data},\\n  author = {Yilun Zhao and Boyu Mi and Zhenting Qi and Linyong Nan and Minghao Guo and Arman Cohan and Dragomir Radev},\\n  year = {2023},\\n  month = {jul},\\n  booktitle = {Annual Meeting of the Association for Computational Linguistics (ACL)},\\n  note = {System Demonstration Track},\\n  abstract = {OpenRT presents an open-source framework for reasoning over tabular data, providing tools and infrastructure for systematic evaluation and development of table reasoning models. The framework includes data processing utilities, model interfaces, evaluation metrics, and benchmarking capabilities. OpenRT enables researchers and practitioners to easily develop, evaluate, and compare different approaches to table reasoning, advancing the field through open collaboration and reproducible research.},\\n  url = {https://aclanthology.org/2023.acl-demo.32/}\\n}\"},{\"id\":\"robut2023\",\"title\":\"RobuT: A Systematic Study of Table QA Robustness Against Human-Annotated Adversarial Perturbations\",\"authors\":[{\"name\":\"Yilun Zhao\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chen Zhao\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Linyong Nan\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhenting Qi\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Wenlin Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xiangru Tang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Boyu Mi\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Dragomir Radev\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2023,\"month\":\"7\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$8:props:children:0:props:publications:13:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"Annual Meeting of the Association for Computational Linguistics (ACL)\",\"abstract\":\"RobuT presents a systematic study of table question answering (QA) robustness against human-annotated adversarial perturbations. We investigate how various types of perturbations affect model performance and identify vulnerabilities in current table QA systems. The study includes comprehensive analysis of different perturbation strategies, evaluation of model robustness across multiple benchmarks, and insights into failure modes. Our findings reveal significant robustness challenges in table QA systems and provide guidance for developing more robust models.\",\"description\":\"We conducted a systematic study of table QA robustness against human-annotated adversarial perturbations.\",\"selected\":false,\"arxivId\":\"2306.14321\",\"url\":\"https://arxiv.org/abs/2306.14321\",\"pdfUrl\":\"https://arxiv.org/pdf/2306.14321.pdf\",\"bibtex\":\"@inproceedings{robut2023,\\n  title = {RobuT: A Systematic Study of Table QA Robustness Against Human-Annotated Adversarial Perturbations},\\n  author = {Yilun Zhao and Chen Zhao and Linyong Nan and Zhenting Qi and Wenlin Zhang and Xiangru Tang and Boyu Mi and Dragomir Radev},\\n  year = {2023},\\n  month = {jul},\\n  booktitle = {Annual Meeting of the Association for Computational Linguistics (ACL)},\\n  abstract = {RobuT presents a systematic study of table question answering (QA) robustness against human-annotated adversarial perturbations. We investigate how various types of perturbations affect model performance and identify vulnerabilities in current table QA systems. The study includes comprehensive analysis of different perturbation strategies, evaluation of model robustness across multiple benchmarks, and insights into failure modes. Our findings reveal significant robustness challenges in table QA systems and provide guidance for developing more robust models.},\\n  arxiv = {https://arxiv.org/abs/2306.14321}\\n}\"},{\"id\":\"safer2023\",\"title\":\"SaFER: A Robust and Efficient Framework for Fine-tuning BERT-based Classifier with Noisy Labels\",\"authors\":[{\"name\":\"Zhenting Qi\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xiaoyu Tan\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chao Qu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yinghui Xu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yuan Qi\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2023,\"month\":\"7\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$8:props:children:0:props:publications:14:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"Annual Meeting of the Association for Computational Linguistics (ACL)\",\"abstract\":\"SaFER introduces a robust and efficient framework for fine-tuning BERT-based classifiers when dealing with noisy labels. The framework addresses the challenge of label noise in real-world datasets by incorporating noise-robust training strategies and efficient fine-tuning techniques. SaFER demonstrates improved performance on noisy datasets while maintaining computational efficiency, making it suitable for practical applications where label quality cannot be guaranteed. The approach combines noise detection, label correction, and robust training methods to achieve reliable classification performance.\",\"description\":\"We developed a robust framework for fine-tuning BERT-based classifiers in the presence of noisy labels.\",\"selected\":false,\"url\":\"https://aclanthology.org/2023.acl-industry.38/\",\"bibtex\":\"$25\"},{\"id\":\"loft2023\",\"title\":\"LoFT: Enhancing Faithfulness and Diversity for Table-to-Text Generation via Logic Form Control\",\"authors\":[{\"name\":\"Yilun Zhao\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhenting Qi\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Linyong Nan\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Lorenzo Jaime Flores\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Dragomir Radev\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2023,\"month\":\"2\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$8:props:children:0:props:publications:15:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"Conference of the European Chapter of the Association for Computational Linguistics (EACL)\",\"abstract\":\"LoFT introduces a novel approach to enhance faithfulness and diversity in table-to-text generation through logic form control. The method leverages logical representations to guide the generation process, ensuring that generated text accurately reflects the underlying table data while maintaining diversity. By incorporating logic form constraints, LoFT addresses common issues in table-to-text generation such as factual errors and repetitive outputs. The approach demonstrates improved faithfulness to source tables and increased diversity in generated text compared to baseline methods.\",\"description\":\"We introduced logic form control mechanisms to guide table-to-text generation and ensure faithfulness to source data.\",\"selected\":false,\"awards\":[\"Oral Presentation\"],\"arxivId\":\"2302.02962\",\"url\":\"https://arxiv.org/abs/2302.02962\",\"pdfUrl\":\"https://arxiv.org/pdf/2302.02962.pdf\",\"bibtex\":\"$26\"},{\"id\":\"reastap2022\",\"title\":\"ReasTAP: Injecting Table Reasoning Skills During Pre-training via Synthetic Reasoning Examples\",\"authors\":[{\"name\":\"Yilun Zhao\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Linyong Nan\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhenting Qi\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Rui Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Dragomir Radev\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2022,\"month\":\"12\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$8:props:children:0:props:publications:16:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"Conference on Empirical Methods in Natural Language Processing (EMNLP)\",\"abstract\":\"ReasTAP introduces a method to inject table reasoning skills during pre-training by generating and incorporating synthetic reasoning examples. The approach creates diverse reasoning scenarios that teach models to understand and reason over tabular data during the pre-training phase. By exposing models to synthetic reasoning examples, ReasTAP improves their ability to handle table-related tasks without requiring extensive task-specific fine-tuning. The method demonstrates improved performance on downstream table reasoning tasks, showing the effectiveness of incorporating reasoning skills during pre-training.\",\"description\":\"We developed methods to generate synthetic reasoning examples for table understanding tasks and integrated table reasoning skills into the pre-training phase.\",\"selected\":false,\"arxivId\":\"2210.12374\",\"url\":\"https://arxiv.org/abs/2210.12374\",\"pdfUrl\":\"https://arxiv.org/pdf/2210.12374.pdf\",\"bibtex\":\"$27\"}]}],false,false]}]\n"])</script><script>self.__next_f.push([1,"d:null\n"])</script><script>self.__next_f.push([1,"11:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\nc:null\n"])</script><script>self.__next_f.push([1,"15:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"Publications | Zhenting Qi\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"A collection of my research work.\"}],[\"$\",\"meta\",\"2\",{\"name\":\"author\",\"content\":\"Zhenting Qi\"}],[\"$\",\"meta\",\"3\",{\"name\":\"keywords\",\"content\":\"Zhenting Qi,PhD,Research,Harvard University\"}],[\"$\",\"meta\",\"4\",{\"name\":\"creator\",\"content\":\"Zhenting Qi\"}],[\"$\",\"meta\",\"5\",{\"name\":\"publisher\",\"content\":\"Zhenting Qi\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:title\",\"content\":\"Zhenting Qi\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:description\",\"content\":\"Computer Science Ph.D. student at Harvard University, researching intelligent and reliable AI systems\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:site_name\",\"content\":\"Zhenting Qi's Academic Website\"}],[\"$\",\"meta\",\"9\",{\"property\":\"og:locale\",\"content\":\"en_US\"}],[\"$\",\"meta\",\"10\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"11\",{\"name\":\"twitter:card\",\"content\":\"summary\"}],[\"$\",\"meta\",\"12\",{\"name\":\"twitter:title\",\"content\":\"Zhenting Qi\"}],[\"$\",\"meta\",\"13\",{\"name\":\"twitter:description\",\"content\":\"Computer Science Ph.D. student at Harvard University, researching intelligent and reliable AI systems\"}],[\"$\",\"link\",\"14\",{\"rel\":\"icon\",\"href\":\"/favicon.svg\"}]],\"error\":null,\"digest\":\"$undefined\"}\nf:{\"metadata\":\"$15:metadata\",\"error\":null,\"digest\":\"$undefined\"}\n"])</script></body></html>